#!/usr/bin/env python3
"""


OVERVIEW:


The code in this module searches a sentence for size measurements and attempts
to determine the subject(s) of those measurements. A 'size measurement' is a
1D, 2D, or 3D expression involving lengths, such as:

        3 mm                                    (1D measurement)
        1.2 cm x 3.6 cm                         (2D measurement)
        2 by 3 by 4 mm                          (3D measurement)
        1.1, 2.3. 8.5, and 12.6 cm              (list of lengths)
        1.5 cm2, 4.3 mm3                        (area and volume)
        2.3 - 4.5 cm                            (range)
        1.5 cm craniocaudal x 2.2 cm transverse (measurement with views)

A measurement 'subject' is the object with those dimensions.

All numeric values are converted to mm, mm2, or mm3 in the JSON results.

The sentence and one or more query terms are specified on the command 
line. A Boolean result is provided in the JSON output indicating 
whether any of the query terms match the measurement subjects.

All term matching is case-insensitive.


OUTPUT:


The set of JSON fields in the output includes:

        sentence           the sentence from which measurements were extracted
        terms              comma-separated list of query terms
        querySuccess       whether any query terms matched a measurement subject
        measurementCount   number of size measurements found
        measurementList    array of individual size measurements

            text           text of the complete size measurement
            start          offset of first char in the matching text
            end            offset of final char in the matching text plus 1

            temporality    indication of when measurement occurred
                           values: 'CURRENT', 'PREVIOUS'

            units          units of the x, y, and z fields
                           values: 'MILLIMETERS', 'SQUARE_MILLIMETERS', 
                                   'CUBIC_MILLIMETERS'

            condition      numeric ranges will have this field set to 'RANGE'
                           all other measurements will set this field to 'EQUAL'

            matchingTerm   an array of all matching query terms
            subject        an array of strings, the possible measurement subjects
            location       string; location of the object being measured

            x              numeric value of first measurement dimension
            y              numeric value of second measurement dimension
            z              numeric value of third measurement dimension
            values         JSON array of all numeric values in a list

            xView          view specification for x value
            yView          view specification for y value
            zView          view specification for z value

            minValue       minimum value of x, y, and z
            maxValue       maximum value of x, y, and z

All JSON results will contain the same number of fields.

Fields that are not applicable to a given sentence will have a value of 
EMPTY_FIELD.

JSON results are written to stdout.


USAGE:


This code requires the SpaCy natural language processing module. SpaCy can be
downloaded and installed by following the instructions at the SpaCy website:

        https://spacy.io/

You will also need to download and install SpaCy's English model files.
Open a terminal and run this command after installing SpaCy:

        python3 -m spacy download en


To use this code as an imported module, add the following lines to the
import list in the importing module:

        import json
        import spacy
        import subject_finder as sf

Before processing any sentences, call init with the path to the ClarityNLP
ngram file:

        sf.init('path/to/clarity_ngrams.txt')

The file 'clarity_ngrams.txt' is generated by ngram_gen.py. If this file
cannot be found, this module generates a FileNotFound exception.


To process a sentence and capture the JSON result:

        json_string = sf.run(term_string, sentence)

        The term string is a comma-separated list of search terms, i.e.
        "cyst, node, kidney".

To unpack the JSON result:

        json_data = json.loads(json_string)
        result = sf.SubjectFinderResult(**json_data)

        The following fields are now available:

            result.sentence
            result.terms
            result.querySuccess
            result.measurementCount
            result.measurementList

To unpack the measurement list:

        meas_list = result.measurementList
        measurements = [sf.Measurement(**m) for m in meas_list]

The fields of each measurement are now accessible via:

        for m in measurements:
            log(m.text)
            log(m.start)
            log(m.end)
            etc.

        If any field has the value EMPTY_FIELD it should be ignored.



COMMAND-LINE USAGE EXAMPLES:



To run the self tests:

        python3 ./subject_finder.py --selftest

        All self-tests passed if no output is generated.

To search for measurements of the spleen using this module's test sentences:

        python3 ./subject_finder.py -t "spleen" --test

To search for a term in a sentence provided on the command line:

        python3 ./subject_finder.py -t "cyst" -s "The cyst in the lower pole of the left kidney measures 1.3 x 1.1 cm."

To visualize a dependency parse of the previous sentence:

        python3 ./subject_finder.py -t "cyst" -s "The cyst in the lower pole of the left kidney measures 1.3 x 1.1 cm." --displacy

        View the dependency parse by opening a browser to the URL 'localhost:5000'. 
        Press <CTRL>C to continue processing after viewing the diagram.

        The words in the sentence depicted in the visualization may contain
        simple word substitutions for medical ngrams. To disable these
        substitutions, run this command:

        python3 ./subject_finder.py -t "cyst" -s "The cyst in the lower pole of the left kidney measures 1.3 x 1.1 cm." --displacy --nosub

        Disabling the ngram substitions may change the dependency parse, though. 
        In general, the use of ngram substitutions results in simpler sentences
        and helps SpaCy produce better results.

"""

import re
import os
import sys
import copy
import json
import spacy
import argparse
from collections import deque
from collections import namedtuple
from spacy.symbols import ORTH, LEMMA, POS, TAG
#from claritynlp_logging import log, ERROR, DEBUG

if __name__ == '__main__':
    # interactive testing; add nlp dir to path to find logging class
    match = re.search(r'nlp/', sys.path[0])
    if match:
        nlp_dir = sys.path[0][:match.end()]
        sys.path.append(nlp_dir)
    else:
        print('\n*** subject_finder.py: nlp dir not found ***\n')
        sys.exit(0)

try:
    import finder_overlap as overlap 
    from size_measurement_finder import run as smf_run, SizeMeasurement, STR_PREVIOUS
except:
    from algorithms.finder import finder_overlap as overlap
    from algorithms.finder.size_measurement_finder import run as smf_run, SizeMeasurement, STR_PREVIOUS

    
FILE_DIR = os.path.dirname(__file__)

# debug only
from spacy import displacy

# load Spacy's English model
nlp = spacy.load('en_core_web_sm')

VERSION_MAJOR = 0
VERSION_MINOR = 10

# set to True to enable debug output
TRACE = False

# serializable result object
# 'measurementList' is an array of Measurement namedtuples
EMPTY_FIELD = None
MEASUREMENT_RESULT_FIELDS = ['sentence', 'terms', 'querySuccess',
                             'measurementCount', 'measurementList']
SubjectFinderResult = namedtuple('SubjectFinderResult', MEASUREMENT_RESULT_FIELDS)

MEASUREMENT_FIELDS = ['text', 'start', 'end', 'temporality', 'units',
                      'condition', 'matchingTerm', 'subject', 'location',
                      'x', 'y', 'z', 'values', 'xView', 'yView', 'zView',
                      'minValue', 'maxValue']
Measurement = namedtuple('Measurement', MEASUREMENT_FIELDS)


# match anything between {} and [], including the brackets
str_brackets_1 = r'\[[^\]]+\]'
str_brackets_2 = r'\{[^\}]+\}'
str_brackets = r'(' + str_brackets_1 + r'|' + str_brackets_2 + r')'
regex_brackets = re.compile(str_brackets)

# match parentheses
str_parens = r'[()]'
regex_parens = re.compile(str_parens)

# image annotations (image 302:33), (782b:49), etc.
regex_image = re.compile(r'\(\s*(image\s+)?[a-z\d]+[:;\s]+[a-z\d]+\)')

# one month ago, two weeks ago, etc.
str_ago = r'\b(one|two|three|four|five|six|seven|eight|nine|ten|eleven|' + \
          r'twelve)\s+(day|week|month|year|)s?\s+ago'
regex_ago = re.compile(str_ago)

# common punctuation (no hyphens)
regex_punctuation = re.compile(r'[.,:;()%\[\]_?!\"]')

# the same, but with hyphen
regex_punctuation_with_hyphen = re.compile(r'[-.,:;()%\[\]_?!\"]')

# regexes for identifying blather that can be removed
blather_regexes = [
    re.compile(r'\b(additional(ly)?|also|another|apparently|approximately|' + \
               r'approx\.?|about|however|immediately|predominantly)\b'),
    re.compile(r'\bas\s+well\s+as\b'),
    re.compile(r'\b(what|there)\s+(may|might|could)\s+(possibly\s+)?be\b'),
    re.compile(r'\bin\s+addition'),
    re.compile(r'\bother\s+than'),
    re.compile(r'\bare\s+[-a-zA-Z\s]+(noted|identified)'),
    re.compile(r'\bfor\s+example\b'),
    re.compile(r'\Aagain\s+seen\s+(is|are)'),
    re.compile(r'\Amost\s+representative\s(is|are)'),
    re.compile(r'\b(no\s+)?evidence\s+of\s+')
]

# verbosity that can be simplified
verbosity_map = {
    re.compile(r'\bmeasuring\s+upwards\s+of\b') : ['measuring'],
    re.compile(r'\bmeasures\s+upwards\s+of\b')  : ['measures'],
    re.compile(r'\bmeasured\s+upwards\s+of\b')  : ['measured'],
    re.compile(r'\bmeasure\s+upwards\s+of\b')   : ['measure'],
    re.compile(r'\bupwards\s+of\b')             : [' '],
    re.compile(r'\bmore\s+so\b')                : ['more'],
    re.compile(r'\bsmall\s+to\s+moderate\b')    : ['small'],
    re.compile(r'\bappears\s+to\s+be\b')        : ['is'],
    re.compile(r'\bcan\s+be\s+seen\b')          : ['is'],
    re.compile(r'\bno\s+evidence\s+of\b')       : ['no'],
    re.compile(r'\binvolving\b')                : ['in'],
    re.compile(r'\bidentified\s+within\b')      : ['inside'],
    re.compile(r'\bright\s+at\s+the\b')         : ['at', 'the'],
    re.compile(r'\bas\s+compared\s+to\b')       : ['compared', 'to'],
    re.compile(r'\bappears\s+unchanged\b')      : ['is', 'unchanged'],
    re.compile(r'\bis\s+seen\s+in\b')           : ['in'],
    re.compile(r'\bexcept\s+to\s+note\b')       : ['except'],
    re.compile(r'\bare\s+again\s+noted\s+to\s+be\b')        : ['are'],
    re.compile(r'\bare\s+again\s+noted\s+arising\s+from\b') : ['in'],
    re.compile(r'\bare\s+again\s+noted\s+at\b')             : ['at'],
    re.compile(r'\bare\s+again\s+noted\s+in\b')             : ['in'],
    re.compile(r'\bare\s+again\s+noted\b')                  : ['exist'],
    re.compile(r'\barising\s+from\b')                       : ['in'],
    re.compile(r'\bagain\s+seen\s+is\b')        : ['there', 'is'],
    re.compile(r'\bis\s+again\s+seen\b')        : ['exists'],
    re.compile(r'\bpresenting\s+with\b')        : ['with'],
    re.compile(r'\branging\s+in\s+size\b')      : ['measuring'],
    re.compile(r'\bconsistent\s+with\s')        : ['like'],
    re.compile(r'\bis\s+seen\s+to\s+contain\b') : ['contains'],
    re.compile(r'\bmeasuring\s+with\b')         : ['with']
}

# roman numerals; must be preceded by whitespace and followed either by
# punctuation or whitespace
str_roman_numeral = r'\s+' + r'(?P<numeral_text>'     +\
                    r'(i|ii|iii|iv|v|vi|vii|viii|ix)' +\
                    r')' + r'([.,:;?!%]|\s+)'
regex_roman_numeral = re.compile(str_roman_numeral)
roman_numeral_map = {
    'i':'1', 'ii':'2', 'iii':'3', 'iv':'4',
    'v':'5', 'vi':'6', 'vii':'7', 'viii':'8', 'ix':'9'
}

# adjectives causing an incorrect dependency parse
str_problem_adjectives = r'\b(diffuse|fusiform|hilar|marked|nondistended)\b'
regex_problem_adjectives = re.compile(str_problem_adjectives)

# colors to be substituted for problematic medical adjectives
COLORS = ['red', 'green', 'blue', 'white', 'yellow', 'orange', 'pink']

# word replacements made during processing
replacements = {}

# liver segments
str_liver_seg_roman_num = r'([i]{1,3}|iv[a,b]?|v[i]{0,3})'
str_liver_seg_decimal = r'(1|2|3|4|4a|4b|5|6|7|8)'
str_liver_seg_num = r'(' + str_liver_seg_roman_num + r'|' + str_liver_seg_decimal + r')'
str_liver_seg = r'\b((liver|hepatic)\s+)?seg(ment)?\s+(?P<segnum>' + str_liver_seg_num + r')\b'
regex_liver_seg = re.compile(str_liver_seg)
liver_seg_map = {
    'i':'1', 'ii':'2', 'iii':'3', 'iv':'4',
    'iva':'4a', 'ivb':'4b', 'v':'5','vi':'6', 'vii':'7', 'viii':'8'
}

# Match various word sequences, including hyphenated words and abbreviations.
# A 'word' can also be a number or a measurement with a '/' character, such as
# a blood pressure value.
str_word = r'\b([-/.a-z\d]+[,;:%?!]?\s+)'
str_words = str_word + r'{0,12}?'          # nongreedy
str_words_g = r'(\b[-/.a-z\d]+\b[,;:%?!]?\s*){0,12}'   # greedy
str_words_0_to_n = r'(\b[-/.a-z\d]+\b[,;:%?!]?\s+)*?'  # 0 or more words, nongreedy
str_words_1_to_n = str_word + r'+?'        # 1 or more words, nongreedy

# measurement verbs
str_meas_verb = r'\b(measure[ds]?|measuring)\s+'

str_meas_verb_no_space = r'\b(measure[ds]?|measuring)\b'
regex_meas_verb_no_space = re.compile(str_meas_verb_no_space)

# measurement qualifiers
str_qualifiers = r'(up\s+to\b|all|approximately|approx\.?\b|about|'           +\
                 r'currently|dominant|mainly|maximally|minimally|mostly|'     +\
                 r'of\b|occasionally|only)\s+'

str_meas_qualifiers = r'(' + str_qualifiers + r'){0,3}'

# recognize sentences containing a measurement verb and 1-3 measurements
str_measures_m_1 = r'(' + str_words_1_to_n + str_meas_verb + str_words_0_to_n + r'\bM\s*' + r')'
str_measures_m_2 = str_measures_m_1 + r'{2}'
str_measures_m_3 = str_measures_m_1 + r'{3}'

regex_measures_m_1 = re.compile(str_measures_m_1)
regex_measures_m_2 = re.compile(str_measures_m_2)
regex_measures_m_3 = re.compile(str_measures_m_3)

# now vs. then sentence template
str_now_vs_then_1 = r'\b((now|currently|today)/s+)?' + str_meas_qualifiers    +\
                    r'\b(measures?|measuring)\s+' + str_meas_qualifiers + r'\bM'
str_now_vs_then_21 = r'\b(compared\s+to|with)\s+' + str_meas_qualifiers + r'\bM\s+previously'
str_now_vs_then_22 = r'\b(previously|formerly)\s+(had\s+)?(measured|measuring)\s+' +\
                     str_meas_qualifiers + r'\bM'
str_now_vs_then_23 = r'\b(previously|formerly)\s+' + str_meas_qualifiers + r'\bM'
str_now_vs_then_24 = r'\bprior\s+measurement\s+of\s+' + str_meas_qualifiers + r'\bM'
str_now_vs_then_25 = r'\bM\s+' + str_meas_qualifiers + r'\b(then|on\s+date)'
str_now_vs_then_26 = r'\bM\s+' + str_meas_qualifiers + r'\b(in|on)\s+(the\s+)?prior'

str_now_vs_then_2  = r'(' + str_now_vs_then_21 + r'|' + str_now_vs_then_22    +\
                     r'|' + str_now_vs_then_23 + r'|' + str_now_vs_then_24    +\
                     r'|' + str_now_vs_then_25 + r'|' + str_now_vs_then_26 + r')'

regex_now_vs_then_1 = re.compile(str_now_vs_then_1);
regex_now_vs_then_2 = re.compile(str_now_vs_then_2);

# subjects before and after the measurements
str_before_and_after = r'\b((a|the)\s+)?' + r'(?P<subject1>' + str_words + r')' +\
                       str_meas_verb + str_meas_qualifiers + r'\bM\s+'        +\
                       str_words_0_to_n + r'\b(a|an)\s+M\s+'                  +\
                       r'(?P<subject2>' + str_words_g + r')'
regex_before_and_after = re.compile(str_before_and_after)

# M and M templates

str_m_and_m_1 = r'(?P<text1>' + str_words_0_to_n + str_meas_verb              +\
                str_meas_qualifiers + r'\bM\s+' + str_words_0_to_n + r')'     +\
                r'\band\s+' + r'(?P<text2>' + str_words_0_to_n                +\
                str_meas_qualifiers + r'\bM\s*' + str_words_g + r')'
regex_m_and_m_1 = re.compile(str_m_and_m_1)

# special form: {subject} measures M and {verb} to M
str_m_and_m_2 = r'(?P<text1>' + str_words_1_to_n + str_meas_verb             +\
                str_meas_qualifiers + r'\bM\s+' + str_words_0_to_n + r')'    +\
                r'\band\s+' + str_words_1_to_n + r'\bto\s+'                  +\
                str_meas_qualifiers + r'\bM\s*' + str_words_g
regex_m_and_m_2 = re.compile(str_m_and_m_2)
        
# 'a M {words}' and 'a {words} M' templates

str_a_m_terminations = r'(?=\b(a|an|measure[ds]?|measuring|M)\b)'

# match constructs such as "a M node in the right low paratracheal station",
# either alone or in a list
str_a_m_wds_1 = r'\b(an?|the)\s+' + str_meas_qualifiers + r'\bM\s+'          +\
                r'(?P<wordsNG>' + str_words_1_to_n + r')'                    +\
                str_a_m_terminations
str_a_m_wds_2 = r'\b(an?|the)\s+' + str_meas_qualifiers + r'\bM\s+'          +\
                r'(?P<wordsG>' + str_words_g + r')'
str_a_m_wds = r'(' + str_a_m_wds_1 + r'|' + str_a_m_wds_2 + r')'
regex_a_m_wds = re.compile(str_a_m_wds)

# match constructs such as "an unchanged M hyperechoic focus..."
str_a_wds_m_1 = r'\b(an?|the)\s+(?P<words1>' + str_words_1_to_n + r')'       +\
                str_meas_qualifiers + r'\bM\s+' + r'(?P<words2>'             +\
                str_words_1_to_n + r')' + str_a_m_terminations
str_a_wds_m_2 = r'\b(an?|the)\s+(?P<words3>' + str_words_1_to_n + r')'       +\
                str_meas_qualifiers + r'\bM\s+' + r'(?P<words4>'             +\
                str_words_g + r')'
str_a_wds_m = r'(' + str_a_wds_m_1 + r'|' + str_a_wds_m_2 + r')'
regex_a_wds_m = re.compile(str_a_wds_m)


# in largest dimension, in size, in largest axis, on inspiratory imaging, etc.
str_size_or_image = r'\b(in|on|with)\s+' + str_words_0_to_n                  +\
                    r'(\b(axis|diameter|dimension|imaging|length|width|'     +\
                    'height|size|area|vol(ume)?)\s*){1,2}(of\s*)?'
regex_size_or_image = re.compile(str_size_or_image)

# carina
str_endo_tube = r'\b(?P<endotube>(et|endo(tracheal)?)\s+tube)\s+'
str_carina = str_endo_tube + str_words_0_to_n + str_meas_qualifiers          +\
             r'\bM\s+(just\s+)?(above|from)\s+(the\s+)?carina'
regex_endo_tube = re.compile(str_endo_tube)
regex_carina = re.compile(str_carina)

# location finding regexes

str_loc_start = r'\b(above|abutting|adjacent\s+to|along|anterior\s+to|'      +\
                r'are\s+present\s+at|arising\s+from|at|below|caudad\s+to|'   +\
                r'cephalad\s+to|in\s+back\s+of|inferior\s+to|'               +\
                r'in\s+front\s+of|inside|in|near|(?<!seen\s)on|'             +\
                r'superior\s+to|throughout|(?<!compared\s)to|under|within)\s+'

# various quantifiers and other words
str_the = r'\b((all|some|the|this|that|those)\s+(?!is)(?!are))?'

str_loc_terminators = r'(?=\b(above|abutting|adjacent|along|are|arising|' +\
                            r'at|below|in|is|near|now|on(?!\sthe)|' +\
                            r'of(?!\sthe)|shows|there|throughout|' +\
                            r'to(?!\slikely)|under|within|with|where|' +\
                            r'which|not|like)\b)'

# check appended space on string to be matched
str_loc_ng = str_loc_start + str_the +\
             r'(?P<location1>' + str_words_1_to_n + r')' + str_loc_terminators
str_loc_g = str_loc_start + str_the +\
            r'(?P<location2>' + str_words_g + r')'
str_loc = r'(' + str_loc_ng + r'|' + str_loc_g + r')'
regex_loc = re.compile(str_loc)

# accept the location start word only if not followed by any of these terms
str_loc_disqualifiers = r'\b(axis|appearance|architecture|echogenicity|'     +\
                        r'presentation|diameter|dimension|size|area|volume|' +\
                        r'vol|length|width|height|likely|place|position)'
str_loc_disq = str_loc_start + str_words_0_to_n + str_loc_disqualifiers
regex_loc_disqualifier = re.compile(str_loc_disq)


# match ngram lengths (from the ngram file)
regex_length = re.compile(r'\A#\s+length:\s+(?P<num>\d+)')

# replacement for dates - capitalize? TBD
STR_DATE = r'date'

# prepositional dependencies, conjuncts, direct objects
IGNORE_DEPS = set(['acomp', 'attr', 'conj', 'dobj', 'pcomp', 'pobj', 'prep'])

# dependencies to ignore when resolving a general term
RESOLVE_DEPS = set(['acl', 'pcomp', 'pobj', 'prep'])

# prepositional dependencies
MEAS_VERB_DEPS = set(['dobj', 'pcomp', 'pobj', 'prep'])

# special handling for these deps preceded by 'with'
WITH_DEPS = set(['acl', 'pcomp', 'pobj', 'prep'])

# dependency relations for acceptable subject modifiers
MODIFIER_DEPS = set(['compound', 'amod', 'nmod', 'nummod'])

# general terms for tokens that need more specific resolution, if possible
GENERAL_TERMS = set(['area', 'volume', 'size', 'base', 'mass'])

# default ngram file
NGRAM_FILE = 'clarity_ngrams.txt'

ngram_dict = {}
ngram_min_chars = 9999999
ngram_word_counts = []

# displacy use flag
ENABLE_DISPLACY = False

# useful constants
EMPTY_STRING = ''

# nonempty string, means to not resolve a location for this measurement
LOCATION_RESOLVED = [' ']

# a value greater than the max number of measurements ever expected in a
# sentence; used in the function 'm_index_from_context'
INVALID_M_INDEX = 9999999


###############################################################################
def _enable_debug():

    global TRACE
    TRACE = True
    

###############################################################################
class Meas():
    """
    The measurement objects manipulated by this code.
    """

    # initialize with a SizeMeasurement namedtuple
    def __init__(self, sm):

        # from SizeMeasurement
        self.text         = sm.text
        self.start        = sm.start
        self.end          = sm.end
        self.temporality  = sm.temporality
        self.units        = sm.units
        self.condition    = sm.condition
        self.matchingTerm = []
        self.subject      = []
        self.location     = EMPTY_FIELD
        self.x            = sm.x
        self.y            = sm.y
        self.z            = sm.z
        self.values       = sm.values
        self.xView        = sm.xView
        self.yView        = sm.yView
        self.zView        = sm.zView
        self.minValue     = sm.minValue
        self.maxValue     = sm.maxValue

        
###############################################################################
class Chunk():
    """
    Groups of tokens that partition the sentence.
    """

    # this chunk spans Spacy token indices [start, end)
    def __init__(self, start, end, doc):
        self.start = start
        self.end   = end

        # concatenate all token strings, ignoring punctuation
        strings = [t.text for t in doc[start:end]]
        self.text = ' '.join(strings)

        
###############################################################################
def init(ngram_file_path=NGRAM_FILE):
    """
    Initialize this module. Must be called once, prior to sentence processing.
    """

    global ngram_word_counts

    ngram_dict.clear()

    load_ngram_file(ngram_file_path, ngram_dict)
    ngram_word_counts = sorted(ngram_dict.keys())

    if TRACE:
        print('ngram min chars: {0}'.format(ngram_min_chars))
        for n in range(1, len(ngram_word_counts)+1):
            print('Number of ngrams of length {0:2}: {1:6}'.format(n, len(ngram_dict[n])))
    
    # 'measures' is a 3rd person singular present verb
    special_case = [{ORTH: u'measures', LEMMA: u'measure', TAG: u'VBZ', POS: u'VERB'}]
    nlp.tokenizer.add_special_case(u'measures', special_case)

    # 'measure' is a non 3rd person singular present verb
    special_case = [{ORTH: u'measure', LEMMA: u'measure', TAG: u'VBP', POS: u'VERB'}]
    nlp.tokenizer.add_special_case(u'measure', special_case)

    # 'measured' is a verb, past participle
    special_case = [{ORTH: u'measured', LEMMA: u'measure', TAG: u'VBN', POS: u'VERB'}]
    nlp.tokenizer.add_special_case(u'measured', special_case)

    # 'measuring' is a verb form, either a gerund or present participle
    special_case = [{ORTH: u'measuring', LEMMA: u'measure', TAG: u'VBG', POS: u'VERB'}]
    nlp.tokenizer.add_special_case(u'measuring', special_case)    

    
###############################################################################
def load_ngram_file(filepath, ngram_dict):
    """
    Load the list of medical ngrams, which are used to simplify sentences
    via replacement of ngrams with simple nouns.
    """

    global ngram_min_chars

    path = os.path.join(FILE_DIR, "%s" % NGRAM_FILE)    
    with open(path, 'rt') as fin:
        
        n = -1
        for line in fin:

            if line.startswith('#'):
                # read the ngram length
                match = regex_length.match(line)
                if match:
                    n = int(match.group('num'))
                else:
                    if TRACE:
                        print('Error reading {0}: length expected: {1}'.
                              format(NGRAM_FILE, line))
                    sys.exit(-1)
                ngram_dict[n] = []
                continue

            # append ngram of length n to its list
            ngram = line.rstrip()
            ngram_dict[n].append(ngram)

            char_count = len(ngram)
            if char_count < ngram_min_chars:
                ngram_min_chars = char_count

                
###############################################################################
def to_json(original_terms, original_sentence, measurements):
    """
    Convert the results to a JSON string.
    """

    result_dict = {}
    result_dict['sentence'] = original_sentence
    result_dict['measurementCount'] = len(measurements)
    result_dict['terms'] = original_terms

    # check for presence of query terms in the meas subjects
    terms_lc = [t.lower() for t in original_terms]

    # Determine if search terms match the subject. A match is declared if
    # a search term is a component of the subject string. For instance,
    # search term 'kidney' would match a subject of 'right kidney'.
    found_it = False
    for m in measurements:
        if len(m.matchingTerm) > 0:
            # indicates success of a simple match
            found_it = True
            break
        for t in terms_lc:
            # m.subject is a list of strings
            for subject_term in m.subject:
                if -1 != subject_term.find(t):
                    m.matchingTerm.append(t)
                    found_it = True
                    # no break, need to append all matches

    result_dict['querySuccess'] = found_it

    dict_list = []
    for m in measurements:
        m_dict = {}

        for field in MEASUREMENT_FIELDS:
            m_dict[field] = getattr(m, field)

        loc = m_dict['location']
        if LOCATION_RESOLVED == loc:
            m_dict['location'] = EMPTY_FIELD
        elif isinstance(loc, list) and 0 == len(loc):
            m_dict['location'] = EMPTY_FIELD

        dict_list.append(m_dict)
    
    # serialize everything to JSON
    result_dict['measurementList'] = dict_list
    return json.dumps(result_dict, indent=4)


###############################################################################
def log_token(token):
    """
    log useful token data to the screen for debugging.
    """

    print('[{0:3}]: {1:30}\t{2:6}\t{3:8}\t{4:12}\t{5}'.format(token.i,
                                                              token.text,
                                                              token.tag_,
                                                              token.pos_,
                                                              token.dep_,
                                                              token.head))

    
###############################################################################
def log_noun_chunks(doc):
    """
    log noun chunk data to the screen.
    """

    print('\nNoun chunks: ')
    print('{0:9}{1:32}\t{2:16}{3:12}{4:32}'.format('INDICES',
                                                   'TEXT',
                                                   'ROOT_TEXT',
                                                   'ROOT_DEP',
                                                   'ROOT_HEAD_TEXT'))
    index = 0
    for chunk in doc.noun_chunks:
        print('[{0:2},{1:2}): {2:32}\t{3:16}{4:12}{5:32}'.format(chunk.start,
                                                                 chunk.end,
                                                                 chunk.text,
                                                                 chunk.root.text,
                                                                 chunk.root.dep_,
                                                                 chunk.root.head.text))
        index += 1
    print()

    
###############################################################################
def log_tokens(doc):
    """
    log all tokens in a SpaCy document.
    """

    print('\nTokens: ')
    print('{0:7}{1:30}\t{2:6}\t{3:8}\t{4:12}\t{5}'.format('INDEX', 'TOKEN', 'TAG',
                                                          'POS', 'DEP', 'HEAD'))
    for token in doc:
        log_token(token)

    log_noun_chunks(doc)

    
###############################################################################
def erase(sentence, start, end):
    """
    Overwrite characters [start, end) with whitespace.
    """
    piece1 = sentence[:start]
    piece2 = ' '*(end-start)
    piece3 = sentence[end:]
    return piece1 + piece2 + piece3


###############################################################################
def collapse_ws(sentence):
    """
    Replace repeated whitespace chars with a sigle space.
    """

    return re.sub(r'\s+', ' ', sentence)


###############################################################################
def get_meas_count(sentence):
    """
    Count the number of measurements in the sentence or fragment.
    """

    count = 0
    for c in sentence:
        if 'M' == c:
            count += 1
    return count


###############################################################################
def replace_verbosity(sentence):
    """
    Replace verbose forms with simpler expressions, preserving sentence length.
    """

    for regex in verbosity_map:
        iterator = regex.finditer(sentence)
        for match in iterator:
            matching_text = match.group()
            num_chars = len(matching_text)

            # replacement word list associated with this regex
            word_list = verbosity_map[regex]

            # if multiple words, join with a single space
            if len(word_list) > 1:
                replacement = ' '.join(word_list)
            else:
                replacement = word_list[0]

            piece1 = sentence[:match.start()]
            piece2 = replacement
            piece3 = ' '*(num_chars - len(replacement))
            piece4 = sentence[match.end():]
            sentence = piece1 + piece2 + piece3 + piece4

    return sentence


###############################################################################
def replace_preserving_length(regex, sentence, str_new):
    """
    Replace all text matched by the regex without changing sentence length.
    """

    iterator = regex.finditer(sentence)
    for match in iterator:
        text = match.group()
        start = match.start()
        end   = match.end()
        
        piece1 = sentence[0:start]
        
        pad_char_count = (end - start - len(str_new))
        #assert pad_char_count >= 0

        # skip if lengths are wrong
        if pad_char_count < 0:
            continue
        
        piece2 = str_new    
        piece3 = ' ' * pad_char_count
        piece4 = sentence[end:]
        sentence = piece1 + piece2 + piece3 + piece4

    return sentence


###############################################################################
def clean_sentence(sentence):
    """
    Attempt to clean up the sentence to make the subject finder's task easier.
    Some of these transformations do NOT preserve the sentence length.
    """

    # convert to lowercase
    #sentence = sentence.lower()

    # erase image annotations
    iterator = regex_image.finditer(sentence)
    for match in iterator:
        sentence = erase(sentence, match.start(), match.end())

    # remove anything in [] or {}, such as anonymized dates
    sentence = replace_preserving_length(regex_brackets, sentence, STR_DATE)
        
    # erase parens from the sentence, but keep any text inbetween
    sentence = regex_parens.sub(' ', sentence)

    # erase "one month ago" and similar expressions
    iterator = regex_ago.finditer(sentence)
    for match in iterator:
        sentence = erase(sentence, match.start(), match.end())

    # erase blather
    for regex in blather_regexes:
        iterator = regex.finditer(sentence)
        for match in iterator:
            sentence = erase(sentence, match.start(), match.end())

    # replace verbosity with simpler expressions padded with spaces
    sentence = replace_verbosity(sentence)

    # replace liver segments in roman numerals with decimals
    iterator = regex_liver_seg.finditer(sentence)
    for match in iterator:
        text = match.group()
        segnum = match.group('segnum')
        start = match.start('segnum')
        end   = match.end('segnum')
        if segnum in liver_seg_map:
            decimal_num = liver_seg_map[segnum]
            piece1 = sentence[0:start]
            piece2 = decimal_num
            piece3 = ' '*(end - start - len(decimal_num))
            piece4 = sentence[end:]
            sentence = piece1 + piece2 + piece3 + piece4
            new_text = re.sub(segnum, decimal_num, text)
            replacements[new_text] = text
            if TRACE:
                print('\treplaced {0} with {1}'.format(segnum, decimal_num))
                print('\tnew match text: ->{0}<-'.format(new_text))
    
    # replace any problem adjectives with colors
    substitutions = []
    iterator = regex_problem_adjectives.finditer(sentence)
    for match in iterator:
        substitutions.append(match.group())

    for adj in substitutions:
        for color in COLORS:
            # if color not already in sentence, use as the replacement adj
            if -1 != sentence.find(color):
                sentence = re.sub(adj, color, sentence)
                replacements[color] = adj
                if TRACE: print("Replaced '{0}' with '{1}'".format(adj, color))
                break

    # replace roman numerals in narrative sections with numbers
    iterator = regex_roman_numeral.finditer(sentence)
    for match in iterator:
        text = match.group('numeral_text')
        start = match.start('numeral_text')
        end   = match.end('numeral_text')
        piece1 = sentence[0:start]
        digit = roman_numeral_map[text]
        piece2 = digit
        piece3 = ' '*(end - start - len(digit))
        piece4 = sentence[end:]
        sentence = piece1 + piece2 + piece3 + piece4
        if TRACE: print("Replaced '{0}' with '{1}'".format(text, digit))            

    # SpaCy sometimes gives dramatically different results depending on whether
    # semicolons are present in the sentence or not, so replace semicolons
    # with whitespace.
    sentence = re.sub(r';', r' ', sentence)
    
    return sentence


###############################################################################
def find_child_candidates(token):
    """
    Examine the syntactic children of the given node and find the child most 
    suitable to be a measurement subject.
    """

    if TRACE: print('find_child_candidates...')
    
    candidates = []
    
    for child in token.children:
        if TRACE: log_token(child)
        # want nouns or nsubj or superlative adjectives such as 'largest'
        if 'NOUN' != child.pos_ and 'nsubj' != child.dep_ and 'JJS' != child.tag_:
            continue
        elif 'M' == child.text:
            continue
        else:
            candidates.append(child)

    # if have both nouns and JJS, keep only the nouns
    pruned_candidates = []
    for c in candidates:
        if 'NOUN' == c.pos_ or 'nsubj' == c.dep_:
            pruned_candidates.append(c)

    if len(pruned_candidates) > 0:
        if TRACE: print('find_child_candidates: subject candidates were pruned')
        return pruned_candidates
    else:
        return candidates
            

###############################################################################
def resolve_superlative_adj(m_token, adj_token):
    """
    Determine what a superlative adjective such as 'largest' refers to.
    Return the result as a list.
    """
    
    # follow a chain of prepositions up the tree to a terminating noun

    root = None
    token = m_token.head
    subject_list = None

    if TRACE: print('resolve_superlative_adj: starting...')
    while True:
        if 'NOUN' == token.pos_ and not token.dep_ in RESOLVE_DEPS:
            if TRACE: print('\tresolved to noun: "{0}"'.format(token.text))
            subject_list = [token]
            break
        elif 'ROOT' == token.dep_:
            if TRACE: print('\tresolved to root: "{0}"'.format(token.text))
            root = token
            break
        else:
            if TRACE: print('\tskipping "{0}", continuing...'.format(token.text))
            token = token.head

    if root is not None:
        if TRACE: print('\tresolving to child candidates of root {0}'.format(root.text))
        subject_list = find_child_candidates(root)

    if 0 == len(subject_list):
        subject_list = [adj_token]
        
    return subject_list


###############################################################################
def get_modifiers_of_token(token):
    """
    Get all suitable 'left' modifiers of the given token. The token itself
    is not included in the 'lefts' set.
    """

    results = []
    for l in token.lefts:
        if l.dep_ in MODIFIER_DEPS:
            if 'M' != l.text:
                results.append(l)

    return results


###############################################################################
def get_modifiers(start_token):
    """
    Recursively get all 'left' modifiers of the start token.
    """

    if TRACE: print('get_modifiers...')

    results = []

    # use like a stack for depth-first traversal
    tokens = [start_token]

    # indices of nodes whose 'lefts' have been pushed
    indices = []
    
    # recurse depth-first through the dependency parse tree; will pick up
    # all modifiers of each token this way
    while len(tokens) > 0:
        # top of token stack
        top = tokens[-1]
        # if 'lefts' haven't already been pushed
        if top.i not in indices:
            lefts = get_modifiers_of_token(top)
            if TRACE: print('\tlefts: {0}'.format(lefts))
            indices.append(top.i)
            if len(lefts) > 0:
                # reverse the order to place the most distant modifier
                # at the top of the stack; the most distant mod comes
                # first in a sentence
                for i in reversed(range(len(lefts))):
                    tokens.append(lefts[i])
                    if TRACE: print('\tappended {0}'.format(lefts[i]))
        else:
            # no more mods for the token at the stack top, so pop it
            token = tokens.pop()
            results.append(token)

    # remove any duplicates
    no_dups = []
    for token in results:
        if token not in no_dups:
            no_dups.append(token)

    return no_dups


###############################################################################
def undo_substitutions(text):
    """
    Undo any word substitutions in the given text and return the restored text.
    """

    if 0 == len(replacements):
        return text

    # remove punctuation
    text = regex_punctuation.sub(' ', text)    

    words = text.split()
    for new_text, old_text in replacements.items():
        for i in range(len(words)):
            if words[i] == new_text:
                words[i] = old_text
    new_text= ' '.join(words)
    return new_text


###############################################################################
def extract_loc(text):
    """
    Use location-finding regexes to extract a location from the supplied text.
    """

    if TRACE:
        print("extract_loc: starting with text '{0}'".format(text))

    iterator = regex_loc.finditer(text)
    for match in iterator:
    #match = regex_loc.search(text)
    #if match:
        loc_text = match.group()

        # check for disqualification
        match_disq = regex_loc_disqualifier.search(loc_text)
        if match_disq:
            if TRACE:
                print('\tDISQUALIFIED')
            #return EMPTY_STRING
            continue
        
        loc1 = match.group('location1') # nongreedy regex
        loc2 = match.group('location2') # greedy regex

        # either the nongreedy or the greedy regex matched, but not both
        assert (loc1 is not None) ^ (loc2 is not None)

        if loc1 is not None:
            loc = loc1.strip()
        else:
            loc = loc2.strip()

        if TRACE:
            print("extract_loc: processing text '{0}'".format(loc))

        # remove punctuation
        loc = regex_punctuation.sub(' ', loc)
            
        # ignore any text that starts with 'to be' or 'be'
        match2 = re.search(r'\A(to\s+)?be\s', loc)
        if match2:
            continue

        # ignore a meas* verb at the end
        match2 = regex_meas_verb_no_space.search(loc)
        if match2:
            loc = loc[:match2.start()]
        
        # remove any terminating words that end in 'ly'
        match2 = re.search(r'\s[a-z]+ly\s*\Z', loc)
        if match2:
            loc = loc[:match2.start()]

        # remove various starting words
        match2 = re.search(r'\A\s*(and|is|its|or|the[ym]?|this|that|those)', loc)
        if match2:
            loc = loc[match2.end():]
        
        # remove various terminating words
        match2 = re.search(r'\s(and|is|its|or|the[ym]?|this|that|those)\s*\Z', loc)
        if match2:
            loc = loc[:match2.start()]

        str_loc_ignore = r'\b(image|imaging|views?|axis|size|dimension)\b'
        match2 = re.search(str_loc_ignore, loc)
        if match2:
            continue
            
        if TRACE:
            print('extract_loc: matching text: ->{0}<-'.format(loc_text))
            print('extract_loc:           loc: ->{0}<-'.format(loc))
            
        return loc

    if TRACE: print('\tno location found')
    return EMPTY_STRING


###############################################################################
def find_location(subj_token, doc):
    """
    Attempt to find an anatomical location for the measurement subject. Use
    the same sentence fragment (doc) that contains the subject and measurement.
    """

    if doc is None or 0 == len(doc):
        return EMPTY_STRING

    # collect all text from tokens starting after the subject token
    # ignore punctuation, stop if meas verb, M, or end of doc
    texts = []
    token_indices = []

    m_token_index = -1
    meas_token_index = -1
    
    for i in range(subj_token.i+1, len(doc)):
        if 'PUNCT' == doc[i].pos_:
            continue

        text = doc[i].text
        if regex_meas_verb_no_space.match(text):
            meas_token_index = i
            break
        elif 'M' == text:
            m_token_index = i
            break
        else:
            texts.append(text)
            token_indices.append(i)

    if 0 == len(texts):

        if TRACE:
            print('find_location: nothing between, trying in front')
            
        # collect all text from the start of the sentence to the subject token
        if subj_token.i < len(doc): # subj token might not be in fragment
            for i in range(0, subj_token.i):
                if 'PUNCT' == doc[i].pos_:
                    continue
                texts.append(doc[i].text)
                token_indices.append(i)

    if 0 == len(texts):

        if TRACE:
            print('find_location: nothing between or in front, trying at end')
            
        if -1 != meas_token_index or -1 != m_token_index:
            index = max(meas_token_index, m_token_index)
            for i in range(index+1, len(doc)):
                if 'PUNCT' == doc[i].pos_:
                    continue
                texts.append(doc[i].text)
                token_indices.append(i)

    if 0 == len(texts):
        if TRACE:
            print('find_location: nothing found')
        return EMPTY_STRING
                
    # concat to a string for location regex search
    text = ' '.join(texts)

    # append a trailing space to satisfy a regex quirk
    text += ' '

    if TRACE:
        print('find_location text: ->{0}<-'.format(text))
    
    return extract_loc(text)
    

###############################################################################
def set_meas_locations(m_count, m_sentence, measurements, doc):
    """
    Attempt to find an anatomical location for each measurement.
    """
    
    # find location
    for m in measurements:

        # ignore if location already set
        if EMPTY_FIELD != m.location:
            continue
        
        location_strings = []
        for s in m.subject:
            this_loc = find_location(s, doc)
            if 0 == len(this_loc):
                continue
            
            # undo substitutions
            if len(replacements) > 0:
                words = this_loc.split()
                for new_text, old_text in replacements.items():
                    for i in range(len(words)):
                        if words[i] == new_text:
                            words[i] = old_text
                this_loc = ' '.join(words)
            
            if this_loc not in location_strings:
                location_strings.append(this_loc)

        if 0 == len(location_strings) and 1 == m_count:

            if TRACE:
                print('\tNo location found, trying fragment match.')

            # join the tokens with special handling for punctuation
            doc_strings = []
            for token in doc:
                if 'PUNCT' != token.pos_:
                    doc_strings.append(token.text)
                elif len(doc_strings) > 0:
                    doc_strings[-1] += token.text
                # else doc[0] is a PUNCT token, so just ignore it
            doc_text = ' '.join(doc_strings)
            
            pos = m_sentence.find(doc_text)
            if -1 != pos:
                frag = erase(m_sentence, pos, pos + len(doc_text))
                frag = collapse_ws(frag)
                this_loc = extract_loc(frag)
                if len(this_loc) > 0:
                    location_strings.append(this_loc)
                
        m.location = location_strings


###############################################################################
def get_chunks(doc):
    """
    Partition the sentence into Chunk objects.
    """

    chunks = []

    start = 0
    chunk_index = 0
    for nc in doc.noun_chunks:        
        nc_start = nc.start
        nc_end   = nc.end
        chunk = Chunk(start, nc_end, doc)
        chunks.append(chunk)
        start = nc_end
        chunk_index += 1
        if start >= len(doc):
            break

    if start < len(doc):
        chunks.append( Chunk(start, len(doc), doc))
    
    if TRACE:
        print('\nFULL CHUNKS: ')
        for chunk in chunks:
            print('[{0:2},{1:2})\t{2}'.format(chunk.start, chunk.end, chunk.text))

    return chunks


###############################################################################
def get_ref_chunk_indices(chunks, doc):
    """
    Find the chunk containing the meas* verb and the measurement token 'M'.
    Return the chunk indices as a tuple, using -1 if not found, in the order
    (meas_chunk_index, m_chunk_index).
    """

    m_chunk_index    = -1
    meas_chunk_index = -1

    chunk_index = 0
    for c in chunks:

        # check for meas* verb
        if -1 == meas_chunk_index and -1 != c.text.find('meas'):
            for token in doc[c.start:c.end]:
                if 'VERB' == token.pos_ and -1 != token.text.find('meas'):
                    meas_chunk_index = chunk_index
                    break

        # check for 'M'
        if -1 == m_chunk_index and -1 != c.text.find('M'):
            for token in doc[c.start:c.end]:
                if -1 != token.text.find('M'):
                    m_chunk_index = chunk_index
                    break

        if -1 != meas_chunk_index and -1 != m_chunk_index:
            break
        else:
            chunk_index += 1

    return (meas_chunk_index, m_chunk_index)


###############################################################################
def resolve_subject(doc, subj_list, chunks):
    """
    Try to choose a preferred subject from a list of candiates.
    """

    if TRACE:
        print('resolve_subject: initial subject list: {0}'.format(subj_list))
    
    if chunks is None or 0 == len(chunks):
        if TRACE: print('resolve_subject: NO CHUNKS')
        return []

    # nothing to resolve if only a single subject candidate
    if 1 == len(subj_list):
        return subj_list

    # if a general term, prefer another subject
    subj_list_2 = [token for token in subj_list if not token.text in GENERAL_TERMS]
    num_subjs_2 = len(subj_list_2)

    if 0 == num_subjs_2:
        # unresolvable; all are general terms
        return subj_list
    elif 1 == num_subjs_2:
        # only one non-general subject remains, use it
        return subj_list_2
    else:
        # use the reduced list
        subj_list = subj_list_2
        
    if TRACE:
        print('resolve_subject: pruned subject list: {0}'.format(subj_list))

    # find the chunk containing the meas* verb, if any, and the M
    meas_chunk_index, m_chunk_index = get_ref_chunk_indices(chunks, doc)
    
    if TRACE:
        print('resolve_subject: meas chunk index: {0}'.format(meas_chunk_index))
        print('resolve_subject: M    chunk index: {0}'.format(m_chunk_index))

    # take the subject closest to the meas verb and the M
    subj_chunk_indices = []
    for subj_token in subj_list:
        # token index of this candidate subject
        s_index = subj_token.i

        # find chunk index containing this subject
        chunk_index = 0
        for c in chunks:
            if s_index >= c.start and s_index < c.end:
                subj_chunk_indices.append(chunk_index)
                break
            else:
                chunk_index += 1
    if TRACE:
        for i in range(len(subj_chunk_indices)):
            subject = subj_list[i]
            chunk_index = subj_chunk_indices[i]
            print("resolve_subject: subj '{0}' is in chunk index {1}".format(subject, chunk_index))

    # compute distances from subject to meas chunk, if any; keep closest subj
    min_dist = 999999
    subj_index = -1
    if -1 != meas_chunk_index:
        for i in range(len(subj_chunk_indices)):
            index = subj_chunk_indices[i]
            dist = abs(meas_chunk_index - index)
            if dist < min_dist:
                min_dist = dist
                subj_index = i

    elif -1 != m_chunk_index:
        for i in range(len(subj_chunk_indices)):
            index = subj_chunk_indices[i]
            dist = abs(m_chunk_index - index)
            if dist < min_dist:
                min_dist = dist
                subj_index = i

    if TRACE:
        print('resolve_subject: preferred subject index: {0}'.format(subj_index))
                
    if -1 != subj_index:
        return [subj_list[subj_index]]
    else:
        return subj_list
    
        
###############################################################################
def get_meas_subj(m_token_index, doc):
    """
    Use information in the dependency parse tree to find what's being measured.
    """

    root = None
    do_child_search = False

    if TRACE: print('get_meas_subj start...')

    # If the current M token is not its own head, start there. If it is its
    # own head, walk backwards through the token list and find the nearest
    # verb and start there. If that fails, see if the M token has a
    # child with a 'compound' or 'nsubj' dependency and take that.
    m_token = doc[m_token_index]
    if m_token.head != m_token:
        token = m_token.head
    else:
        found_it = False
        for i in reversed(range(m_token_index)):
            token = doc[i]
            if 'VERB' == token.pos_:
                found_it = True
                break
        if not found_it:
            if TRACE: print('\tno verb found, trying nsubj or compound child...')
            for child in m_token.children:
                if 'compound' == child.dep_ or 'nsubj' == child.dep_:
                    token = child
                    found_it = True
                    if TRACE: print('\tfound compound or nsubj child')
                    break
        if not found_it:
            if TRACE: print('\treturning empty subject...')
            return []
        
    prev_dep = m_token.dep_

    # any other candidate subjects
    extra_candidates = []

    if TRACE:
        print('\tstarting token: {0}'.format(token.text))
    
    while True:
        if TRACE: log_token(token)
        if 'ROOT' == token.dep_:
            # subject of measurement is a child of this node
            if TRACE: print('\tat root token')
            root = token
            break
        elif 'VERB' == token.pos_:
            if 'nsubj' == token.dep_:
                if TRACE: print('\tverb with nsubj dep')
                break
            elif -1 != token.text.find('meas'):
                # measurement verb
                if token.head and 'NOUN' == token.head.pos_ and token.head.dep_ not in MEAS_VERB_DEPS:
                    if TRACE: print('\tbreak on meas verb')
                    token = token.head
                    break
                elif token.head and token.head.text == 'with':
                    if TRACE: print('\texit at with prior to meas verb')
                    do_child_search = True
                    break
        elif 'NOUN' == token.pos_ and prev_dep in WITH_DEPS:
            if token.head and 'with' == token.head.text:
                # special case - break on 'with'
                if TRACE: print('\textra candidate at check-with: {0}'.format(token))
                extra_candidates.append(token)
                prev_dep = token.dep_
                token = token.head
                continue
                #break
            elif 'dobj' == token.dep_:
                if TRACE: print('\tbreak on dobj at check-with')
                break
        elif 'NOUN' == token.pos_ and token.dep_ in IGNORE_DEPS:
            # this noun is part of an ignorable dependency
            if TRACE: print('\tcontinue at NOUN and ignorable dep: "{0}"'.format(token.dep_))
            prev_dep = token.dep_
            token = token.head
            continue
        elif 'NOUN' == token.pos_ and 'acl' == prev_dep:
            # prevous deps modify this noun, candidate subject of meas
            if TRACE: print('\tbreak at NOUN and acl dep')
            break
        elif token.dep_ in IGNORE_DEPS:
            if TRACE: print('\tcontinue at ingorable dep: "{0}"'.format(token.dep_))
            prev_dep = token.dep_
            token = token.head
            continue
        elif 'NOUN' != token.pos_:
            if TRACE: print('\tcontinue at POS == "{0}", not NOUN'.format(token.pos_))
            prev_dep = token.dep_
            token = token.head
            continue
        else:
            if TRACE: print('\tdefault break')
            break

        if TRACE: print('\tcontinuing up the tree')
        prev_dep = token.dep_
        token = token.head

    candidates = []

    if root is not None:
        # subject is potentially either the root or a child of the root
        if TRACE:
            print('\tget_meas_subj: root is not None')
        if 'NOUN' == root.pos_:
            if TRACE: print('\troot is a noun')
            candidates.append(root)
        else:
            if TRACE: print('\tcalling find_child_candidates')
            candidates = find_child_candidates(root)
    else:

        if token.text in GENERAL_TERMS:
            if TRACE:
                print('\tget_meas_subj: attempt resolution of general term "{0}"'.format(token.text))
            token = resolve_superlative_adj(m_token, token)[0]
            candidates.append(token)
        elif not do_child_search:
            if TRACE:
                print('\tget_meas_subj: appending token')
            candidates.append(token)
        elif do_child_search:
            candidates = find_child_candidates(token)

    if len(extra_candidates) > 0:
        candidates.extend(extra_candidates)
            
    return candidates


###############################################################################
def tokenize_and_find_subjects(sentence):
    """
    Analyze the sentence or sentence fragment with SpaCy and attempt to
    find all measurement subjects.
    """

    # save a copy of the original sentence
    original_sentence = sentence
    
    # tokenize and produce a dependency parse of the sentence
    doc = nlp(sentence)

    if TRACE:
        log_tokens(doc)
        
    if ENABLE_DISPLACY:
        displacy.serve(doc, style='dep')
        
    meas_subjects = []

    first_try = True
    num_tokens = len(doc)

    # find the M tokens and resolve the subject of each

    i = 0
    while i < num_tokens:
        tok = doc[i]
        if 'M' == tok.text:
            meas_subject = get_meas_subj(i, doc)

            # if no subject found, remove punctuation and try again
            if 0 == len(meas_subject) and first_try:
                if TRACE: print('no subject found, retrying...')
                sentence2 = regex_punctuation.sub(' ', sentence)
                doc = nlp(sentence2)
                i = 0
                num_tokens = len(doc)
                first_try = False
                continue

            # if subject is a single superlative adjective, find what it refers to
            if 1 == len(meas_subject):
                if 'JJS' == meas_subject[0].tag_:
                    if TRACE:
                        print('resolving superlative adj: {0}'.format(meas_subject[0]))
                    meas_subject = resolve_superlative_adj(tok, meas_subject[0])
                    
            meas_subjects.append(meas_subject)
            
        i += 1
    
    return (doc, meas_subjects)


###############################################################################
def flatten(l):
    """
    Non-recursive list flattener from
    http://rightfootin.blogspot.com/2006/09/more-on-python-flatten.html,
    based on code from Mike Fletcher's BasicTypes library.
    """
    
    l = list(l)
    i = 0
    while i < len(l):
        while isinstance(l[i], list):
            if not l[i]:
                l.pop(i)
                i -= 1
                break
            else:
                l[i:i + 1] = l[i]
        i += 1
        
    return list(l)


###############################################################################
def _find_simple_subjects(terms, sentence, measurements):
    """
    Find <term><separator><measurement> instances, where the separator is
    either whitespace or a colon or dash character, with optional spaces on
    either side.
    
    Example:   "LV V1 VTI: 16.0 cm"
    """

    results = []
    candidates = []
    
    if TRACE:
        print('Called _find_simple_subjects...')
        print('\t   terms: {0}'.format(terms))
        print('\tsentence: {0}'.format(sentence))
        for m in measurements:
            print('\t    meas: {0}'.format(m.text))

    # check for <term><separator><measurement>
    for term in terms:
        match = re.search(r'\b{0}\b'.format(term), sentence)
        if match:
            term_start = match.start()
            # find the index of the measurement nearest to the end of the term
            term_end = term_start + len(term)
            index = -1
            min_distance = len(sentence)
            for i,m in enumerate(measurements):
                if m.start >= term_end:
                    distance = m.start - term_end
                    if distance < min_distance:
                        min_distance = distance
                        index = i
                        
            # check chars between term_end and start of closest measurement
            closest_meas_start = measurements[index].start
            text = sentence[term_end:closest_meas_start]
            # separator is at least one space, dash, colon, or equal sign
            match = re.match(r'\A[-:=\s]+\Z', text)
            if match:
                start = term_start
                end   = measurements[index].end
                c = overlap.Candidate(start,
                                      end,
                                      sentence[start:end],
                                      # save meas index and term, needed later
                                      other=(index,term))
                candidates.append(c)

    # remove overlapping candidates
    candidates = sorted(candidates, key=lambda x: x.end-x.start, reverse=True)
    pruned_candidates = overlap.remove_overlap(candidates, TRACE)

    # update the winning measurements
    resolved_meas_tuples = []    
    for pc in pruned_candidates:
        meas_index, matching_term = pc.other
        resolved_meas = copy.deepcopy(measurements[meas_index])
        resolved_meas.subject      = [matching_term]
        resolved_meas.matchingTerm = [matching_term]
        resolved_meas.location     = LOCATION_RESOLVED
        resolved_meas_tuples.append( (meas_index, resolved_meas) )

    # sort in ascending order of measurement index
    resolved_meas_tuples = sorted(resolved_meas_tuples, key=lambda x: x[0])

    if TRACE:
        for i,m in resolved_meas_tuples:
            print('\tFound simple result: {0} => {1}'.
                  format(m.matchingTerm, m.text))

    return resolved_meas_tuples
    

###############################################################################
def run(term_string, sentence, nosub=False, use_displacy=False):
    """
    Do the main work of this module.
    """

    global ENABLE_DISPLACY

    if use_displacy:
        ENABLE_DISPLACY = True

    replacements.clear()

    # save a copy of the original sentence, needed for JSON output
    original_sentence = sentence

    # if any search terms, split on comma into individual words
    if term_string and len(term_string) > 0:
        terms = term_string.split(',') # produces a list
        terms = [term.strip() for term in terms]
    else:
        terms = []

    # save a copy of the original terms, needed for JSON output
    original_terms = terms.copy()

    # convert terms and sentence to lowercase
    terms = [term.lower() for term in terms]
    sentence = sentence.lower()
    
    sentence = clean_sentence(sentence)

    # find all size measurements
    json_string = smf_run(sentence)
    json_data = json.loads(json_string)
    size_measurements = [SizeMeasurement(**m) for m in json_data]

    if TRACE:
        print('\n\nCalled subject_finder run()...')
        print('SizeMeasurements: ')
        for sm in size_measurements:
            print('\t{0}'.format(sm))

    # convert from immutable smf.SizeMeasurement namedtuple to mutable Meas
    measurements = [Meas(sm) for sm in size_measurements]
            
    # if no measurements then no measurement subjects
    if 0 == len(measurements):
        return to_json(original_terms, original_sentence, [])

    # attempt to resolve the simple constructs first
    resolved_meas_tuples = []
    if len(terms) > 0:
        resolved_meas_tuples = _find_simple_subjects(terms,
                                                sentence,
                                                measurements)

    # separate resolved tuple list into separate indices and measurement lists
    resolved_meas_indices = []
    resolved_measurements = []
    for meas_index, meas in resolved_meas_tuples:
        resolved_meas_indices.append(meas_index)
        resolved_measurements.append(meas)
        
    # take an early exit if possible
    num_resolved = len(resolved_meas_indices)
    if num_resolved == len(measurements) or num_resolved == len(terms):
        return to_json(original_terms, original_sentence, resolved_measurements)

    # replace measurement text with <space>M<space+>, preserves sentence length
    for i,m in enumerate(measurements):
        num_chars = len(m.text)
        piece1 = sentence[:m.start]
        piece2 = ' M' + ' '*(num_chars - 2)
        piece3 = sentence[m.end:]                        
        sentence = piece1 + piece2 + piece3
        
    # replace \s+ with a single space, could change sentence length
    sentence_ss = collapse_ws(sentence)

    # do ngram substitutions unless the 'nosub' flag is set
    if not nosub:
        sentence_ss = replace_ngrams(sentence_ss)
    
    # save a copy, used to find context later
    m_sentence = sentence_ss
    
    if TRACE:
        print('Sentence prior to analysis: ')
        print('\t{0}'.format(sentence_ss))
        
    meas_subjects = []        
    m_count = get_meas_count(sentence_ss)

    # try to find subject of each measurement
    ok = False
    if 3 == m_count:
         ok, doc = process_3(m_sentence, sentence_ss, measurements, resolved_meas_indices)
    elif 2 == m_count:
         ok, doc = process_2(m_sentence, sentence_ss, measurements, resolved_meas_indices)
    elif 1 == m_count:
        ok, doc = process_1(m_sentence, sentence_ss, measurements, resolved_meas_indices)

    if not ok:
        if TRACE: print('\tno subject found, using default')
        set_default_subject(m_sentence, sentence_ss, measurements)
    
    # remove any duplicated subject tokens
    for i,m in enumerate(measurements):
        if i in resolved_meas_indices:
            continue
        m.subject = [t for t in set(m.subject)]

    # compute chunks and use them to find a best candidate subject
    if ok and m_count > 0:
        chunks = get_chunks(doc)
        for i,m in enumerate(measurements):
            if i in resolved_meas_indices:
                continue
            subj_list = resolve_subject(doc, m.subject, chunks)
            m.subject = subj_list
            
    # attempt to find any missing locations, one last time...
    if 1 == m_count and doc is not None:
        set_meas_locations(m_count, m_sentence, measurements, doc)

    # add additional modifiers to each subject, now that locations have been found
    for i,m in enumerate(measurements):
        if i in resolved_meas_indices:
            continue
        meas_save = m.subject
        m.subject = []
        for s in meas_save:
            token_list = get_modifiers(s)
            m.subject.append(token_list)
            
    # undo ngram replacements and other substitutions for subjects and locations
    if len(replacements) > 0:
        for i,m in enumerate(measurements):
            if i in resolved_meas_indices:
                continue
            if EMPTY_FIELD == m.subject:
                continue
            new_subj_texts = []
            # m.subject is a list of spacy tokens
            for token_list in m.subject:
                subj_texts = [t.text for t in token_list]
                for new_text, old_text in replacements.items():
                    for i in range(len(subj_texts)):
                        if subj_texts[i] == new_text:
                            subj_texts[i] = old_text
                text = ' '.join(subj_texts)
                new_subj_texts.append(text)
            m.subject = new_subj_texts

    if len(replacements) > 0:
        for m in measurements:
            if EMPTY_FIELD == m.location or LOCATION_RESOLVED == m.location:
                continue
            new_loc_texts = []
            # m.location is a list of python strings
            for string_list in m.location:
                loc_texts = string_list.split()
                for new_text, old_text in replacements.items():
                    for i in range(len(loc_texts)):
                        if loc_texts[i] == new_text:
                            loc_texts[i] = old_text
                text = ' '.join(loc_texts)
                new_loc_texts.append(text)
            m.location = new_loc_texts

    # flatten the subject and location lists
    for i,m in enumerate(measurements):
        if i in resolved_meas_indices:
            continue
        if m.subject is not None:
            m.subject = flatten(m.subject)
            # convert from Spacy strings, if needed
            converted = []
            for s in m.subject:
                if isinstance(s, spacy.tokens.token.Token):
                    converted.append(s.text)
                else:
                    converted.append(s)
            m.subject = converted
            
        if m.location is not None:
            m.location = flatten(m.location)

    # overwrite resolved measurements
    for i,meas in resolved_meas_tuples:
        measurements[i] = meas
            
    # convert to a JSON result
    return to_json(original_terms, original_sentence, measurements)


###############################################################################
def set_default_subject(m_sentence, sentence_ss, measurements):
    """
    Find all nouns in the sentence as use them as the measurement subject.
    """

    # generate a dependency parse of the sentence
    doc = nlp(sentence_ss)

    noun_list = []
    for token in doc:
        if 'NOUN' == token.pos_:
            noun_list.append(token)

    # remove duplicates and keep lowercase words only (no 'M')
    # words could be abbreviations or contain hyphens
    noun_list = [n for n in set(noun_list) if re.match(r'[-.a-z]+', n.text)]

    for m in measurements:
        if LOCATION_RESOLVED == m.location:
            continue
        m.subject = noun_list.copy()

        
###############################################################################
def process_3(m_sentence, sentence, measurements, resolved_meas_indices):
    """
    Find subjects of three measurements.
    """

    if TRACE: print('called process_3')

    # check for three independent "measures M' clauses
    matcher_3 = regex_measures_m_3.search(sentence)
    if matcher_3:

        if TRACE: print('process_3: MEASURES_M_3 match')
        
        count = 0
        found_it = False
        prev_subject = []
        iterator = regex_measures_m_1.finditer(sentence)
        for match in iterator:

            match_text = match.group()
            if TRACE:
                print('process_3: text for iteration {0}: {1}'.
                      format(count, match_text))
            doc, subjects = tokenize_and_find_subjects(match_text)
            if TRACE: print('subjects for iteration: {0}: {1}'.
                            format(count, subjects))

            m_index = m_index_from_context(m_sentence, match_text, resolved_meas_indices)
            #assert m_index < len(measurements)
            if INVALID_M_INDEX != m_index and m_index < len(measurements):
            
                #assert len(subjects) > 0
                if subjects and subjects[0] and len(subjects[0]) > 0:
                    # found subject for this measurement
                    for s in subjects[0]:
                        measurements[m_index].subject.append(s)
                    prev_subject = subjects[0]

                    # look for location(s)
                    locations = []
                    if TRACE:
                        print("process_3: looking for loc for meas {0} in text '{1}'".format(m_index, match_text))
                    for s in subjects[0]:
                        loc = extract_loc(match_text)
                        if EMPTY_STRING != loc:
                            loc = undo_substitutions(loc)
                            if loc not in locations:
                                locations.append(loc)
                    if len(locations) > 0:
                        measurements[m_index].location = locations.copy()

                elif count > 0:
                    # no subjects found on this iteration, so use previous subject
                    for s in prev_subject:
                        measurements[m_index].subject.append(s)
                    found_it = True

                count += 1

        if found_it:
            return (True, doc)

    # try 'a M wds' and 'a wds M' forms...
    found_it, sentence, doc = process_a_m_wds(m_sentence, sentence, measurements, resolved_meas_indices)
    m_count = get_meas_count(sentence)
    if found_it and 0 == m_count:
        return (True, doc)

    found_it, sentence, doc = process_a_wds_m(m_sentence, sentence, measurements, resolved_meas_indices)
    m_count = get_meas_count(sentence)
    if found_it and 0 == m_count:
        return (True, doc)

    if 2 == m_count:
        return process_2(m_sentence, sentence, measurements, resolved_meas_indices)
    elif 1 == m_count:
        return process_1(m_sentence, sentence, measurements, resolved_meas_indices)
    else:
        return (False, None)

    
###############################################################################
def process_2(m_sentence, sentence, measurements, resolved_meas_indices):
    """
    Find subjects of two measurements.
    """

    if TRACE: print('called process_2')
    
    # check for a now-vs-then sentence form
    matcher_nvt1 = regex_now_vs_then_1.search(sentence)
    if matcher_nvt1:
        if TRACE: print('process_2: NVT1 match')
        text2 = sentence[matcher_nvt1.end():]
        matcher_nvt2 = regex_now_vs_then_2.search(text2)
        if matcher_nvt2:
            if TRACE: print('process_2: NVT2 match')

            text1 = sentence[0:matcher_nvt1.end()]
            doc, subjects = tokenize_and_find_subjects(text1)

            m_index = m_index_from_context(m_sentence, text1, resolved_meas_indices)
            #assert m_index < len(measurements) - 1
            if INVALID_M_INDEX != m_index and m_index < len(measurements)-1:

                if subjects and len(subjects) > 0 and len(subjects[0]) > 0:
                    # found a subject for this measurement and the next
                    for s in subjects[0]:
                        measurements[m_index + 0].subject.append(s)
                        measurements[m_index + 1].subject.append(s)

                    # this is also the subject for the second measurement

                    # set temporality of 2nd measurement to PREVIOUS
                    measurements[m_index + 1].temporality = STR_PREVIOUS

                    # find remaining locations, if any
                    set_meas_locations(2, m_sentence, measurements, doc)
                    return (True, doc)
            
    # check for two independent 'measures M' clauses
    matcher_2 = regex_measures_m_2.search(sentence)
    if matcher_2:

        if TRACE: print('process_2: MEASURES_M_2 match')
        
        count = 0
        found_it = False
        prev_subject = []
        iterator = regex_measures_m_1.finditer(sentence)
        for match in iterator:

            match_text = match.group()
            if TRACE:
                print('process_2: text for iteration {0}: {1}'.
                      format(count, match_text))
            doc, subjects = tokenize_and_find_subjects(match_text)
            if TRACE: print('subjects for iteration: {0}: {1}'.
                            format(count, subjects))

            m_index = m_index_from_context(m_sentence, match_text, resolved_meas_indices)
            #assert m_index < len(measurements)
            if INVALID_M_INDEX != m_index and m_index < len(measurements):
            
                if subjects and subjects[0] and len(subjects[0]) > 0:
                    # found a subject for this measurement
                    for s in subjects[0]:
                        measurements[m_index].subject.append(s)
                    prev_subject = subjects[0]

                    # look for location(s)
                    locations = []
                    if TRACE:
                        print("process_2: looking for loc for meas {0} in text '{1}'".
                              format(m_index, match_text))
                    for s in subjects[0]:
                        loc = extract_loc(match_text)
                        if EMPTY_STRING != loc:
                            loc = undo_substitutions(loc)
                            if loc not in locations:
                                locations.append(loc)
                    if len(locations) > 0:
                        measurements[m_index].location = locations.copy()

                elif count > 0:
                    # no subjects found on this iteration, so use previous subject
                    for s in prev_subject:
                        measurements[m_index].subject.append(s)
                    found_it = True
                    
            count += 1

        if found_it:
            return (True, doc)

    # check for subjects before and after the measurements
    matcher_ba = regex_before_and_after.search(sentence)
    if matcher_ba:

        if TRACE: print('process_2: BA match')

        # find the measurement subject up to the first M
        m_pos = sentence.find('M')
        text1 = sentence[0:m_pos+1]
        doc, subjects = tokenize_and_find_subjects(text1)
        m_index = m_index_from_context(m_sentence, text1, resolved_meas_indices)
        #assert m_index < len(measurements)-1
        if INVALID_M_INDEX != m_index and m_index < len(measurements)-1:
            if subjects and subjects[0] and len(subjects[0]) > 0:
                for s in subjects[0]:
                    measurements[m_index].subject.append(s)

                locations = []
                for s in subjects[0]:
                    loc = extract_loc(text1)
                    if EMPTY_STRING != loc:
                        loc = undo_substitutions(loc)
                        if loc not in locations:
                            locations.append(loc)
                if len(locations) > 0:
                    measurements[m_index].location = locations.copy()
            else:
                return (False, None)

            ## find the subject in the text after the first M
            m_pos = sentence.find('M', m_pos+1)
            text2 = sentence[m_pos:]
            doc, subjects = tokenize_and_find_subjects(text2)
            if subjects and subjects[0] and len(subjects[0]) > 0:
                for s in subjects[0]:
                    measurements[m_index+1].subject.append(s)

                locations = []
                for s in subjects[0]:
                    loc = extract_loc(matcher_ba.group('subject2'))
                    if EMPTY_STRING != loc:
                        loc = undo_substitutions(loc)
                        if loc not in locations:
                            locations.append(loc)
                if len(locations) > 0:
                    measurements[m_index+1].location = locations.copy()

            else:
                return (False, None)

            return (True, doc)

    # check for 'M and M' forms
    matcher_m_and_m = regex_m_and_m_2.search(sentence)
    if matcher_m_and_m:
        if TRACE: print('process_2: M AND M 2 match')

        text1 = matcher_m_and_m.group('text1')
        doc, subjects = tokenize_and_find_subjects(text1)
        if TRACE: print('SUBJECTS 1: {0}'.format(subjects))
        m_index = m_index_from_context(m_sentence, text1, resolved_meas_indices)
        #assert m_index < len(measurements) - 1
        if INVALID_M_INDEX != m_index and m_index < len(measurements)-1:
            if subjects and subjects[0] and len(subjects[0]) > 0:
                for s in subjects[0]:
                    measurements[m_index].subject.append(s)

                # extract text up to the subject token for loc search
                locations = []
                for s in subjects[0]:
                    pos = text1.find(s.text)
                    loc = extract_loc(text1[:pos])
                    if EMPTY_STRING != loc:
                        loc = undo_substitutions(loc)
                        if loc not in locations:
                            locations.append(loc)
                if len(locations) > 0:
                    measurements[m_index].location = locations.copy()

            else:
                return (False, None)

            # this form has the same subject for both measurements
            for s in subjects[0]:
                measurements[m_index + 1].subject.append(s)
            measurements[m_index + 1].location = measurements[m_index].location

            return (True, doc)
    
    matcher_m_and_m = regex_m_and_m_1.search(sentence)
    if matcher_m_and_m:
        if TRACE: print('process_2: M AND M 1 match')

        text1 = matcher_m_and_m.group('text1')
        doc, subjects = tokenize_and_find_subjects(text1)
        if TRACE: print('\tSUBJECTS 1: {0}'.format(subjects))
        m_index = m_index_from_context(m_sentence, text1, resolved_meas_indices)
        #assert m_index < len(measurements) - 1
        if INVALID_M_INDEX != m_index and m_index < len(measurements)-1:
            if subjects and subjects[0] and len(subjects[0]) > 0:
                for s in subjects[0]:
                    measurements[m_index].subject.append(s)
                prev_subject = subjects[0]

                locations = []
                for s in subjects[0]:
                    loc = extract_loc(text1)
                    if EMPTY_STRING != loc:
                        loc = undo_substitutions(loc)
                        if loc not in locations:
                            locations.append(loc)
                if len(locations) > 0:
                    measurements[m_index].location = locations.copy()
                prev_loc = locations

            else:
                return (False, None)

            text2 = matcher_m_and_m.group('text2')
            doc, subjects2 = tokenize_and_find_subjects(text2)
            if TRACE: print('\tSUBJECTS 2: {0}'.format(subjects2))

            if subjects2 and subjects2[0] and len(subjects2[0]) > 0:
                # found next subject
                for s in subjects2[0]:
                    measurements[m_index + 1].subject.append(s)

                locations = []
                for s in subjects[0]:
                    loc = extract_loc(text2)
                    if EMPTY_STRING != loc:
                        loc = undo_substitutions(loc)
                        if loc not in locations:
                            locations.append(loc)
                if len(locations) > 0:
                    measurements[m_index + 1].location = locations.copy()

                prev_loc = locations

            elif prev_subject: #len(prev_+subject) >= 1:
                # no subjects found, so duplicate the previous subject and location
                if TRACE: print('\tusing previous subject')
                for s in prev_subject:
                    measurements[m_index + 1].subject.append(s)
                measurements[m_index + 1].location = prev_loc
            else:
                # no subjects found
                return (False, None)

            # find remaining locations, if any
            #set_meas_locations(2, m_sentence, measurements, doc)

            return (True, doc)

    # try 'a M wds' and 'a wds M' forms...
    found_it, sentence, doc = process_a_m_wds(m_sentence, sentence, measurements, resolved_meas_indices)
    m_count = get_meas_count(sentence)
    if found_it and 0 == m_count:
        return (True, doc)

    found_it, sentence, doc = process_a_wds_m(m_sentence, sentence, measurements, resolved_meas_indices)
    m_count = get_meas_count(sentence)
    if found_it and 0 == m_count:
        return (True, doc)

    if 2 == m_count:
        return (False, None)
    else:
        return process_1(m_sentence, sentence, measurements, resolved_meas_indices)

    
###############################################################################
def m_index_from_context(m_sentence, match_text, resolved_meas_indices):
    """
    Given a text string 'match_text', search 'm_sentence' for it and find
    which measurement the match_text is associated with.
    """

    start = m_sentence.find(match_text)
    if -1 == start:
        text = match_text
        while -1 == start:
            # remove one word at a time and repeat the search
            pos = text.find(' ')
            if -1 == pos:
                # big problem: match_text not contained in m_sentence
                return INVALID_M_INDEX
            else:
                text = text[pos+1:]
                start = m_sentence.find(text)

    if -1 == start:
        return INVALID_M_INDEX
    end = start + len(match_text)

    # the desired m_index is the index of the 'M' between [start, end)
    
    iterator = re.finditer(r'\bM\b', m_sentence)

    index = 0
    for match in iterator:
        if match.start() >= start and match.end() <= end:
            break
        else:
            index += 1

    if index in resolved_meas_indices:
        return INVALID_M_INDEX
            
    # this value might be equal to len(measurements) - need to check
    # prior to use
    return index


###############################################################################
def process_1(m_sentence, sentence, measurements, resolved_meas_indices):
    """
    Find the subject of a sentence (or sentence fragment) containing a 
    single measurement.
    """

    if TRACE: print('called process_1')
    
    match = regex_carina.search(sentence)
    if match:
        if TRACE: print('\tprocess_1: carina match')

        text = match.group()
        m_index = m_index_from_context(m_sentence, text, resolved_meas_indices)
        if INVALID_M_INDEX != m_index and m_index < len(measurements):

            texts = [sentence, text]
            for t in texts:
                doc, subjects = tokenize_and_find_subjects(t)
                if len(subjects) > 0 and len(subjects[0]) > 0:

                    # if multiple candiate subjects, prefer 'tube', 'et', or 'endo'
                    # to anything else
                    preferred_index = -1
                    if len(subjects[0]) > 0:
                        for i in range(len(subjects[0])):
                            s = subjects[0][i]
                            if -1 != s.text.find('tube') or \
                               -1 != s.text.find('et') or   \
                                     -1 != s.text.find('endo'):
                                preferred_index = i
                                if TRACE: print('process_1: found preferred tube subject')
                                break

                        if -1 != preferred_index:
                            subjects[0] = [subjects[0][preferred_index]]
                
                    for s in subjects[0]:
                        measurements[m_index].subject.append(s)
                    return (True, doc)

                # no subject found, so use endo tube group text
                if TRACE: print('\tprocess_1: using endodube group text')
                measurements[m_index].subject.append(match.group('endotube'))
                return (True, doc)
    
    match = regex_measures_m_1.search(sentence)
    if match:        
        text = match.group()

        if TRACE:
            print('process_1 matching text: {0}'.format(text))

        m_index = m_index_from_context(m_sentence, text, resolved_meas_indices)
        if INVALID_M_INDEX != m_index and m_index < len(measurements):
            
            # subjects is a list of lists
            doc, subjects = tokenize_and_find_subjects(text)
            if len(subjects) > 0 and len(subjects[0]) > 0:
                # found a subject for this measurement
                for s in subjects[0]:
                    measurements[m_index].subject.append(s)
                return (True, doc)

    # try 'a M wds' and 'a wds M' forms...
    found_it, sentence, doc = process_a_m_wds(m_sentence, sentence, measurements, resolved_meas_indices)
    if found_it:
        return (True, doc)

    found_it, sentence, doc = process_a_wds_m(m_sentence, sentence, measurements, resolved_meas_indices)
    if found_it:
        return (True, doc)

    # try to tokenize whatever is left
    if TRACE:
        print('process_1 text: {0}'.format(sentence))

    m_index = m_index_from_context(m_sentence, sentence, resolved_meas_indices)
    if INVALID_M_INDEX != m_index and m_index < len(measurements):
        
        doc, subjects = tokenize_and_find_subjects(sentence)
        if len(subjects) > 0 and len(subjects[0]) > 0:
            for s in subjects[0]:
                measurements[m_index].subject.append(s)

            return (True, doc)
    
    return (False, None)


###############################################################################
def process_a_m_wds(m_sentence, sentence, measurements, resolved_meas_indices):
    """
    Try to match regex_a_m_wds to the sentence or fragment and derive a
    measurement subject from it.
    """

    if TRACE: print('called process_a_m_wds')
    
    found_subject = False

    match = regex_a_m_wds.search(sentence)
    if not match:
        return (False, sentence, None)

    # either group 'wordsNG' or group 'wordsG' matches, but not both
    text1 = match.group('wordsNG')
    text2 = match.group('wordsG')
    assert (text1 is not None) ^ (text2 is not None)                                   

    if text1 is not None:
        matching_text = text1
    else:
        matching_text = text2

    msi = regex_size_or_image.search(matching_text)
    if msi:
        matching_text = erase(matching_text, msi.start(), msi.end())

    # restore the 'M' and search for subjects
    matching_text = 'M ' + matching_text
    matching_text = collapse_ws(matching_text)

    if TRACE:
        print('\tA M WDS matching_text: ->{0}<-'.format(matching_text))

    #found_it = process_1(m_sentence, matching_text, measurements)
    text = match.group()
    m_index = m_index_from_context(m_sentence, text, resolved_meas_indices)
    if INVALID_M_INDEX != m_index and m_index < len(measurements):    

        # subjects is a list of lists
        texts = [matching_text, text]
        for t in texts:
            doc, subjects = tokenize_and_find_subjects(t)
            if len(subjects) > 0 and len(subjects[0]) > 0:
                # found a subject for this measurement
                for s in subjects[0]:
                    measurements[m_index].subject.append(s)
                found_subject = True
                if TRACE:
                    print('\tSUBJECT: {0}'.format(subjects[0]))

                # erase matching text from sentence
                sentence = erase(sentence, match.start(), match.end())
                sentence = collapse_ws(sentence)

                if found_subject:

                    locations = []
                    for s in subjects[0]:
                        loc = extract_loc(t)
                        if EMPTY_STRING != loc:
                            loc = undo_substitutions(loc)
                            if loc not in locations:
                                locations.append(loc)
                    if len(locations) > 0:
                        measurements[m_index].location = locations.copy()
                    break

            if TRACE:
                if not found_subject:
                    print('\tno subject found with first text, trying again...')

    if found_subject:
        return (True, sentence, doc)
    else:
        return (False, sentence, None)

            
###############################################################################
def process_a_wds_m(m_sentence, sentence, measurements, resolved_meas_indices):
    """
    Try to match regex_a_wds_m to the sentence or fragment and derive a
    measurement subject from it.
    """

    if TRACE: print('called process_a_wds_m')
    
    found_subject = False

    match = regex_a_wds_m.search(sentence)
    if not match:
        return (False, sentence, None)

    if TRACE:
        print('\tRegex match: {0}'.format(match.group()))

    # either the groups 'words1' and 'words2' match
    # or the groups 'words3' and 'words4' do
    text1 = match.group('words1')
    text2 = match.group('words2')
    if text1 is None and text2 is None:
        text1 = match.group('words3')
        text2 = match.group('words4')

    assert text1 is not None or text2 is not None

    msi = regex_size_or_image.search(text1)
    if msi:
        text1 = erase(text1, msi.start(), msi.end())
    msi = regex_size_or_image.search(text1)
    if msi:
        text2 = erase(text2, msi.start(), msi.end())

    # restore the M, which occurs between the two matching texts
    matching_text = text1 + ' M ' + text2
    matching_text = collapse_ws(matching_text)

    if TRACE:
        print('\tA WDS M matching_text: ->{0}<-'.format(matching_text))

    text = match.group()
    m_index = m_index_from_context(m_sentence, text, resolved_meas_indices)
    if INVALID_M_INDEX != m_index and m_index < len(measurements):

        # subjects is a list of lists
        texts = [matching_text, text]
        for t in texts:
            doc, subjects = tokenize_and_find_subjects(t)
            if len(subjects) > 0 and len(subjects[0]) > 0:
                # found a subject for this measurement
                for s in subjects[0]:
                    measurements[m_index].subject.append(s)
                found_subject = True
                if TRACE:
                    print('\tSUBJECT: {0}'.format(subjects[0]))

                # erase matching text from sentence
                sentence = erase(sentence, match.start(), match.end())
                sentence = collapse_ws(sentence)

                if found_subject:

                    locations = []
                    for s in subjects[0]:
                        loc = extract_loc(t)
                        if EMPTY_STRING != loc:
                            loc = undo_substitutions(loc)
                            if loc not in locations:
                                locations.append(loc)
                    if len(locations) > 0:
                        measurements[m_index].location = locations.copy()
                    break

            if TRACE:
                if not found_subject:
                    print('\tno subject found with first text, trying again...')
                
    if found_subject:
        return (True, sentence, doc)
    else:
        return (False, sentence, None)
    

###############################################################################
def get_ngrams(words, n):
    """
    Return a list of all possible ngrams (of size n) from the given wordlist.
    The ngrams are returned as strings containing words separated by a single
    space.
    """
    result = []

    for i in range(0, len(words) - n + 1):
        result.append(' '.join(words[i:i+n]))

    return result


###############################################################################
def replace_ngrams(sentence):
    """
    Search the sentence for ngrams from the ngram file. Replace any ngrams
    found with a single noun from the 'ngram_replacements' list.
    """

    # replacement nouns - ensure none are in the ngram file
    ngram_replacements = ['car', 'city', 'year', 'news', 'math', 'hall',
                          'poet', 'fact', 'idea', 'oven', 'poem', 'dirt',
                          'tale', 'world', 'hotel']

    original_sentence = sentence
    max_ngram_length = ngram_word_counts[-1]
    
    # skip over common words at the start of the sentence
    str_common_start = r'\b(an?|the|there\s(is|are|was|were)(\san?)?)\b'
    regex_common_start = re.compile(str_common_start)
    match = regex_common_start.match(sentence)
    if match:
        sentence = sentence[match.end():]

    start = -1
    matches = []
    for i in range(len(sentence)):
        c = sentence[i:i+1]

        # accept only lowercase letters, spaces and hyphens
        if 'M' != c and (c.isalpha() or ' ' == c or '-' == c):
            if -1 == start:
                start = i
            continue
        else:
            # skip if too short
            if i - start < ngram_min_chars:
                start = -1
                continue
            if -1 == start:
                continue
            
            chunk = sentence[start:i]
            words = chunk.split()
            max_n = min( len(words), max_ngram_length)
            
            # find ngrams and substitute, work largest to smallest
            for n in reversed(ngram_word_counts):
                if n <= max_n:
                    ngrams_n = get_ngrams(words, n)

                    search_start = 0
                    for ngram in ngrams_n:
                        if ngram in ngram_dict[n]:

                            # search for ngrams as isolated words
                            iterator = re.finditer(r'\b' + ngram + r'\b', chunk)
                            for match in iterator:
                                n_start = start + match.start()
                                n_end   = start + match.end()

                                # check esisting matches for overlap
                                has_overlap = False
                                for t_ngram, t_start, t_end in matches:
                                    if n_start >= t_start and n_end <= t_end:
                                        has_overlap = True
                                        break
                                if not has_overlap:
                                    matches.append( (ngram, n_start, n_end))
                                    if TRACE:
                                        print('\tngram match: {0}'.format(ngram))
            start = -1

    # replace matching words in sentence
    sentence = original_sentence
    
    pos = 0
    index = 0
    for old_wd, start, end in matches:
        new_wd = ngram_replacements[index]
        sentence = re.sub(r'\b' + old_wd + r'\b', new_wd, sentence)
        replacements[new_wd] = old_wd
        if TRACE: print('\tReplaced {0} with {1}'.format(old_wd, new_wd))
        index += 1
        if index >= len(ngram_replacements):
            # found a long sentence with lots of replacements
            # probably a sentence tokenization error
            print('*** subject_finder: more matches than ngram replacements ***')
            print('original sentence: ' + original_sentence)
            break

    return sentence

    
###############################################################################
def self_test(TEST_DICT, terms, nosub):
    """
    Verify that the subject finder can correctly find the measurement subject
    for all test sentences.
    """

    # subj_list is a list of lists, the known results
    for sentence, subj_lists in TEST_DICT.items():

        # find the subject of each measurement
        json_string = run(terms, sentence, nosub)

        # parse the JSON result
        json_data = json.loads(json_string)

        # unpack to a SubjectFinderResult namedtuple
        result = SubjectFinderResult(**json_data)
        
        # get the array of measurements
        measurement_list = result.measurementList

        # unpack to a list of Measurement namedtuples
        measurements = [Measurement(**m) for m in measurement_list]

        if 0 == len(measurements):
            print('\n*** SELF TEST FAILURE: ***\n{0}'.format(sentence))
            print('\tNo measurement subjects were found.')
        else:
            num_entries = len(subj_lists)
            assert num_entries == len(measurements)
            
            # verify the subject of each
            for i in range(len(subj_lists)):
                m = measurements[i]
                subjects = subj_lists[i]
                for s in subjects:
                    # search for the truth string among all candidate strings
                    # (m.subject is a list of strings)
                    found_it = False
                    for candidate_str in m.subject:
                        match = re.search(r'\b{0}\b'.format(s), candidate_str)
                        if match:
                            found_it = True
                            break

                    if not found_it:
                        print('\n*** SELF TEST FAILURE: ***\n{0}'.format(sentence))
                        print('\t   Measurement: {0}'.format(m.text))
                        print('\tSubjects found: {0}'.format(m.subject))
                        print('\t         Truth: {0}'.format(subjects))
        

###############################################################################
def get_version():
    return 'subject_finder {0}.{1}'.format(VERSION_MAJOR, VERSION_MINOR)

        
# ###############################################################################
# def show_help():
#     print(get_version())
#     print("""
#     USAGE: python3 ./subject_finder.py -t <terms> -s <sentence> [-hvznxd]

#     OPTIONS:

#         -t, --terms     <quoted string> List of comma-separated search terms.
#         -s, --sentence  <quoted string> Sentence to be processed.

#     FLAGS:

#         --debug                         Print debug info to stdout
#         -h, --help                      log this information and exit.
#         -v, --version                   log version information and exit.
#         -z, --test                      Disable -s option and use test sentences.
#         -n, --nosub                     Do not perform ngram substitution.
#         -x, --selftest                  Run self-tests.
#         -d, --displacy                  Show the dependency parse using SpaCy's
#                                         'displacy' tool. Not valid if -r or -z
#                                         option are used. See the visualization
#                                         by opening a web browser at the URL
#                                         localhost:5000.
#     """)

    
###############################################################################
if __name__ == '__main__':

    # maps a sentence to a list of lists, one list for each measurement
    TEST_DICT = {

        # {subject} is/measure(s|ed|ing) M
        'The spleen is 7.5 cm.' :
        [['spleen']],
        'The spleen was 7.5 cm.' :
        [['spleen']],
        'The spleen measures 7.5 cm.' :
        [['spleen']],
        'The lymph nodes are all 2 cm.' :
        [['nodes']],
        'The spleen is unremarkable measuring 8.6 cm.' :
        [['spleen']],
        'The spleen measures 10 cm and appears normal.' :
        [['spleen']],
        'Local lymphadenopathy measures up to 10 x 7 mm.' :
        [['lymphadenopathy']],
        'These nonobstructing calculi measure up to 6 mm.' :
        [['calculi']],
        'The spleen measures 8.6 cm and is normal in appearance.' :
        [['spleen']],
        'Lymph nodes measure up to approximately 2 cm in levels II '         +\
        'through IV.' :
        [['nodes']],
        'The cyst in the lower pole of the kidney is 1.3 cm in size.' :
        [['cyst']],
        'There are small surrounding lymph nodes, the largest measuring '    +\
        '10 x 7 millimeters.' :
        [['nodes']],
        'The spleen is top normal in size, measuring 12.3 centimeters in '   +\
        'craniocaudal dimension.' :
        [['spleen']],
        'The duct tapers smoothly to the head of the pancreas, where it '    +\
        'measures approximately 5 mm.' :
        [['duct']],
        'Another indistinct cluster can be seen also in the peripheral '     +\
        'right middle lobe, measuring 12 x 11 millimeters (3: 39).' :
        [['cluster']],
        'Immediately inferior to the right lobe of the thyroid gland there ' +\
        'is a hypoechoic nodule measuring 0.7 cm in greatest dimension.' :
        [['nodule']],
        'There is no evidence of intrahepatic or extrahepatic biliary '      +\
        'dilatation with the common bile duct measuring 5.3 millimeters.' :
        [['duct']],
        'The largest right renal cyst arising from the lower pole measures ' +\
        '5.5 x 5.4 x 5.5 cm, and demonstrates benign features with a thin '  +\
        'wall and anechoic center.' :
        [['cyst']],
        'Right hilar node (5:28) is enlarged, measuring 1.4 cm, unchanged '  +\
        'from prior, and was previously shown to be non-FDG avid.' :
        [['node']],
        'Again seen is a loculated low density at the pancreatic head, now ' +\
        'measuring 6 cm x 4.5 cm (5:24) essentially unchanged compared to '  +\
        'the prior exam.' :
        [['density']],
        'There is a small to moderate pericardial effusion, predominantly '  +\
        'adjacent to the right ventricle and best seen on subcostal views, ' +\
        'measuring up to 1.4 centimeters in greatest dimension.' :
        [['effusion']],
        'There is diffuse severe dilatation of the stomach and small bowel ' +\
        'loops, with the small bowel maximally measuring 6.3 cm in '         +\
        'dimension.' :
        [['bowel']],
        'Scale imaging of the artery in this region shows a diffuse '        +\
        'symmetric wall thickening with the arterial wall measuring up to '  +\
        '7 mm in diameter over several centimeters length.' :
        [['wall']],        
        'Soft tissue structures demonstrate mediastinal lymphadenopathy '    +\
        'with numerous lymph nodes throughout the mediastinum, with the '    +\
        'largest node in the right paratracheal region measuring about 1.7 ' +\
        'cm in greatest short axis dimension.' :
        [['node']],        
        'Extensive, pronounced cervical lymphadenopathy throughout levels '  +\
        'II through IV, with lymph nodes measuring up to 2 cm.' :
        [['nodes']],        
        'Multiple lymph nodes are present at different mediastinal '         +\
        'stations with largest measuring 13 mm in the right upper '          +\
        'paratracheal region.' :
        [['nodes']],

        'Ectatic abdominal aorta, with multiple regions of enlargement, '    +\
        'with a focal dilatation measuring 4.3 cm just below the renal '     +\
        'arteries.' :
        [['dilatation']],
        
        'Multiple simple renal cysts are noted within the bilateral kidneys,'+\
        ' left more so than right, with the largest cyst identified within ' +\
        'the inferior pole of the left kidney measuring 2.4 cm.' :
        [['cyst']],        
        'The liver is normal in architecture and echogenicity, and is '      +\
        'seen to contain numerous small cysts ranging in size from a few '   +\
        'millimeters to approximately 1.2 cm in diameter.' :
        [['cysts']],        
        'Nondistended gallbladder with small amount of intraluminal sludge, '+\
        'and marked gallbladder wall edema measuring 7 mm.' :
        [['edema']],
        'Additionally, there is a small, loculated pericardial fluid '       +\
        'collection consistent with cellular debris abutting the '           +\
        'inferolateral wall of the left ventricle, measuring up to 0.9 '     +\
        'centimeters in size (clips 10, 11).' :
        [['collection']],
        'In the left upper lobe, there is a large irregular mass abutting '  +\
        'the anterior mediastinal and the anterior pleural space (3:29), '   +\
        'measuring with maximum cross-sectional area of 8.1 x 6.6 cm '       +\
        '(AP x TRV), found to likely represent non-small carcinoma per '     +\
        'outside hospital ([**Hospital6 **]) biopsy report.' :
        [['mass']],

        # ranging in size
        'Distended gallbladder with multiple stones ranging in size from '   +\
        'a few millimeters to 1 cm in diameter.' :
        [['stones']],
        'The liver is normal in architecture and echogenicity, and is '      +\
        'seen to contain numerous small cysts ranging in size from a few '   +\
        'millimeters to approximately 1.2 cm in diameter.' :
        [['cysts']],
        'Just cephalad to the solid mass is an area of multiloculated '      +\
        'cystic change in the pancreatic head, with multiple cysts ranging ' +\
        'in size from a few millimeters up to 1.2 cm.' :
        [['cysts']],

        # two measurements

        # now vs. then
        'A necrotic periportal lymph node  measures 2.0 cm compared to '     +\
        '1.7 cm previously.' :
        [['node'], ['node']],
        'There is a fusiform infrarenal abdominal aortic aneurysm measuring '+\
        '4.4 x 5.2 cm, which previously measured 4.3 x 5.3 cm, and has '     +\
        'therefore not significantly changed.' :
        [['aneurysm'], ['aneurysm']],
        'A left deltoid mass measures 4.2 x 4.6 cm (5:22, previously '       +\
        'measuring 3.4 x 4.4 cm).' :
        [['mass'], ['mass']],
        'A left adrenal nodule measures 1.2 x 1.4 cm as compared to 1.2 x '  +\
        '1.4 cm previously (2:55).' :
        [['nodule'], ['nodule']],
        'A right hilar node measures 12 mm in short axis (7:24), previously '+\
        '11 mm.' :
        [['node'], ['node']],
        'A segment III lesion currently measures 1.3 x 1.8 cm and '          +\
        'previously measured 1.2 x 1.6 cm (2:45).' :
        [['lesion'], ['lesion']],
        'Lesion one within the hilar region measures 35 mm x 24 mm, '        +\
        'previously measuring 31 mm x 23 mm.' :
        [['lesion'], ['lesion']],
        'The main pulmonary trunk measures 3.4 cm in diameter, previously '  +\
        'measured 3.8 cm.' :
        [['trunk'], ['trunk']],
        'Target lesion 1 which was a supraclavicular lymph node measures '   +\
        '5.5 x 3.3 cm today (5:17, measuring 5 x  2.7 cm on the prior).' :
        [['lesion'], ['lesion']],
        'The dominant lesion in the right parietal lobe near the vertex '    +\
        'now measures 2.5 x 2.1 cm (image 1000:28), compared to the 2.4 x '  +\
        '1.7 cm on [**2869-12-17**], appears unchanged.' :
        [['lesion'], ['lesion']],
        'Within the abdomen lesion five, which likely comprises a left '     +\
        'adrenal lesion now measures 36 mm x 28 mm previously measuring '    +\
        '38 mm x 29 mm.' :
        [['lesion'], ['lesion']],
        'The markedly enlarged spleen now measures 16 cm craniocaudally, '   +\
        'compared to approximately 13 cm in the prior PET-CT one month ago, '+\
        'concerning for lymphoma  recurrence.' :
        [['spleen'], ['spleen']],
        'The second largest lymph node in the left inferior axilla now '     +\
        'measures 1.9 x 2.0 x 2.5 cm (4:27), which is increased from the '   +\
        'prior measurement of 2.2 x 1.9 x 1.8 cm.' :
        [['node'], ['node']],
        'Interval decrease is seen in the fluid collections in the '         +\
        'perirenal space specifically in the right perirenal space the '     +\
        'fluid collection measures 2.2 cm x 2.4 cm now versus 2.6 x 2.5 cm ' +\
        'then.' :
        [['collection'], ['collection']],        
        'For example a left upper lung nodule measures 8.1 mm and '          +\
        'previously had measured 7.4 mm (3:36).' :
        [['nodule'], ['nodule']],
        'Again seen is aneurysmal dilatation of the left iliac artery, '     +\
        'currently measuring 3.3 x 3.3 cm, which previously measured 3.2 x ' +\
        '3.2 cm.' :
        [['dilatation'], ['dilatation']],
        'Again seen is an expansile soft tissue lytic lesion in the '        +\
        'anterior left second rib which now measures 7.9 x 2.6 cm, '         +\
        'previously measuring 6.7 x 2.7 cm (3:21).' :
        [['lesion'], ['lesion']],
        'A cavitary nodule is again seen within the right upper lobe '       +\
        'currently measuring 1.5 x 1 cm, from the previous measurement of '  +\
        '1.6 x 2.2 cm on [**2680-6-1**] study (4:21).' :
        [['nodule'], ['nodule']],

        # before and after
        'The left kidney measures 8.5 cm and contains an 8 mm x 8 mm '       +\
        'anechoic rounded focus along the lateral edge, which is most '      +\
        'likely a simple renal cyst.' :
        [['kidney'], ['focus']],
        'The left kidney measures 11.5 cm and contains a 2.8 x 2.2 cm '      +\
        'cyst in the upper pole which may contain a small septation, '       +\
        'however, no increased vascularity.' :
        [['kidney'], ['cyst']],
        'The right kidney measures 9.2 cm, and contains a 5 mm, hyperechoic '+\
        'calculus with posterior shadowing within the interpolar region.' :
        [['kidney'], ['calculus']],
        'The right kidney measures 12.1 cm in length, and in its lower '     +\
        'pole, there is a 3.0 x 2.1 x 2.7 cm simple cyst.' :
        [['kidney'], ['cyst']],
        'The right kidney measures 11.9 cm and demonstrates a 7 mm '         +\
        'hyperechoic focus without acoustic shadowing that may represent a ' +\
        'small nonobstructing renal calculus.' :
        [['kidney'], ['focus']],
        'The left kidney measures 12.6 cm, with that measurement including ' +\
        'a 4.5 cm exophytic cyst at the upper pole of the kidney, not '      +\
        'significantly changed compared to prior CT.' :
        [['kidney'], ['cyst']],

        # two 'measures M'
        'The right kidney measures 11.6 centimeters and the left kidney '    +\
        'measures 12.8 centimeters.' :
        [['kidney'], ['kidney']],
        'The right kidney measures 10 cm and left kidney measures 9.4 '      +\
        'cm, with no hydronephrosis, masses, or stones.' :
        [['kidney'], ['kidney']],
        'The right kidney measures 11.7 cm and the left kidney measures '    +\
        'approximately 12 cm.' :
        [['kidney'], ['kidney']],
        'The cyst on the right measures approximately 3.0 millimeters and '  +\
        'the cyst on the left measures approximately 3.6 millimeters.' :
        [['cyst'], ['cyst']],


        # ### problem 2
        'Two additional smaller  lesions are newly identified; one within '  +\
        'segment VIII measures 0.5 x 1.1 cm (2:45) and the second in '       +\
        'segment VI measures 11 x 4 mm  (601b:31).' :
        [['lesions'], ['second']],


        
        'The right kidney measures 11.5 cm and contains multiple '           +\
        'thin-walled anechoic rounded structures consistent with simple '    +\
        'renal cysts, the largest of which is within the mid pole and '      +\
        'measures 4.5 x 4.8 x 3.9 cm.' :
        [['kidney'], ['structures']],

        # M and M forms
        'The lower trachea measures 14 x 8 mm on expiratory imaging, and '   +\
        '16 x 17 mm on inspiratory imaging.' :
        [['trachea'], ['trachea']],
        'The largest porta hepatis lymph node measures 1.6 cm in short axis '+\
        'and 2.6 cm in long axis.' :
        [['node'], ['node']],
        'A large bulla at the left lung base measuring 4.6 x 4.2 cm and '    +\
        '1 cm bleb in the right lung base are unchanged.' :
        [['bulla'], ['bleb']],
        'Right kidney measures 12.1 cm, and the left kidney 13.1 cm in its ' +\
        'long axis.' :
        [['kidney'], ['kidney']],
        'The spleen is enlarged measuring 12.5 cm and has a simple 1.2 x '   +\
        '2.9 x 2.9 cm cyst.' :
        [['spleen'], ['cyst']],

        # # broken
        # 'The other bronchi demonstrate no evidence of malacia except to '    +\
        # 'note moderate malacia of the bronchus intermedius, which measures ' +\
        # '10 cm on inspiratory imaging and 4 mm in diameter on expiratory '   +\
        # 'imaging.' :
        # [['bronchus intermedius'], ['bronchus intermedius']],

        'At the level of the left main stem bronchus, the airway measures '  +\
        '171 square millimeters at end inspiration and reduces to 67 square '+\
        'millimeters (61% reduction in cross sectional area).' :
        [['airway'], ['airway']],
        'The right kidney measures 9.7 cm and again seen is an unchanged '   +\
        'approximately 1 cm hyperechoic focus in the upper pole consistent ' +\
        'with an angiomyolipoma.' :
        [['kidney'], ['focus']],
        'Two large enhancing masses are again noted arising from the lower ' +\
        'pole of the left kidney, which measure 5.0 cm x 4.1 cm (4:87) '     +\
        'and 7.3 cm x 5.7 cm (4:84).' :
        [['masses'], ['masses']],

        # three measurements
        'There is a small lesion measuring 1.3 cm, an enlarged lymph node '  +\
        'measuring 1.5 cm, and an echogenic focus in the lower pole '        +\
        'measuring 2.1 cm.' :
        [['lesion'], ['node'], ['focus']],
        'Additional lesions include a 6 mm ring-enhancing mass within the '  +\
        'left lentiform nucleus, a 10 mm peripherally based mass within the '+\
        'anterior left frontal lobe as well as a more confluent plaque-like '+\
        'mass with a broad base along the tentorial surface measuring '      +\
        'approximately 2 cm in greatest dimension.' :
        [['mass'], ['mass'], ['mass']],
        'Most representative are a 1.9 cm nodal mass in the right low '      +\
        'paratracheal station (2:16), a 1.8 cm node in the right low '       +\
        'paratracheal station (2:18), and a 1.6 cm infracarinal lymph node ' +\
        '(2:26).' :
        [['mass'], ['node'], ['node']],

        # Carina
        'ET tube tip is 2.2 cm above the carina.' :
        [['tip']],
        'Endotracheal tube is in place, roughly 4 cm above the carina.' :
        [['tube']],
        'Endotracheal tube is in standard position about 5 cm above the '    +\
        'carina.' :
        [['tube']],
        'ET tube is in the standard position, the tip is 6.1 cm above the '  +\
        'carina.' :
        [['tip']],
        'ET tube is low terminating in the lower thoracic trachea '          +\
        'approximately 1 mm above the carina.' :
        [['tube']],
        'A single supine frontal view of the chest shows an endotracheal '   +\
        'tube terminating 3 cm from the carina.' :
        [['tube']],
        'Tip of endotracheal tube is above the level of the clavicles, '     +\
        'terminating about 7 cm above the carina.' :
        [['tip']],
        'ET tube tip is high, 8.1 cm above the carina at the level of the '  +\
        'clavicles and should be advanced couple of centimeters for '        +\
        'standard positioning.' :
        [['tip']],

        # distance
        'The distance from the top of the graft to the aortic bifurcation '  +\
        'measures 117 mm.' :
        [['distance']],

        'The second lesion has a volume of 1.4 cubic centimeters.':
        [['lesion']],

        # need more distance examples - TBD

        # use --terms 'LV V1 VTI,t2'
        'The LV V1 VTI: 18.0 cm, t2: 3x4 cm':
        [['lv v1 vti'], ['t2']],

        # use --terms 'LV V1 VTI,t2,Ao V2 VTI'
        'mean: 103.7 cm/sec LV V1 VTI: 18.0 cm Ao mean PG: 5.3 mmHg Ao V2 VTI: 30.1 cm CO(LVOT): 3.3 l/min':
        [['lv v1 vti'], ['ao v2 vti']],

        # use --terms 'VTI,V1 VTI,V2 VTI,LV V1 VTI, Ao V2 VTI,VTI'
        "103.7 cm/sec lv v1 vti: 18.0 cm ao mean pg: 5.3 mmhg "             +\
        "ao v2 vti: 30.1 cm co(lvot): 3.3 l/min tr max vel: 212.3 cm/sec "  +\
        "sv(lvot): 55.2 ml tr max pg: 18.0 mmhg avg: 4.7 cmg: 4.7 cm/ sec " +\
        "e/e': 20.2 . . .":
        [['lv v1 vti'], ['ao v2 vti']],

        'LVOT diam: 2.0 cm EDV(MOD-sp4): 91.0 ml ESV(MOD-sp4): 48.0 ml . . .':
        [['lvot diam']],

        'LVOT diam 2.0 cm EDV(MOD-sp4): 91.0 ml ESV(MOD-sp4): 48.0 ml . . .':
        [['lvot diam']],
    }

    parser = argparse.ArgumentParser(
        description='Run validation tests on the subject finder module.'
    )

    parser.add_argument('--debug',
                        help='print debug information to stdout',
                        action='store_true')
    parser.add_argument('-v', '--version',
                        help='show version and exit',
                        action='store_true')
    parser.add_argument('-t', '--terms',
                        help='quoted string, list of comma-separated search terms',
                        required=True)
    parser.add_argument('-s', '--sentence',
                        help='quoted string, the sentence to be processed')
    parser.add_argument('-n', '--nosub',
                        help='do not perform ngram substitution',
                        action='store_true')
    parser.add_argument('-x', '--selftest',
                        help='run self-test suite (requires a term list)',
                        action='store_true')
    parser.add_argument('-z', '--test',
                        help='disable -s option and use test sentences',
                        action='store_true')
    parser.add_argument('-d', '--displacy',
                        help="Show dependency parse using SpaCy's 'displacy' tool (localhost:5000)",
                        action='store_true')
    
    args = parser.parse_args()

    if 'version' in args and args.version:
        print(get_version())
        sys.exit(0)

    if 'debug' in args and args.debug:
        _enable_debug()

    terms = args.terms

    nosub = False
    if 'nosub' in args and args.nosub:
        nosub = args.nosub

    sentence = None
    if 'sentence' in args and args.sentence:
        sentence = args.sentence

    selftest = False
    if 'selftest' in args and args.selftest:
        selftest = args.selftest

    use_display = False
    if 'displacy' in args and args.displacy:
        use_displacy = args.use_displacy

    use_test_sentences = False
    if 'test' in args and args.test:
        use_test_sentences = args.use_test_sentences

    if not sentence and not (selftest or use_test_sentences):
        print('A sentence must be specified on the command line.')
        sys.exit(-1)

    if not terms and not selftest:
        print('One or more search terms must be provided on the command line.')
        sys.exit(-1)

    # displacy option valid only for single-sentence use
    if selftest or use_test_sentences:
        use_displacy = False

    sentences = []
    if selftest or use_test_sentences:
        sentences = [s for s in TEST_DICT]
    else:
        sentences.append(sentence)


    # call prior to processing any sentences
    init()
    
        
    if selftest:
        self_test(TEST_DICT, terms, nosub)

    else:
        for sentence in sentences:
            if use_test_sentences:
                print(sentence)
            json_result = run(terms, sentence, nosub, use_displacy)
            print(json_result)

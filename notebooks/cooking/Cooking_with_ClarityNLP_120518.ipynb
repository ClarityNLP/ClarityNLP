{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cooking with ClarityNLP - Session #7: NLPQL Expressions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Today we will take a behind-the-scenes look at NLPQL expressions and how the system evaluates them. We will also provide an overview of our new [NLPQL editor tool](https://nlpql-editor.herokuapp.com/demo.html) that makes the task of creating NLPQL files much easier.\n",
    "\n",
    "For background on installing and using ClarityNLP, please see our [documentation](https://claritynlp.readthedocs.io/en/latest/index.html).\n",
    "\n",
    "We welcome questions via Slack or on [GitHub](https://github.com/ClarityNLP/ClarityNLP/issues)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extracting Measurements from Radiology Reports"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To start things off, suppose that we have access to a medical center's electronic health records for radiology. We are interested in searching the radiology reports for patients with lesions in the 1cm - 2cm size range. How can we use ClarityNLP to find these patients?\n",
    "\n",
    "As you've learned in previous cooking sessions, we need to **create an NLPQL file** that defines a **documentset** for the radiology reports and a **termset** with lesion-related terms. We will need to run the **measurement finder** to extract measurements, and then **filter the measurements** with mathematical expressions that constrain the allowable lesion sizes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Creating the NLPQL File"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When developing a new NLPQL file, it is best to limit the number of documents processed until the NLPQL is fully debugged and working. So let's start by limiting our initial document set to 50 documents. It shouldn't take too long to processes 50 documents, and if we make a mistake, we can quickly recover.\n",
    "\n",
    "A limit on the number of documents is specified by a ``limit`` statement in the NLPQL file. So open a text editor, create a new file called ``lesion.nlpql``, and enter the following line:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<pre>limit 50;</pre>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we need to insert some boilerplate that identifies the phenotype and version, provides a description, and imports the ClarityNLP libraries. All of your NLPQL files will have text similar to this at the start:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<pre>phenotype \"Lesions1to2Cm\" version \"1\";\n",
    "description \"Find lesions with sizes ranging from 1 to 2 cm.\";\n",
    "include ClarityCore version \"1.0\" called Clarity;</pre>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we only want to search radiology reports, we can create a documentset specifically for this purpose. Note that the ``report_types`` field actually takes an array argument (identified by the square brackets). We will use a single-element array containing the term ``Radiology``, the label used by the MIMIC-III data set:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<pre>\n",
    "documentset Docs:\n",
    "    Clarity.createDocumentSet({\n",
    "        \"report_types\":[\"Radiology\"]\n",
    "    });\n",
    "</pre>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we need to create a list of the tumor-related terms we want ClarityNLP to search for. We ponder this for a while and eventually arrive at a termset that uses some radiology lingo:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<pre>\n",
    "termset LesionTerms: [\n",
    "    \"lesion\", \"growth\", \"mass\", \"malignancy\", \"tumor\",\n",
    "    \"neoplasm\", \"nodule\", \"cyst\", \"focus of enhancement\",\n",
    "    \"echodensity\", \"hypoechoic focus\", \"echogenic focus\"\n",
    "];\n",
    "</pre>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we need to find and extract measurements, we must insert a command to run ClarityNLP's measurement finder. The simplest command to do this is:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<pre>\n",
    "define LesionMeasurement:\n",
    "    Clarity.MeasurementFinder({\n",
    "        documentset: [Docs],\n",
    "        termset: [LesionTerms]\n",
    "    });\n",
    "</pre>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This command runs the measurement finder on each sentence of our source documents. It returns any measurements that occur in the same sentence as a term in our ``LesionTerms`` termset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our goal is to find **patients** with tumors of the specified dimensions, so we specify a ``Patient`` context:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<pre>\n",
    "context Patient;\n",
    "</pre>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we're ready to write the expressions that constrain the lesion measurements to our desired size of 1-2 cm. Here we will insert three commands to do so, and we will explain the differences in results for each below:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<pre>\n",
    "define xBetween10and20mm:\n",
    "    where LesionMeasurement.dimension_X >= 10 AND LesionMeasurement.dimension_X <= 20;\n",
    "\n",
    "define xyBetween10and20mm:\n",
    "    where LesionMeasurement.dimension_X >= 10 AND LesionMeasurement.dimension_X <= 20 AND\n",
    "          LesionMeasurement.dimension_Y >= 10 AND LesionMeasurement.dimension_Y <= 20;\n",
    "\n",
    "define xyzBetween10and20mm:\n",
    "    where LesionMeasurement.dimension_X >= 10 AND LesionMeasurement.dimension_X <= 20 AND\n",
    "          LesionMeasurement.dimension_Y >= 10 AND LesionMeasurement.dimension_Y <= 20 AND\n",
    "          LesionMeasurement.dimension_Z >= 10 AND LesionMeasurement.dimension_Z <= 20;\n",
    "</pre>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ClarityNLP normalizes all dimensional measurements to units of **millimeters**, so our desired range of 1-2 cm becomes 10-20 mm.\n",
    "\n",
    "Next, why do we need constraints in one, two, and three dimensions? **Because that's how radiology measurements are reported.** Sometimes the radiologist will state that a lesion measures ``15 mm``, ignoring the other dimensions. At other times the radiologist will note the values of the second and third dimensions. We want to enforce constraints on all possibilities, so we need a constraint statement for each dimension.\n",
    "\n",
    "How did we know to use the ``dimension_X``, ``dimension_Y``, and ``dimension_Z`` field names?  Because our [MeasurementFinder API documentation](https://claritynlp.readthedocs.io/en/latest/api_reference/nlpql/measurementfinder.html) says to!\n",
    "\n",
    "And with that we're done. Here's is the final text of our ``lesion.nlpql`` all in one place:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<pre>\n",
    "limit 50;\n",
    "phenotype \"Lesions1to2Cm\" version \"1\";\n",
    "description \"Find lesions with sizes ranging from 1 to 2 cm.\";\n",
    "include ClarityCore version \"1.0\" called Clarity;\n",
    "\n",
    "// radiology documents only in the documentset\n",
    "documentset Docs:\n",
    "    Clarity.createDocumentSet({\n",
    "        \"report_types\":[\"Radiology\"]\n",
    "    });\n",
    "\n",
    "// lesion terms\n",
    "termset LesionTerms: [\n",
    "    \"lesion\", \"growth\", \"mass\", \"malignancy\", \"tumor\",\n",
    "    \"neoplasm\", \"nodule\", \"cyst\", \"focus of enhancement\",\n",
    "    \"echodensity\", \"hyperechogenic focus\"\n",
    "];\n",
    "\n",
    "// extract lesion measurements\n",
    "define LesionMeasurement:\n",
    "    Clarity.MeasurementFinder({\n",
    "        documentset: [Docs],\n",
    "        termset: [LesionTerms]\n",
    "    });\n",
    "\n",
    "// we want to find patients, so use 'Patient' context\n",
    "context Patient;\n",
    "\n",
    "// constraints on X, XY, and XYZ\n",
    "\n",
    "define xBetween10and20mm:\n",
    "    where LesionMeasurement.dimension_X >= 10 AND LesionMeasurement.dimension_X <= 20;\n",
    "\n",
    "define xyBetween10and20mm:\n",
    "    where LesionMeasurement.dimension_X >= 10 AND LesionMeasurement.dimension_X <= 20 AND\n",
    "          LesionMeasurement.dimension_Y >= 10 AND LesionMeasurement.dimension_Y <= 20;\n",
    "\n",
    "define xyzBetween10and20mm:\n",
    "    where LesionMeasurement.dimension_X >= 10 AND LesionMeasurement.dimension_X <= 20 AND\n",
    "          LesionMeasurement.dimension_Y >= 10 AND LesionMeasurement.dimension_Y <= 20 AND\n",
    "          LesionMeasurement.dimension_Z >= 10 AND LesionMeasurement.dimension_Z <= 20;\n",
    "</pre>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing the NLPQL for Syntax Errors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before trying to process documents with our new NLPQL file, it is a good idea to first check it for syntax errors. We can do this by submitting it to the ``nlpql_tester`` API endpoint, a useful tool for the NLPQL developer.\n",
    "\n",
    "In prevous cooking sessions we showed you how to use the [Postman](www.postman.com) GUI tool to submit NLPQL files to the ClarityNLP webserver. Today we will show you how to use a command-line tool called [curl](https://curl.haxx.se/) to do the same thing.\n",
    "\n",
    "The nlpql_tester API for a local ClarityNLP instance is typically found at ``localhost:5000/nlpql_tester``. The NLPQL file should be sent via HTTP POST using a content type of ``text/plain``.\n",
    "\n",
    "To submit the file, install ``curl`` on your system, then open a terminal window, change directories to the location of ``lesion.nlpql``, and run the next command. If you are following along in the notebook, there is a copy of ``lesion.nlpql`` in ``notebooks/cooking/assets/``.\n",
    "\n",
    "<pre>\n",
    "curl -i -X POST http://localhost:5000/nlpql_tester -H \"Content-Type: text/plain\" --data-binary \"@lesion.nlpql\"\n",
    "</pre>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's what the various options mean:\n",
    "```\n",
    "-i: include the HTTP header in the output\n",
    "-X: request type (must be POST)\n",
    "-H: add the subsequent Content-Type: text/plain to the header of the HTTP request\n",
    "--data-binary: POST the data exactly as specified, no additional processing\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note also that an ``@`` character precedes the file name. If you run the command outside of the folder that contains ``lesion.nlpql``, replace the final quoted string with ``\"@/path/to/lesion.nlpql\"``, substituting the appropriate path on your system."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The ``curl`` command sends the file to the ``nlpql_tester`` API endpoint via HTTP POST. If the syntax is OK the system responds with a JSON result. Otherwise the system responds with an error message."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can run the NLPQL tester directly from this notebook by first running the code in the next cell:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This code below is only required for running ClarityNLP in Jupyter notebooks.\n",
    "# It is not required if running NLPQL via API or the ClarityNLP GUI.\n",
    "import pandas as pd\n",
    "import claritynlp_notebook_helpers as claritynlp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now run the next cell to test the NLPQL file. You should see some JSON output as the result. For some reason, the output is truncated when running in this notebook. You can see the full JSON result by using the ``curl`` command above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "    \"limit\": 50,\n",
      "    \"phenotype\": {\n",
      "        \"declaration\": \"phenotype\",\n",
      "        \"values\": [],\n",
      "        \"named_arguments\": {},\n",
      "        \"library\": \"ClarityNLP\",\n",
      "        \"name\": \"\\\"LesionDemo\\\"\",\n",
      "        \"description\": \"\",\n",
      "        \"alias\": \"\",\n",
      "        \"concept\": \"\",\n",
      "        \"arguments\": [],\n",
      "        \"funct\": \"\",\n",
      "        \"version\": \"1\"\n",
      "    },\n",
      "    \"description\": \"\\\"Find lesions of various sizes.\\\"\",\n",
      "    \"document_sets\": [\n",
      "        {\n",
      "            \"declaration\": \"documentset\",\n",
      "            \"values\": [],\n",
      "            \"named_arguments\": {\n",
      "                \"report_types\": [\n",
      "                    \"Radiology\"\n",
      "                ]\n",
      "            },\n",
      "            \"library\": \"Clarity\",\n",
      "            \"name\": \"Docs\",\n",
      "            \"description\": \"\",\n",
      "            \"alias\": \"\",\n",
      "            \"concept\": \"\",\n",
      "            \"arguments\": [],\n",
      "            \"funct\": \"createDocumentSet\",\n",
      "            \"version\": \"\"\n",
      "        }\n",
      "    ],\n",
      "    \"includes\": [\n",
      "        {\n",
      "            \"declaration\": \"include\",\n",
      "            \"values\": [],\n",
      "            \"named_arguments\": {},\n",
      "            \"library\": \"ClarityNLP\",\n",
      "            \"name\": \"ClarityCore\",\n",
      "            \"description\": \"\",\n",
      "            \"alias\": \"Clarity\",\n",
      "            \"concept\": \"\",\n",
      "            \"arguments\": [],\n",
      "            \"funct\": \"\",\n",
      "            \"version\": \"1.0\"\n",
      "        }\n",
      "    ],\n",
      "    \"population\": \"All\",\n",
      "    \"context\": \"Patient\",\n",
      "    \"debug\": false,\n",
      "    \"owner\": \"claritynlp\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "lesion_nlpql_text = claritynlp.load_file('assets/lesion.nlpql')\n",
    "json_result = claritynlp.run_nlpql_tester(lesion_nlpql_text)\n",
    "print(json_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Running the NLPQL File"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Having verified that the NLPQL file has the proper syntax, we submit the job to the ClarityNLP server with a similar ``curl`` command:\n",
    "<pre>\n",
    "curl -i -X POST http://localhost:5000/nlpql -H \"Content-Type: text/plain\" --data-binary \"@lesion.nlpql\"\n",
    "</pre>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alternatively, you can run from the next notebook cell:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lesion_nlpql_text = claritynlp.load_file('assets/lesion.nlpql')\n",
    "run_result, main_csv, intermediate_csv, luigi = claritynlp.run_nlpql(lesion_nlpql_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The job may take several minutes to run.** After it runs to completion, browse to the location of the CSV file containing the intermediate results, and open it in in a spreadsheet application such as Microsoft Excel. We have saved the results of a run to ``notebooks/cooking/assets/lesion_intermediate.csv``. The next cell displays some data from this file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>dimension_X</th>\n",
       "      <th>dimension_Y</th>\n",
       "      <th>dimension_Z</th>\n",
       "      <th>job_id</th>\n",
       "      <th>nlpql_feature</th>\n",
       "      <th>subject</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>28</td>\n",
       "      <td>16</td>\n",
       "      <td>NaN</td>\n",
       "      <td>11131</td>\n",
       "      <td>LesionMeasurement</td>\n",
       "      <td>40463</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>6</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>11131</td>\n",
       "      <td>LesionMeasurement</td>\n",
       "      <td>40463</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>17</td>\n",
       "      <td>8</td>\n",
       "      <td>NaN</td>\n",
       "      <td>11131</td>\n",
       "      <td>LesionMeasurement</td>\n",
       "      <td>40463</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>110</td>\n",
       "      <td>101</td>\n",
       "      <td>NaN</td>\n",
       "      <td>11131</td>\n",
       "      <td>LesionMeasurement</td>\n",
       "      <td>40463</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>11131</td>\n",
       "      <td>LesionMeasurement</td>\n",
       "      <td>37766</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>6</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>11131</td>\n",
       "      <td>LesionMeasurement</td>\n",
       "      <td>37766</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>7</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>11131</td>\n",
       "      <td>LesionMeasurement</td>\n",
       "      <td>37766</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>39</td>\n",
       "      <td>20</td>\n",
       "      <td>NaN</td>\n",
       "      <td>11131</td>\n",
       "      <td>LesionMeasurement</td>\n",
       "      <td>26259</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>8</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>11131</td>\n",
       "      <td>LesionMeasurement</td>\n",
       "      <td>43634</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>11131</td>\n",
       "      <td>LesionMeasurement</td>\n",
       "      <td>43634</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>6</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>11131</td>\n",
       "      <td>LesionMeasurement</td>\n",
       "      <td>43634</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>12</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>11131</td>\n",
       "      <td>LesionMeasurement</td>\n",
       "      <td>43634</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>9</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>11131</td>\n",
       "      <td>LesionMeasurement</td>\n",
       "      <td>43634</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>11</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>11131</td>\n",
       "      <td>LesionMeasurement</td>\n",
       "      <td>43634</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>8</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>11131</td>\n",
       "      <td>LesionMeasurement</td>\n",
       "      <td>43634</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>34</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>11131</td>\n",
       "      <td>LesionMeasurement</td>\n",
       "      <td>43634</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>6</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>11131</td>\n",
       "      <td>LesionMeasurement</td>\n",
       "      <td>32971</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>25</td>\n",
       "      <td>23</td>\n",
       "      <td>NaN</td>\n",
       "      <td>11131</td>\n",
       "      <td>LesionMeasurement</td>\n",
       "      <td>32971</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>15</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>11131</td>\n",
       "      <td>LesionMeasurement</td>\n",
       "      <td>32971</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>17</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>11131</td>\n",
       "      <td>LesionMeasurement</td>\n",
       "      <td>32971</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>11131</td>\n",
       "      <td>LesionMeasurement</td>\n",
       "      <td>23384</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>16</td>\n",
       "      <td>18</td>\n",
       "      <td>NaN</td>\n",
       "      <td>11131</td>\n",
       "      <td>LesionMeasurement</td>\n",
       "      <td>19296</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>11131</td>\n",
       "      <td>LesionMeasurement</td>\n",
       "      <td>4383</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>11131</td>\n",
       "      <td>LesionMeasurement</td>\n",
       "      <td>35074</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>11131</td>\n",
       "      <td>LesionMeasurement</td>\n",
       "      <td>35074</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>11131</td>\n",
       "      <td>LesionMeasurement</td>\n",
       "      <td>35074</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>30</td>\n",
       "      <td>40</td>\n",
       "      <td>NaN</td>\n",
       "      <td>11131</td>\n",
       "      <td>LesionMeasurement</td>\n",
       "      <td>35074</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>11131</td>\n",
       "      <td>LesionMeasurement</td>\n",
       "      <td>35074</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>20</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>11131</td>\n",
       "      <td>LesionMeasurement</td>\n",
       "      <td>35074</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>11131</td>\n",
       "      <td>LesionMeasurement</td>\n",
       "      <td>35074</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>164</th>\n",
       "      <td>[14.0]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>11131</td>\n",
       "      <td>xBetween10and20mm</td>\n",
       "      <td>27718</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>165</th>\n",
       "      <td>[14.0]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>11131</td>\n",
       "      <td>xBetween10and20mm</td>\n",
       "      <td>27718</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>166</th>\n",
       "      <td>[15.0]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>11131</td>\n",
       "      <td>xBetween10and20mm</td>\n",
       "      <td>27718</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>167</th>\n",
       "      <td>[15.0]</td>\n",
       "      <td>[15.0]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>11131</td>\n",
       "      <td>xBetween10and20mm</td>\n",
       "      <td>24429</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>168</th>\n",
       "      <td>[15.0]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>11131</td>\n",
       "      <td>xBetween10and20mm</td>\n",
       "      <td>2423</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>169</th>\n",
       "      <td>[16.0]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>11131</td>\n",
       "      <td>xBetween10and20mm</td>\n",
       "      <td>35307</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>170</th>\n",
       "      <td>[11.0]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>11131</td>\n",
       "      <td>xBetween10and20mm</td>\n",
       "      <td>35307</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>171</th>\n",
       "      <td>[18.0]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>11131</td>\n",
       "      <td>xBetween10and20mm</td>\n",
       "      <td>37536</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>172</th>\n",
       "      <td>[10.0]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>11131</td>\n",
       "      <td>xBetween10and20mm</td>\n",
       "      <td>37536</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>173</th>\n",
       "      <td>[10.0]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>11131</td>\n",
       "      <td>xBetween10and20mm</td>\n",
       "      <td>37536</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>174</th>\n",
       "      <td>[18.0]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>11131</td>\n",
       "      <td>xBetween10and20mm</td>\n",
       "      <td>37536</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>175</th>\n",
       "      <td>[17.0]</td>\n",
       "      <td>[26.0]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>11131</td>\n",
       "      <td>xBetween10and20mm</td>\n",
       "      <td>16730</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>176</th>\n",
       "      <td>[14.0]</td>\n",
       "      <td>[25.0]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>11131</td>\n",
       "      <td>xBetween10and20mm</td>\n",
       "      <td>16730</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>177</th>\n",
       "      <td>[13.0]</td>\n",
       "      <td>[12.0]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>11131</td>\n",
       "      <td>xBetween10and20mm</td>\n",
       "      <td>16730</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>178</th>\n",
       "      <td>[11.0]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>11131</td>\n",
       "      <td>xBetween10and20mm</td>\n",
       "      <td>16730</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>179</th>\n",
       "      <td>[11.0]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>11131</td>\n",
       "      <td>xBetween10and20mm</td>\n",
       "      <td>16730</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>180</th>\n",
       "      <td>[19.0]</td>\n",
       "      <td>[15.0]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>11131</td>\n",
       "      <td>xBetween10and20mm</td>\n",
       "      <td>37362</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>181</th>\n",
       "      <td>[12.0]</td>\n",
       "      <td>[11.0]</td>\n",
       "      <td>[7.0]</td>\n",
       "      <td>11131</td>\n",
       "      <td>xBetween10and20mm</td>\n",
       "      <td>38408</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>182</th>\n",
       "      <td>[20.0]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>11131</td>\n",
       "      <td>xBetween10and20mm</td>\n",
       "      <td>27726</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>183</th>\n",
       "      <td>[20.0]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>11131</td>\n",
       "      <td>xBetween10and20mm</td>\n",
       "      <td>27726</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>184</th>\n",
       "      <td>[16.0]</td>\n",
       "      <td>[18.0]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>11131</td>\n",
       "      <td>xyBetween10and20mm</td>\n",
       "      <td>19296</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>185</th>\n",
       "      <td>[11.0]</td>\n",
       "      <td>[10.0]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>11131</td>\n",
       "      <td>xyBetween10and20mm</td>\n",
       "      <td>43310</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>186</th>\n",
       "      <td>[18.0]</td>\n",
       "      <td>[17.0]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>11131</td>\n",
       "      <td>xyBetween10and20mm</td>\n",
       "      <td>38528</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>187</th>\n",
       "      <td>[11.0]</td>\n",
       "      <td>[20.0]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>11131</td>\n",
       "      <td>xyBetween10and20mm</td>\n",
       "      <td>29739</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>188</th>\n",
       "      <td>[20.0]</td>\n",
       "      <td>[10.0]</td>\n",
       "      <td>[10.0]</td>\n",
       "      <td>11131</td>\n",
       "      <td>xyBetween10and20mm</td>\n",
       "      <td>12017</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>189</th>\n",
       "      <td>[15.0]</td>\n",
       "      <td>[15.0]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>11131</td>\n",
       "      <td>xyBetween10and20mm</td>\n",
       "      <td>24429</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>190</th>\n",
       "      <td>[13.0]</td>\n",
       "      <td>[12.0]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>11131</td>\n",
       "      <td>xyBetween10and20mm</td>\n",
       "      <td>16730</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>191</th>\n",
       "      <td>[19.0]</td>\n",
       "      <td>[15.0]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>11131</td>\n",
       "      <td>xyBetween10and20mm</td>\n",
       "      <td>37362</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>192</th>\n",
       "      <td>[12.0]</td>\n",
       "      <td>[11.0]</td>\n",
       "      <td>[7.0]</td>\n",
       "      <td>11131</td>\n",
       "      <td>xyBetween10and20mm</td>\n",
       "      <td>38408</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>193</th>\n",
       "      <td>[20.0]</td>\n",
       "      <td>[10.0]</td>\n",
       "      <td>[10.0]</td>\n",
       "      <td>11131</td>\n",
       "      <td>xyzBetween10and20mm</td>\n",
       "      <td>12017</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>194 rows Ã— 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    dimension_X dimension_Y dimension_Z  job_id        nlpql_feature  subject\n",
       "0            28          16         NaN   11131    LesionMeasurement    40463\n",
       "1             6         NaN         NaN   11131    LesionMeasurement    40463\n",
       "2            17           8         NaN   11131    LesionMeasurement    40463\n",
       "3           110         101         NaN   11131    LesionMeasurement    40463\n",
       "4             7         NaN         NaN   11131    LesionMeasurement    37766\n",
       "5             6         NaN         NaN   11131    LesionMeasurement    37766\n",
       "6             7         NaN         NaN   11131    LesionMeasurement    37766\n",
       "7            39          20         NaN   11131    LesionMeasurement    26259\n",
       "8             8         NaN         NaN   11131    LesionMeasurement    43634\n",
       "9             5         NaN         NaN   11131    LesionMeasurement    43634\n",
       "10            6         NaN         NaN   11131    LesionMeasurement    43634\n",
       "11           12         NaN         NaN   11131    LesionMeasurement    43634\n",
       "12            9         NaN         NaN   11131    LesionMeasurement    43634\n",
       "13           11         NaN         NaN   11131    LesionMeasurement    43634\n",
       "14            8         NaN         NaN   11131    LesionMeasurement    43634\n",
       "15           34         NaN         NaN   11131    LesionMeasurement    43634\n",
       "16            6         NaN         NaN   11131    LesionMeasurement    32971\n",
       "17           25          23         NaN   11131    LesionMeasurement    32971\n",
       "18           15         NaN         NaN   11131    LesionMeasurement    32971\n",
       "19           17         NaN         NaN   11131    LesionMeasurement    32971\n",
       "20            2         NaN         NaN   11131    LesionMeasurement    23384\n",
       "21           16          18         NaN   11131    LesionMeasurement    19296\n",
       "22            4         NaN         NaN   11131    LesionMeasurement     4383\n",
       "23            5         NaN         NaN   11131    LesionMeasurement    35074\n",
       "24            4         NaN         NaN   11131    LesionMeasurement    35074\n",
       "25            3         NaN         NaN   11131    LesionMeasurement    35074\n",
       "26           30          40         NaN   11131    LesionMeasurement    35074\n",
       "27            4           3           2   11131    LesionMeasurement    35074\n",
       "28           20         NaN         NaN   11131    LesionMeasurement    35074\n",
       "29            2           2           2   11131    LesionMeasurement    35074\n",
       "..          ...         ...         ...     ...                  ...      ...\n",
       "164      [14.0]         NaN         NaN   11131    xBetween10and20mm    27718\n",
       "165      [14.0]         NaN         NaN   11131    xBetween10and20mm    27718\n",
       "166      [15.0]         NaN         NaN   11131    xBetween10and20mm    27718\n",
       "167      [15.0]      [15.0]         NaN   11131    xBetween10and20mm    24429\n",
       "168      [15.0]         NaN         NaN   11131    xBetween10and20mm     2423\n",
       "169      [16.0]         NaN         NaN   11131    xBetween10and20mm    35307\n",
       "170      [11.0]         NaN         NaN   11131    xBetween10and20mm    35307\n",
       "171      [18.0]         NaN         NaN   11131    xBetween10and20mm    37536\n",
       "172      [10.0]         NaN         NaN   11131    xBetween10and20mm    37536\n",
       "173      [10.0]         NaN         NaN   11131    xBetween10and20mm    37536\n",
       "174      [18.0]         NaN         NaN   11131    xBetween10and20mm    37536\n",
       "175      [17.0]      [26.0]         NaN   11131    xBetween10and20mm    16730\n",
       "176      [14.0]      [25.0]         NaN   11131    xBetween10and20mm    16730\n",
       "177      [13.0]      [12.0]         NaN   11131    xBetween10and20mm    16730\n",
       "178      [11.0]         NaN         NaN   11131    xBetween10and20mm    16730\n",
       "179      [11.0]         NaN         NaN   11131    xBetween10and20mm    16730\n",
       "180      [19.0]      [15.0]         NaN   11131    xBetween10and20mm    37362\n",
       "181      [12.0]      [11.0]       [7.0]   11131    xBetween10and20mm    38408\n",
       "182      [20.0]         NaN         NaN   11131    xBetween10and20mm    27726\n",
       "183      [20.0]         NaN         NaN   11131    xBetween10and20mm    27726\n",
       "184      [16.0]      [18.0]         NaN   11131   xyBetween10and20mm    19296\n",
       "185      [11.0]      [10.0]         NaN   11131   xyBetween10and20mm    43310\n",
       "186      [18.0]      [17.0]         NaN   11131   xyBetween10and20mm    38528\n",
       "187      [11.0]      [20.0]         NaN   11131   xyBetween10and20mm    29739\n",
       "188      [20.0]      [10.0]      [10.0]   11131   xyBetween10and20mm    12017\n",
       "189      [15.0]      [15.0]         NaN   11131   xyBetween10and20mm    24429\n",
       "190      [13.0]      [12.0]         NaN   11131   xyBetween10and20mm    16730\n",
       "191      [19.0]      [15.0]         NaN   11131   xyBetween10and20mm    37362\n",
       "192      [12.0]      [11.0]       [7.0]   11131   xyBetween10and20mm    38408\n",
       "193      [20.0]      [10.0]      [10.0]   11131  xyzBetween10and20mm    12017\n",
       "\n",
       "[194 rows x 6 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lesion_csv = pd.read_csv('assets/lesion_intermediate.csv', \n",
    "                         usecols=['dimension_X', 'dimension_Y', 'dimension_Z', 'nlpql_feature', 'subject', 'job_id'])\n",
    "lesion_csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spreadsheet Rows are MongoDB Documents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our run generated a CSV file containing a header row and 194 rows of data. This CSV file is a dump of the results for our particular job, which has a unique ``job_id`` of 11131. These results are stored in a MongoDB collection called ``phenotype_results`` in a database called ``nlp``. It is important to understand that **each row** of data above is a **separate document** in the MongoDB database. For instance, here is the underlying database document for row 2 above, which was written directly by the ``MeasurementFinder`` task:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "    \"_id\": \"5bfd9c9a31ab5b2e981dca14\",\n",
      "    \"sentence\": \"there is a 1.7 x 0.8 cm fdg-avid soft tissue nodule in the subcutaneous tissues of the right breast.\",\n",
      "    \"text\": \"1.7 x 0.8 cm\",\n",
      "    \"start\": 11,\n",
      "    \"value\": 17,\n",
      "    \"end\": 23,\n",
      "    \"term\": \"avid soft tissue nodule\",\n",
      "    \"dimension_X\": 17,\n",
      "    \"dimension_Y\": 8,\n",
      "    \"dimension_Z\": null,\n",
      "    \"units\": \"MILLIMETERS\",\n",
      "    \"location\": [\n",
      "        \"subcutaneous tissues of the right breast\"\n",
      "    ],\n",
      "    \"condition\": \"EQUAL\",\n",
      "    \"value1\": null,\n",
      "    \"value2\": \"\",\n",
      "    \"temporality\": \"CURRENT\",\n",
      "    \"min_value\": 8,\n",
      "    \"max_value\": 17,\n",
      "    \"pipeline_type\": \"MeasurementFinder\",\n",
      "    \"pipeline_id\": 12573,\n",
      "    \"job_id\": 11131,\n",
      "    \"batch\": 50,\n",
      "    \"owner\": \"claritynlp\",\n",
      "    \"nlpql_feature\": \"LesionMeasurement\",\n",
      "    \"inserted_date\": \"2018-11-27T14:35:54.749Z\",\n",
      "    \"concept_code\": -1,\n",
      "    \"phenotype_final\": false,\n",
      "    \"report_id\": \"1048492\",\n",
      "    \"subject\": \"40463\",\n",
      "    \"report_date\": \"2119-02-16T00:00:00Z\",\n",
      "    \"report_type\": \"Radiology\",\n",
      "    \"source\": \"MIMIC\",\n",
      "    \"solr_id\": \"1048492\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "obj = { \"_id\" : \"5bfd9c9a31ab5b2e981dca14\", \"sentence\" : \"there is a 1.7 x 0.8 cm fdg-avid soft tissue nodule in the subcutaneous tissues of the right breast.\", \"text\" : \"1.7 x 0.8 cm\", \"start\" : 11, \"value\" : 17, \"end\" : 23, \"term\" : \"avid soft tissue nodule\", \"dimension_X\" : 17, \"dimension_Y\" : 8, \"dimension_Z\" : None, \"units\" : \"MILLIMETERS\", \"location\" : [ \"subcutaneous tissues of the right breast\" ], \"condition\" : \"EQUAL\", \"value1\" : None, \"value2\" : \"\", \"temporality\" : \"CURRENT\", \"min_value\" : 8, \"max_value\" : 17, \"pipeline_type\" : \"MeasurementFinder\", \"pipeline_id\" : 12573, \"job_id\" : 11131, \"batch\" : 50, \"owner\" : \"claritynlp\", \"nlpql_feature\" : \"LesionMeasurement\", \"inserted_date\" : \"2018-11-27T14:35:54.749Z\", \"concept_code\" : -1, \"phenotype_final\" : False, \"report_id\" : \"1048492\", \"subject\" : \"40463\", \"report_date\" : \"2119-02-16T00:00:00Z\", \"report_type\" : \"Radiology\", \"source\" : \"MIMIC\", \"solr_id\" : \"1048492\" }\n",
    "print(json.dumps(obj, indent=4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You will need to view the results in a spreadsheet to see how the field names become column names in the intermediate CSV file. Thus the CSV file provides a 'flattened' view of the database results for a particular job."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interpreting the Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at the output rows above, you can see that the results are broadly grouped by the value of the ``nlpql_feature`` field. There are four such groups with values ``LesionMeasurement``, ``xBetween10and20mm``, ``xyBetween10and20mm``, and ``xyzBetween10and20mm``. Take a look at the NLPQL file above and observe that these are the name strings in each ``define`` statement.\n",
    "\n",
    "A value of ``NaN`` (not a number) is the equivalent of a null result, meaning that no data was found for that measurement dimension.\n",
    "\n",
    "Rows 0-144 contain the extracted measurements, all of which have their ``nlpql_feature`` field equal to ``LesionMeasurement``. These rows comprise the output of the measurement extractor. They are the **input** data for the mathematical expressions that constrain the lesion measurements. The underlying documents for these ``LesionMeasurement`` results in the MongoDB database are called *task result documents*.\n",
    "\n",
    "Rows 145-183 have their ``nlpql_feature`` field equal to ``xBetween10and20mm``. Unlike the ``LesionMeasurement`` rows, which are directly generated by the MeasurementFinder task, these rows are generated by evaluation of a mathematical epxression. This expression places a constraint on the X dimension of each measurement. Only those measurements that satisfy the constraint fill these rows of the intermediate result file.\n",
    "\n",
    "Rows 184-192 have their ``nlpql_feature`` field equal to ``xyBetween10and20mm``. These rows are generated by evaluation of a mathematical expression that constrains both the X and Y measurement dimensions. Only those measurements that satisfy the constraints fill these rows of the intermediate result file.\n",
    "\n",
    "Row 193 has its ``nlpql_feature`` field equal to ``xyzBetween10and20mm``.  This row is the only measurement that survives the constraint on all three dimensions.\n",
    "\n",
    "Note that the ``xBetween10and20mm`` results contain 2D and 3D measurements, some of which have Y or Z dimensions that exceed 20 mm (such as rows 175 and 176). The constraint for these rows is only on the X dimension. The Y and Z dimensions can have any value whatsoever, even NaN (which means they don't exist).\n",
    "\n",
    "We see a single 3D measurement in the ``xy`` result section, in row 188. This measurement happens to have its Z dimension satisfying the constraints on the X and Y dimensions, but there is no constraint imposed on the measurement by the code itself."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NLPQL Expressions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the NLPQL example above, we enforced constraints on the measurement dimensions with NLPQL expressions. In this section we describe the different expression types and provide an overview of how ClarityNLP evaluates them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Mathematical Expressions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An NLPQL mathematical expression is found in a ``define`` statement such as:\n",
    "<pre>\n",
    "define hasFever:\n",
    "    where Temperature.value >= 100.4;\n",
    "    \n",
    "define xBetween10and20mm:\n",
    "    where LesionMeasurement.dimension_X >= 10 AND LesionMeasurement.dimension_X <= 20;\n",
    "</pre>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The portion of the statement following the ``where`` keyword is the mathematical expression. These expressions involve mathematical operations on variables of the form ``nlpql_feature.variable_name`` such as ``Temperature.value``, ``LesionMeasurement.dimension_X``, etc. They can also include numeric literals such as ``100.4``.\n",
    "\n",
    "NLPQL mathematical expressions produce a numerical result from data contained in a **single** task result document. Since each task result document comprises a row in the intermediate results CSV file (see above), the evaluation of mathematical expressions is also called a **single-row operation**. The numerical result from the expression evaluation is written to a new MongoDB result document, as demonstrated in the lesion example above."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Logical Expressions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An NLPQL logical expression is also found in a ``define`` statement and involves the logical operators AND, OR, and NOT, such as:\n",
    "<pre>\n",
    "define hasSepsis:\n",
    "    where hasFever AND hasSepsisSymptoms;\n",
    "\n",
    "define hasNoZConstraint:\n",
    "    where xBetween10and20mm OR xyBetween10and20mm;\n",
    "</pre>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The ``where`` portion of the statement is the logical expression. Logical expressions **operate on NLPQL features** such as ``hasFever`` and ``hasSepsisSymptoms``, **not** on individual variables such as ``Temperature.value``.\n",
    "\n",
    "NLPQL logical expressions use data from one or more task result documents to compute results. The results are written back to MongoDB as a set of new result documents. The evaluation of a logical expression is also called a **multi-row operation**, since it typically consumes and generates multiple rows in the intermediate results CSV file.\n",
    "\n",
    "The presence of an ``nlpql_feature.variable_name`` token indicates that the expression is actually a single-row operation, not multi-row.\n",
    "\n",
    "One further constraint: **no parentheses are allowed in logical expressions**. The reason for this limitation is probably clear by now: each logical operand refers to a set of result documents in the Mongo database, all of which share an identical ``nlpql_feature`` field value. Parenthesized operands have no such ``nlpql_feature`` value.\n",
    "\n",
    "The practical meaning of this is that you need to **synthesize complex logic from simpler parts**. For instance:\n",
    "\n",
    "<pre>\n",
    "// not legal\n",
    "define myLogicFeature:\n",
    "    where A AND (B OR C);\n",
    "</pre>\n",
    "\n",
    "If you imagine looking at a spreadsheet containing the intermediate results for this run, you would see a set of rows for nlpql_feature ``A``, a set of rows for nlpql_feature ``B``, and another set of rows for nlpql_feature ``C``. Where is the set of rows for ``(B OR C)``? It doesn't exist! It's not in the spreadsheet nor in the Mongo database.\n",
    "\n",
    "To actually evaluate this expression you should write it as two simple statements, each with **its own defined nlpql_feature**:\n",
    "\n",
    "<pre>\n",
    "// nlpql_feature == BorC\n",
    "define BorC:\n",
    "    where B OR C;\n",
    "\n",
    "define myLogicFeature:\n",
    "    where A AND BorC;\n",
    "</pre>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation of Single-Row (Mathematical) Expressions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So what does ClarityNLP actually do to evaluate a mathematical expression?\n",
    "\n",
    "First, the NLPQL front end parses the NLPQL file and generates a string of whitespace-separated tokens for each expression. The token string is passed to the evaluator which determines if it is single-row, multi-row, or something else that cannot be evaluated. If single-row, the the **nlpql_feature** and **field list** are extracted.\n",
    "\n",
    "Consider these examples, both of which are single-row mathematical expressions:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<pre>\n",
    "where Temperature.value >= 100.4\n",
    "where LesionMeasurement.dimension_X < 5 AND LesionMeasurement.dimension_Y < 5\n",
    "</pre>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The ``nlpql_feature`` and field list for the first example is:\n",
    "<pre>\n",
    "    nlpql_feature: Temperature\n",
    "    field_list:    ['value']\n",
    "</pre>\n",
    "For the second example:\n",
    "<pre>\n",
    "    nlpql_feature: LesionMeasurement\n",
    "    field_list:    ['dimension_X', 'dimension_Y']\n",
    "</pre>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Important point: there is always a **single** ``nlpql_feature`` for a mathematical expression, since each MongoDB result document contains a single value for this field."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MongoDB Aggregation Framework"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is desirable to use the capabilities of MongoDB for expression evaluation since the data is already contained within Mongo itself. Use of another library for evaluation would require queries to extract the data from Mongo; transmission across a network (from a remote Mongo host); ingest into a new library; then network transmission of the results to the Mongo host followed by insertion back into Mongo. This could be a very inefficient process for large data volumes and busy networks.\n",
    "\n",
    "The [MongoDB aggregation framework](https://docs.mongodb.com/manual/aggregation/) is a powerful facility upon which to build an expression evaluator. The aggregation framework provides filtering, document transformation, and mathematical operations that ClarityNLP uses to evaluate expressions. \n",
    "\n",
    "The aggregation framework evaluates expressions via an \"aggregation pipeline\", which is a series of commands (called stages) that operate on a set of documents and produce a new set as a result. The pipeline stages are ideally arranged so that the operations that affect the most documents are performed first."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Initial Filter Stage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The conversion process involves the generation of an initial [``$match``](https://docs.mongodb.com/manual/reference/operator/aggregation/match/#pipe._S_match) query to filter out everything but the data for the desired job, which is identified by its ``job_id``. The match query also checks for the existence of all entries in the field list and that they have non-null values. **A simple existence check is not sufficient**, since a null field actually exists but has a value that cannot be used for computation. Hence checks for existence and a non-null value are both necessary.\n",
    "\n",
    "For the two examples above, the ``$match`` query generates a pipeline filter stage that looks like this, assuming a job_id of 12345:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<pre>\n",
    "// Temperature.value >= 100.4\n",
    "{\n",
    "    $match : {\n",
    "        \"job_id\" : 12345,\n",
    "        \"nlpql_feature\" : {$exists:true, $ne:null},\n",
    "        \"value\"         : {$exists:true, $ne:null}\n",
    "    }\n",
    "}\n",
    "\n",
    "// LesionMeasurement.dimension_X < 5 AND LesionMeasurement.dimension_Y < 5\n",
    "{\n",
    "    $match : {\n",
    "        \"job_id\" : 12345,\n",
    "        \"nlpql_feature\" : {$exists:true, $ne:null},\n",
    "        \"dimension_X\"   : {$exists:true, $ne:null},\n",
    "        \"dimension_Y\"   : {$exists:true, $ne:null}\n",
    "    }\n",
    "}\n",
    "</pre>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the presence of this initial filter is the reason why the ``xBetween10and20mm`` results ignore the Y and Z dimensions, and why the ``xyBetween10and20mm`` results ignore the Z dimension. There are no filters on those variables in their respective ``define`` statements!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Conversion from Infix to Postfix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After generating the initial filter stage, ClarityNLP **converts the mathematical expression from infix to postfix**. The postifx conversion removes parentheses and resolves operator precedence and associativity issues. NLPQL uses the same [operator precedence](https://docs.python.org/3/reference/expressions.html#operator-precedence) and associativity as the Python programming language."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Stack-Based Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A postfix expression can be evaluated with a stack-based evaluator. The general idea is to push the postfix tokens onto a stack until an operator is encountered, at which point its operands are popped, the operator expression evaluated, and the result pushed back onto the stack.\n",
    "\n",
    "ClarityNLP uses this method, but **the evaluation process does not compute a mathematical result**. Instead, it performs string processing to format MongoDB aggregation commands for evaluating the mathematical expression. MongoDB aggregation uses a consistent syntax that makes this automated formatting process possible.\n",
    "\n",
    "After the postfix evaluation and formatting operations the expressions become:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<pre>\n",
    "// (nlpql_feature == Temperature) and (value >= 100.4)\n",
    "{\n",
    "   $match : {\n",
    "       \"job_id\" : 11116,\n",
    "       \"nlpql_feature\" : {$exists:true, $ne:null},\n",
    "       \"value\"         : {$exists:true, $ne:null}\n",
    "   }\n",
    "},\n",
    "{\n",
    "    \"$project\" : {\n",
    "        \"value\" : {\n",
    "            \"$and\" : [\n",
    "                {\"$eq\"  : [\"$nlpql_feature\", \"Temperature\"]},\n",
    "                {\"$gte\" : [\"$value\", 100.4]}\n",
    "            ]\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "// (nlpql_feature == LesionMeasurement) and (dimension_X < 5 and dimension_Y < 5)\n",
    "{\n",
    "    \"$match\" : {\n",
    "        \"job_id\" : 11116,\n",
    "        \"nlpql_feature\" : {$exists:true, $ne:null},\n",
    "        \"dimension_X\"   : {$exists:true, $ne:null},\n",
    "        \"dimension_Y\"   : {$exists:true, $ne:null}\n",
    "    }\n",
    "},\n",
    "{\n",
    "    \"$project\" : {\n",
    "        \"value\" : {\n",
    "            \"$and\" : [\n",
    "                {\n",
    "                    \"$eq\" : [\"$nlpql_feature\", \"LesionMeasurement\"]\n",
    "                },\n",
    "                {\n",
    "                    \"$and\" : [\n",
    "                        {\"$lt\" : [\"$dimension_X\", 5]},\n",
    "                        {\"$lt\" : [\"$dimension_Y\", 5]}\n",
    "                    ]\n",
    "                }\n",
    "            ]\n",
    "        }\n",
    "    }\n",
    "}\n",
    "</pre>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this point the aggregation pipelines for both expressions are complete. Each pipeline is sent to MongoDB where it runs and generates the results seen in the spreadsheet output above."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation of Multi-Row (Logical) Expressions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Multi-row expressions apply the logical operations ``AND``, ``OR``, and ``NOT`` to **sets** of MongoDB result documents. The sets are determined by the different values of the ``nlpql_feature`` field. In the lesion example above, a multi-row expression for accepting measurements with no constraint on the Z-component is:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<pre>\n",
    "define hasNoZConstraint:\n",
    "    where xBetween10and20mm OR xyBetween10and20mm;    \n",
    "</pre>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This logical OR operates on two **sets** of results. The first set contains all result documents whose ``nlpql_feature`` field has the value ``xBetween10and20mm``. The second set contains all result documents whose ``nlpql_feature`` field has the value ``xyBetween10and20mm``. The result of this logical OR is a new set of documents, each of which **satisfies the logical OR condition individually**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The logical ``NOT`` operation is used to compute **set differences**, such as in this expression:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<pre>\n",
    "define hasSepsisSymptomsWithoutRigors:\n",
    "    where hasSepsisSymptoms NOT hasRigors;\n",
    "</pre>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The result of the set difference operation ``A NOT B`` is all elements of set ``A`` that are not also elements of set ``B``.\n",
    "\n",
    "Also, **unary ``NOT`` is not supported**. So you cannot write statements such as ``NOT A``, or ``NOT hasNoZConstraint``."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Document Filtering and Grouping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluation of an n-ary logical expression uses the MongoDB aggregation pipeline as well. The evaluator proceeds by filtering result documents by the job_id, similar to the process described above for single-row expressions. Next, an additional filter stage is applied that discards all documents whose ``nlpql_feature`` value differs from those of the sets being logically combined.\n",
    "\n",
    "Any documents that remain are **grouped by value of the context variable**, which is the ``document_id`` for a Document context, or the ``subject`` field for a Patient context. For a logical NOT operation, any documents whose ``nlpql_feature`` field equals that of the \"B\" set are discarded."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For logical AND the documents within each group are **counted**. Any groups not having **at least** ``n`` members for an n-ary logical AND are discarded. Additionally, any groups not having **at least** ``n`` different nlpql_features are discarded as well."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, the documents in each group are sorted on the value of the â€˜otherâ€™ context variable. Thus for a patient context the documents in each group are sorted on the ``report_id`` field. This sort operation generates **subgroups** of documents sharing the same value of the â€˜otherâ€™ field."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To summarize the state of the result documents at this point: all surviving documents have been filtered and separated into groups. The members of each group all share identical values of the context variable. Within each group, the documents are further separated into subgroups. The documents in each subgroup have identical values of the â€˜otherâ€™ context variable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Formation of Ntuples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For set difference and logical OR, the processing is complete, and the documents are written out to MongoDB with an ``nlpql_feature`` determined by the ``define`` statement containing the expression.\n",
    "\n",
    "For an n-ary logical AND, the grouped documents are further arranged into ntuples, since the AND condition requires n-document groups. Each ntuple contains n documents, each of which has a different value for the ``nlpql_feature`` field.\n",
    "\n",
    "The ntuple formation process generates a **subset** of the full result of performing an n-way inner join on the context field. **The full inner join is not necessary to find the patient IDs**, since the join condition is **not** the entire document, but only the values of the context field. These values are known completely after the grouping operation mentioned a few paragraphs above, so exploding the result set in to a full n-way inner join is not going to find any more of those values. The same is also true for a document context."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NLPQL Editor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We would now like to show you how to use our [NLPQL editor tool](https://nlpql-editor.herokuapp.com/demo.html) to reproduce the ``lesion.nlpql`` file that we created earlier. You will see that this tool greatly simplifies the process of creating NLPQL files."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When you start the editor you see a screen that looks like this:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![nlpql_editor_1.png](assets/nlpql_editor_1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On the left side, the blue buttons allow you to create pre-formatted \"templates\" for NLPQL statements. The text of the templated statement appears in the edit window to the right of the buttons. The blue-green buttons at the upper right allow you to clear the edit window and start over, as well as copy the NLPQL text to the system clipboard.\n",
    "\n",
    "Now let's walk through the process of creating our lesion.nlpql file using the editor.\n",
    "\n",
    "To begin, notice that all but the two topmost blue buttons are disabled. You should click one of these buttons to start things off. \n",
    "\n",
    "Set the name for the lesion phenotype by pressing the ``Phenotype Name`` button. After you click the button a new dialog box pops up into which you can enter the text strings as shown in the next image:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![nlpql_editor_2.png](assets/nlpql_editor_2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When finished, save your changes and you should see this:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![nlpql_editor_3.png](assets/nlpql_editor_3.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Simple, isn't it?\n",
    "\n",
    "The next step is to click the ``Add Library`` button, which will insert the ``include`` statement for the ClarityNLP libraries. When you do that the lower set of buttons become active, so click the ``Limit Query`` button and enter ``50`` into the dialog that appears. Save your changes and you should see this:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![nlpql_editor_4.png](assets/nlpql_editor_4.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is no button for the ``description`` field, since this is just a quoted string that you can enter directly into the edit window. The description is optional, so we will omit it for now.\n",
    "\n",
    "Now let's create our documentset of radiology reports. Click the ``Add Document Set`` button and enter ``Docs`` into the topmost edit control. From the ``Document Set Type`` combo box, select the ``By Query`` option. This causes the dialog to expand in size, and you will need to scroll the window to see it all. Enter ``Radiology`` into the ``Report Types`` edit control, as shown here:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![nlpql_editor_5.png](assets/nlpql_editor_5.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After entering the data, scroll down to the bottom of the dialog and save your changes. You should see the documentset statement appear in the edit window, as shown here:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![nlpql_editor_6.png](assets/nlpql_editor_6.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we're getting somewhere. The next task is to enter the termset containing our carefully-selected lesion terms. Click the ``Add Term Set`` button. Set the ``Termset Name`` to ``LesionTerms``, and enter the lesion terms as a (long) comma-separated list:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![nlpql_editor_7.png](assets/nlpql_editor_7.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After entering the list of lesion terms, save your changes and you should see the termset appear in the edit window. The long list of terms will scroll off of the screen, so insert some newlines in the list to get everything to appear onscreen at once:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![nlpql_editor_8.png](assets/nlpql_editor_8.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next thing to do is insert the command to run the measurement finder, which you can do by clicking on the ``Define Feature`` button (recall the discussion of NLPQL **features** above). In the popup dialog enter ``Lesion Measurement`` as the feature name and select ``Measurement Finder`` from the algorithm selection combo box. When you do this, additional edit controls will appear. Enter ``LesionTerms`` for the termset and ``Docs`` for the document set:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![nlpql_editor_9.png](assets/nlpql_editor_9.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After saving changes you should see this as the result:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![nlpql_editor_10.png](assets/nlpql_editor_10.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set the context by clicking the ``Set Logical Context`` button and selecting ``Patient`` in the pop-up dialog. This should be the result:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![npql_editor_11.png](assets/nlpql_editor_11.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now all that's left is for us to add the expressions that set constraints on the measurements. To do this, click the ``Define Result`` button. In the ``Result Name`` edit control, enter ``xBetween10and20mm``. In the ``Logic`` edit control below it, enter the string ``LesionMeasurement.dimension_X >= 10 and LesionMeasurement.dimension_X <= 20``. Note that you do not need to type the ``where`` keyword or the terminating semicolon:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![nlpql_editor_12.png](assets/nlpql_editor_12.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When you save changes you should see this as the result:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![nlpql_editor_13.png](assets/nlpql_editor_13.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observe the appearance of the ``final`` keyword. We did not use this keyword previously because we wanted all results to appear in the single intermediate CSV file, so that we could see everything together in one place. If we keep the ``final`` keyword it will cause the ``xBetween10and20mm`` result set to appear in the \"final\" phenotype CSV file. Since we want to reproduce the results above, go ahead and delete this keyword from the text in the edit window.\n",
    "\n",
    "At this point, you could click the ``Define Result`` button and create the other constraints in a similar manner. It is probably easier to just cut and paste from the existing constraint and enter the text directly into the edit control. By whatever method you choose, create the two other constraints and arrive at this final result:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![nlpql_editor_14.png](assets/nlpql_editor_14.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Your NLPQL creation can now be copied to the clipboard and saved to a file called ``lesion.nlpql``. With this file you can follow the discussion above and you should see very similar results if you run it on the MIMIC-III data set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Thanks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thank you for joining this week's Cooking with ClarityNLP!  Please send any requests or ideas for future Cooking shows to <charity.hilton@gtri.gatech.edu>.\n",
    "\n",
    "Have a great week!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

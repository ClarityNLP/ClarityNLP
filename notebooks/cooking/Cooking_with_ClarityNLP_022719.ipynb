{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cooking with ClarityNLP - Session #10: Clinical Trials and Language Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For today's cooking session we will use ClarityNLP to find mentions of chemotherapy regimens in clinical trial eligibility criteria. We will then use the extracted text to build an ngram language model that recognizes usage contexts for chemotherapy regimens. We will also estimate the performance of our model via K-fold cross validation.\n",
    "\n",
    "For details on installing and using ClarityNLP, please see our [documentation](https://claritynlp.readthedocs.io/en/latest/index.html).  We welcome questions via Slack or on [GitHub](https://github.com/ClarityNLP/ClarityNLP/issues)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ClinicalTrials.gov and the AACT Database"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The U.S. Library of Medicine maintains a website called [ClinicalTrials.gov](https://clinicaltrials.gov/), which hosts a large database of clinical trial information. The data includes the trial objectives, inclusion and exclusion criteria, the participating organizations, the personnel conducting the trial, and other related information. Each clinical trial is assigned a unique **NCT ID**, such as NCT03601923, which happens to be the ID for a now-recruiting trial to evaluate the drug Niraparib as a treatment for pancreatic cancer.\n",
    "\n",
    "The home page for ClinicalTrials.gov provides a web form by which users can enter search criteria and find clinical trials of interest:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![clinicaltrials_gov.png](session_10/clinicaltrials_gov.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This form is convenient for interactive searches, but larger-scale analysis of the clinical trial data requires access to the database, which this site does not provide.\n",
    "\n",
    "Luckily, there is a [related AACT website](https://aact.ctti-clinicaltrials.org/) (AACT == Aggregate Analysis of ClinicalTrials.gov) that hosts a downloadable PostgreSQL database containing all of the data at clinicaltrials.gov. As of this writing, the current download is 871 MB in size, and it contains data for more than 289,000 clinical trials. The download file is updated at near-daily intervals. If you want to run natural language processing or machine learning algorithms on the clinical trial data, the AACT database is what you need. The AACT website contains links to download and installation instructions, the data dictionary and database schema, and lots of other documentation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading the AACT Database into Solr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To be able to interrogate the AACT data with ClarityNLP, the relevant text from the clinical trials needs to be ingested into Solr. At GA Tech we downloaded a snapshot of the AACT database, installed it, got it working, and ingested the eligibility criteria into our Solr instance. The eligibility criteria can be found in the ``criteria`` column of the ``eligibilities`` table.\n",
    "\n",
    "Some python scripts that we used can be found in our [utilities project](https://github.com/ClarityNLP/Utilities). Look at the .py files containing *aact* in the filename if you're interested. Documentation on the fields that ClarityNLP expects can be found in our [documentation](https://claritynlp.readthedocs.io/en/latest/developer_guide/technical_background/solr.html).\n",
    "\n",
    "Our data ingestion process maps these [required fields](https://claritynlp.readthedocs.io/en/latest/developer_guide/technical_background/solr.html) as follows:\n",
    "* ``report_type``: either ``\"Clinical Trial Inclusion Criteria\"`` or ``\"Clinical Trial Exclusion Criteria\"``\n",
    "* ``report_text``: text of the inclusion or exclusion criteria\n",
    "* ``source: AACT``\n",
    "* ``subject``: the NCT ID of the trial\n",
    "\n",
    "These field mappings will be important for understanding the NLPQL presented below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**IMPORTANT NOTE**: After ingesting this data we found that our python scripts did not capture all of the eligibility criteria for some trials. The text of the eligibility criteria unfortunately does not conform to the official data dictionary in all instances. The data dictionary [states the following](https://prsinfo.clinicaltrials.gov/definitions.html#EligibilityCriteria) for the eligibility criteria:\n",
    "\n",
    "<pre>\n",
    "Eligibility Criteria *\n",
    "Definition: A limited list of criteria for selection of participants in the clinical study,\n",
    "provided in terms of inclusion and exclusion criteria and suitable for assisting potential\n",
    "participants in identifying clinical studies of interest. Use a bulleted list for each\n",
    "criterion below the headers \"Inclusion Criteria\" and \"Exclusion Criteria\".\n",
    "Limit: 15,000 characters. \n",
    "</pre>\n",
    "\n",
    "Note the bulleted list requirement. Some trials do not use a bulleted list for the criteria; others are missing either inclusion or exclusion criteria or both; others alternate back and forth between inclusion and exclusion, etc. Our ingest scripts do not capture all of the data under these circumstances. We estimate that approximately 5% of the eligibility criteria are non-conformant.\n",
    "\n",
    "We are working on improved data ingest code and will update our scripts when we are satisfied that we can cleanly capture all of the criteria. For purposes of this cooking session, though, the scripts work well enough."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chemotherapy Regimens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our goal is to be able to find mentions of chemotherapy regimens in the clinical trial eligibility criteria. We would also like to be able to assign a score to each criterion that indicates how confident we are in the identification. For some regimens such as BEACOPP, FOLFOX, ProMACE-CytaBOM, and XELOX this is not too difficult. For other regimens such as CA, ICE, and BEAM this task is somewhat more difficult, since the regimen names match those of common words or abbreviations.\n",
    "\n",
    "Our approach to this problem is to use ClarityNLP to extract regimen names and the surrounding text from the clinical trial eligibility criteria. We then normalize the text, replace the regimen names with a token, and compute all ngrams that include the regimen token. These ngrams provide us with sample contexts for the use of regimen names. By counting all such ngrams and scoring them in a smoothed language model, we can assign a probability to each ngram and use it to estimate the likelihood that a given ngram contains a regimen name."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regimen Names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So where can one find a list of the names of chemotherapy regimens? There happens to be a comprehensive, peer-reviewed oncology wiki at [HemeOnc.org](https://hemonc.org/wiki/Main_Page) that contains information on treatment regimens for many different types of cancer. One of our intrepid co-workers scraped this wiki and generated hundreds of NLPQL files for extracting the treatment regimens and relevant drugs from clinical text. The full list can be found [here](https://github.com/ClarityNLP/Utilities/tree/master/regimen_nlpql), along with NLPQL for related drugs and conditions. A JSON file containing all of this data can be found [here](https://github.com/ClarityNLP/Utilities/blob/master/cancer/regimen_tree.json)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We provide a python utility script called ``get_all_regimens.py`` that extracts the regimen names from the JSON data and writes them to a .csv file. You can find this utility script [here](https://github.com/ClarityNLP/Utilities/blob/master/get_all_regimens.py).  Running this script generates the file ``all_regimen_names.csv``, which you can find in our github repo in the support folder for this notebook: ``ClarityNLP/notebooks/cooking/session_10/``. Here is a sample of what the file contains (**1961 lines total**):"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![all_regimens.png](session_10/all_regimens.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The regimen name(s) are found in the first column. Alternate names are found in the second column."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Believe it or not, there are still other chemotherapy regimen names not contained in this file. We have collected additional regimen names (mostly from Wikipedia and clinical trial data) and saved them to the file ``notebooks/session_10/supplemental_chemo_regimens.txt``. This file contains 88 additional regimens."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NLPQL for Finding Chemotherapy Regimens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we want to use ClarityNLP to find chemotherapy regimens in clinical text, we need to generate an NLPQL file containing the relevant instructions. Here is a template for an NLPQL file that we can use to find chemotherapy regimens in the AACT data:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<pre>\n",
    "phenotype \"Chemotherapy Regimens\" version \"1\";\n",
    "include ClarityCore version \"1.0\" called Clarity;\n",
    "\n",
    "// NOTE: uses field mappings described above\n",
    "documentset Docs:\n",
    "    Clarity.createDocumentSet({\n",
    "        \"report_types\":[\n",
    "            \"Clinical Trial Inclusion Criteria\",\n",
    "            \"Clinical Trial Exclusion Criteria\"\n",
    "        ],\n",
    "        \"filter_query\":\"source:AACT\"\n",
    "    });\n",
    "\n",
    "// << INSERT TERMSET FOR CHEMOTHERAPY REGIMEN NAMES HERE >>\n",
    "\n",
    "// find ANY mention of an entry in our termset\n",
    "define hasRegimenTerm:\n",
    "    Clarity.TermFinder({\n",
    "        termset: [RegimenTerms],\n",
    "        documentset: [Docs]\n",
    "    });\n",
    "</pre>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This NLPQL file does the following things:\n",
    "\n",
    "* Creates a document set customized for our AACT data\n",
    "* Creates a placeholder for a chemotherapy regimen termset\n",
    "* Creates a [TermFinder](https://claritynlp.readthedocs.io/en/latest/api_reference/nlpql/term_finder.html) task to search for the regimen terms in the AACT data\n",
    "\n",
    "It is straightforward to extract the data from the CSV file and the supplemental regimen file and generate an NLPQL termset containing all of the ~2000 regimen names. Unfortunately, the number of false positives that ClarityNLP finds is overwhelming. This is because many of the regimen names such as HAM, RICE, ACT, and others are common English words. Most two and three-letter regimen acronyms seem to collide with other common abbreviations in this data.\n",
    "\n",
    "There is also another problem with such a large termset. **Solr has a limit on the number of Booleans that it can use in a given query.** This value can be found in the ``solrconfig.xml`` file as the entry ``maxBooleanClauses``. The default seems to be 1024. ClarityNLP converts the termset into a Boolean query, so running with a huge termset may cause you to exceed your system's ``maxBooleanClauses`` limit. Please check the value in your ``solrconfig.xml`` file and adjust if necessary.\n",
    "\n",
    "To overcome both of these problems, we have ruthlessly **pruned the termlist** to remove drug names, common acronyms, and any regimen names that are not present in our AACT data. This process required trial-and-error experimentation and repeated runs to produce a result amenable to the **automated** analysis described below. Our final pruned termset can be found in ``notebooks/cooking/session_10/regimen_termset.txt``. This termset includes **349** regimen names."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the termset in hand, you can paste it into the NLPQL file above and run it. The full NLPQL file can be found in ``notebooks/cooking/session_10/find_regimens.nlpql``.\n",
    "\n",
    "When we run this file on our AACT dataset it generates a .csv result file containing 2077 entries. This file has been saved to ``notebooks/cooking/session_10/phenotype_regimens.csv``."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset Generation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We would now like to use the results from ClarityNLP to generate an ngram language model. Unfortunately, we cannot just extract the [sentence](https://claritynlp.readthedocs.io/en/latest/api_reference/nlpql/term_finder.html) field from the results, since **the sentence tokenizer is often confused by the strange formatting of the clinical trial criteria data**. We need to do some more work to extract the sentences that we need."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First some preliminaries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import os\n",
    "import sys\n",
    "import csv\n",
    "import nltk\n",
    "import random\n",
    "import statistics\n",
    "import subprocess\n",
    "\n",
    "# max n for ngrams\n",
    "MAX_N = 5\n",
    "\n",
    "# output file for language model cross-validation data\n",
    "SENTENCE_FILE = 'session_10/sentences.txt'\n",
    "\n",
    "# output file for sentences that get discarded\n",
    "DISCARD_FILE = 'session_10/discard_sentences.txt'\n",
    "\n",
    "# error log\n",
    "LOG_FILE = 'session_10/error_log.txt'\n",
    "\n",
    "# replace all chemotherapy regimen names with this token\n",
    "REGIMEN_TOKEN = '<regimen>'\n",
    "\n",
    "# these tokens denote the start and end of a sentence in the ngram model\n",
    "START_TOKEN = '<s>'\n",
    "END_TOKEN   = '</s>'\n",
    "\n",
    "# name of the final language model file\n",
    "FINAL_MODEL_FILE = 'session_10/final_model.arpa'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will need a function to load the phenotype .csv file and extract the \"sentence\" and matching term from the termset. We will also need a function to load the termset, since the terms need to undergo the same text normalization process as the sentences. A function to load a list of drug names is also needed.\n",
    "\n",
    "Here is the loading code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "###############################################################################\n",
    "def load_csv_data(csv_file):\n",
    "    \"\"\"\n",
    "    Extract the 'sentence' field from the file and return list of strings.\n",
    "    \"\"\"\n",
    "\n",
    "    data = []\n",
    "    with open(csv_file, 'rt') as infile:\n",
    "        dict_reader = csv.DictReader(infile)\n",
    "        for row_dict in dict_reader:\n",
    "            sentence = row_dict['sentence']\n",
    "            data.append(sentence)\n",
    "\n",
    "    return data\n",
    "\n",
    "\n",
    "###############################################################################\n",
    "def load_termset(termset_file):\n",
    "    \"\"\"\n",
    "    Extract individual regimen terms from an NLPQL termset.\n",
    "    \"\"\"\n",
    "\n",
    "    term_list = []\n",
    "    with open(termset_file, 'rt') as infile:\n",
    "        for line in infile:\n",
    "            line = line.strip()\n",
    "\n",
    "            # regimen names are quoted\n",
    "            if not line.startswith('\"'):\n",
    "                continue\n",
    "\n",
    "            # strip quotes and commas\n",
    "            if line.endswith('\",'):\n",
    "                regimen = line[1:-2]\n",
    "            else:\n",
    "                # final term\n",
    "                regimen = line[1:-1]\n",
    "\n",
    "            term_list.append(regimen)\n",
    "            \n",
    "    return term_list\n",
    "\n",
    "###############################################################################\n",
    "def load_drugs(drug_file):\n",
    "    \"\"\"\n",
    "    Load a list of drug names from the given file. Assumes a file format of\n",
    "    one string per line. Returns a set for faster lookup.\n",
    "    \"\"\"\n",
    "    \n",
    "    drug_set = set()\n",
    "    with open(drug_file, 'rt') as infile:\n",
    "        for line in infile:\n",
    "            if line.isspace() or 0 == len(line) or line.startswith('#'):\n",
    "                continue\n",
    "            drug_set.add(line.strip())\n",
    "            \n",
    "    return drug_set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will normalize the text by converting it to lowercase, stripping punctuation, removing extraneous Unicode codepoints, and a few other transformations. Here is the code to cleanup the text:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "###############################################################################\n",
    "def cleanup_text(sentences):\n",
    "    \"\"\"\n",
    "    Normalize the input data by performing a series of cleanup operations.\n",
    "    The input is assumed to be a list of strings. Returns a list of strings\n",
    "    containing the cleaned data. In this function a 'sentence' could also be\n",
    "    a regimen name, since this function is used to clean both the sentence and\n",
    "    regimen data.\n",
    "    \"\"\"\n",
    "\n",
    "    cleaned_sentences = []\n",
    "    for sent in sentences:\n",
    "\n",
    "        # convert to lowercase\n",
    "        sent = sent.lower()\n",
    "\n",
    "        # strip registered trademark and related symbols\n",
    "        # Gliadel wafer sometimes has these\n",
    "        sent = re.sub(r'(\\u00AE|\\u2122|\\u2120|\\u00A9|\\u2117)', '', sent)\n",
    "        \n",
    "        # replace 'r/r' with 'relapsed or refractory'\n",
    "        sent = re.sub(r'\\br/r\\b', 'relapsed or refractory', sent)\n",
    "\n",
    "        # replace symbols used in regimen names with whitespace\n",
    "        sent = re.sub(r'[-+/&]', ' ', sent)\n",
    "\n",
    "        # replace i.e. with ie, .e.g. with eg, to prevent single-letter\n",
    "        # tokens when punctuation is replaced with whitespace\n",
    "        sent = re.sub(r'\\bi\\.e\\.?\\b', 'ie', sent)\n",
    "        sent = re.sub(r'\\be\\.g\\.?\\b', 'eg', sent)\n",
    "        \n",
    "        # replace punctuation with whitespace\n",
    "        sent = re.sub(r'[^a-z0-9]', ' ', sent)\n",
    "\n",
    "        # correct misspellings:\n",
    "        #    'regiment'         => regimen\n",
    "        #    'hypersensibility' => 'hypersensitivity'\n",
    "        sent = re.sub(r'\\bregiment\\b', 'regimen', sent)\n",
    "        sent = re.sub(r'\\bhypersensibility\\b', 'hypersensitivity', sent)\n",
    "        \n",
    "        # collapse repeated spaces to a single space\n",
    "        sent = re.sub(r'\\s+', ' ', sent)\n",
    "\n",
    "        cleaned_sentences.append(sent.strip())\n",
    "\n",
    "    return cleaned_sentences\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To normalize the regimen terms, we normalize the regimen text and then enter a sufficient number of variants to capture common usage. For instance, a regimen such as \"XELOX and Bevacizumab\" could appear in the clinical trial eligibility criteria in any of these forms:\n",
    "\n",
    "<pre>\n",
    "XELOX & Bevacizumab\n",
    "XELOX / Bevacizumab\n",
    "XELOX + Bevacizumab\n",
    "    \n",
    "XELOX and Bevacizumab\n",
    "XELOX plus Bevacizumab\n",
    "    \n",
    "XELOX&Bevacizumab\n",
    "XELOX/Bevacizumab\n",
    "XELOX+Bevacizumab\n",
    "</pre>\n",
    "\n",
    "We can match all of these forms after the normalization process with three variants:\n",
    "\n",
    "<pre>\n",
    "xelox and bevacizumab\n",
    "xelox plus bevacizumab\n",
    "xelox bevacizumab\n",
    "</pre>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "###############################################################################\n",
    "def cleanup_terms(term_list):\n",
    "    \"\"\"\n",
    "    Apply text cleanup operations to the list of regimen names. Also enter\n",
    "    additional variants to capture the inconsistent usage of '&', +', and '/'\n",
    "    in the clinical trial data.\n",
    "    \"\"\"\n",
    "    \n",
    "    regimens = cleanup_text(term_list)\n",
    "\n",
    "    # enter variants for \"word1 + word2\" forms\n",
    "    result_set = set()\n",
    "    for regimen in regimens:\n",
    "\n",
    "        words = regimen.split()\n",
    "        if 1 == len(words):\n",
    "            result_set.add(regimen)\n",
    "        else:\n",
    "            if 3 == len(words) and 'and' in words:\n",
    "                index = words.index('and')\n",
    "                result_set.add(regimen)\n",
    "                words[index] = 'plus'\n",
    "                result_set.add(' '.join(words))\n",
    "                del words[index]\n",
    "                result_set.add(' '.join(words))\n",
    "            else:\n",
    "                result_set.add(regimen)\n",
    "\n",
    "    results = sorted(list(result_set))\n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The sentence normalization process applies the same text cleanup function, then substitutes **``<regimen>``** for each regimen term contained in the sentence. This is referred to in the code as the **REGIMEN_TOKEN**. This code also checks the common CHOP regimen for matches against \"Childrens Hospital of Philadelphia\", which unfortunately shares the same acronym."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "###############################################################################\n",
    "def cleanup_sentences(sentence_list, term_list):\n",
    "    \"\"\"\n",
    "    Apply cleanup operations to the sentence list. Substitute the REGIMEN_TOKEN\n",
    "    for all identified regimen terms in the sentence. Do some special handling\n",
    "    for the CHOP regimen to remove sentences referring to the Children's\n",
    "    Hospital of Philadelphia, which shares the same acronym.\n",
    "    \"\"\"\n",
    "    \n",
    "    results = set()\n",
    "    sentence_list = cleanup_text(sentence_list)\n",
    "    \n",
    "    # substitute <regimen> for all identified regimen names\n",
    "\n",
    "    counter = 0\n",
    "    for i in range(len(sentence_list)):\n",
    "        before = sentence_list[i]\n",
    "\n",
    "        after = before\n",
    "        for term in term_list:\n",
    "            after = re.sub(r'\\b' + term + r'\\b', REGIMEN_TOKEN, after)\n",
    "        if before != after:\n",
    "            # this sentence contains a regimen name\n",
    "            # ignore CHOP == Children's Hospital of Philadephia\n",
    "            # picu == Pediatric Intensive Care Unit\n",
    "            if -1 != after.find('childrens') or    \\\n",
    "               -1 != after.find('philadelphia') or \\\n",
    "               -1 != after.find('picu') or         \\\n",
    "               -1 != after.find('travel to'):\n",
    "                continue\n",
    "            else:\n",
    "                results.add(after)\n",
    "\n",
    "        counter += 1\n",
    "        if 0 == counter % 1000:\n",
    "            print('\\tprocessed {0}/{1}'.format(counter, len(sentence_list)))\n",
    "            \n",
    "    return list(results)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The text extracted by ClarityNLP can be resolved into individual sentences with the following code. The \"bullet\" character in the clinical trial data seems to be the \"dash\" character, which can appear either as an isolated dash or a dash immediately following the period of the preceding sentence. This function finds such occurrences and splits the text at those points. It also removes the \"Inclusion Criteria\" and \"Exclusion Criteria\" headers, should they be present."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "###############################################################################\n",
    "def resolve_sentences(text):\n",
    "    \"\"\"\n",
    "    Extract individual sentences from the text and return as a list.\n",
    "    Ignore the header lines and keep only the text relevant to the\n",
    "    clinical trial. A 'sentence' may actually only be a phrase or a brief\n",
    "    description in a bulleted list.\n",
    "    \"\"\"\n",
    "\n",
    "    sentence_list = []\n",
    "    \n",
    "    # replace occurrences of '.- ', which denote the end of a sentence\n",
    "    # followed by a new bulleted item with '. - ' to isolate the hyphen\n",
    "    text = re.sub(r'\\.\\-\\s', '. - ', text)\n",
    "    \n",
    "    # split on ' - ' to isolate the bulleted items\n",
    "    chunks = text.split(' - ')\n",
    "\n",
    "    # run the sentence tokenizer on the chunks\n",
    "    for chunk in chunks:\n",
    "        candidates = nltk.sent_tokenize(chunk)\n",
    "\n",
    "        # remove sentences consisting of '(In|Ex)clusion Criteria:'\n",
    "        for sent in candidates:\n",
    "            match = re.match(r'(In|Ex)clusion\\s+Criteria:\\s*\\Z', sent, re.IGNORECASE)\n",
    "            if match:\n",
    "                continue\n",
    "            sentence_list.append(sent)\n",
    "\n",
    "    return sentence_list\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, here is the code that generates data for the ngram language model. We use a keyword search to filter the text found by ClarityNLP. A candidate sentence is kept if any of these cancer-related keywords appear in the sentence. We also keep a sentence if it contains a mention of a cancer drug, immunotherapy drug, or a drug used treat Graft vs Host disease.\n",
    "\n",
    "These lookups are our substitute for a laborious human-in-the-loop review of thousands of sentences. The drawback of doing this is that our results may, to some extent, be trained on usages common only to sentences that include these terms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "###############################################################################\n",
    "def gen_language_model_data(csv_file, termset_file, drug_file):\n",
    "    \"\"\"\n",
    "    Load a ClarityNLP phenotype .csv file and extract the 'sentence' and 'term'\n",
    "    fields. The 'sentence' field can contain multiple sentences, so resolve\n",
    "    the text into individual sentences. Write sentences and terms to disk.\n",
    "    \"\"\"\n",
    "\n",
    "    # Keep the text extracted by ClarityNLP if any of these terms are present,\n",
    "    # since they are all relevant to cancer, chemotherapy, and immunotherapy.\n",
    "    # This is our (somewhat inadequate) substitute for human-in-the-loop\n",
    "    # review of the sentences.\n",
    "    # The 'r <regimen>' and '<regimen> r' options catch R-REGIMEN or REGIMEN-R\n",
    "    # names that we might have missed (R means 'rituximab').\n",
    "    str_keywords = r'(adjuvant|(b|t) cell|antiemetic|cancer|carcinoma|chemo|'             +\\\n",
    "                   r'chemoimmunotherapy|chemotheraputic|chemotherapy|cml|course(s)? of|'  +\\\n",
    "                   r'combination therapy|\\bcr\\b|cycle|(first|second) line|immunotherapy|' +\\\n",
    "                   r'induction|leukemia|lymphoma|malignancy|malignant|oncologist|'        +\\\n",
    "                   r'oncologic|oncology|metastases|monotherapy|refractory|remission|'     +\\\n",
    "                   r'relapse|resection|response|round|stage|stem cell|treatment|'         +\\\n",
    "                   r'toxic|toxicity|tumor|upfront|'                                       +\\\n",
    "                   r'r <regimen>|<regimen> r|'                                            +\\\n",
    "                   r'(?<!<)regimen(?!>))'\n",
    "    regex_keywords = re.compile(str_keywords, re.IGNORECASE)\n",
    "    \n",
    "    # data is a list of (term, text) tuples\n",
    "    data = load_csv_data(csv_file)\n",
    "\n",
    "    # load the NLPQL termset\n",
    "    term_list = load_termset(termset_file)\n",
    "    \n",
    "    # get the drug list as a set of strings, for faster lookup\n",
    "    drug_set = load_drugs(drug_file)\n",
    "    \n",
    "    sentence_list = []\n",
    "    for text in data:\n",
    "        new_sentences = resolve_sentences(text)\n",
    "        sentence_list.extend(new_sentences)\n",
    "        \n",
    "    # cleanup terms and sentences, substitute <regimen> token\n",
    "    print('cleaning terms...')\n",
    "    term_list = cleanup_terms(term_list)\n",
    "\n",
    "    print('cleaning sentences...')\n",
    "    sentence_list = cleanup_sentences(sentence_list, term_list)\n",
    "    \n",
    "    # try for a keyword or drug match; if no match, discard the sentence\n",
    "    keep_sentences    = []\n",
    "    discard_sentences = []\n",
    "    for s in sentence_list:\n",
    "        \n",
    "        num_kept = len(keep_sentences)\n",
    "        match = regex_keywords.search(s)\n",
    "        if match:\n",
    "            keep_sentences.append(s)\n",
    "        else:\n",
    "            # look for a known cancer-related drug name\n",
    "            words = s.split()\n",
    "            for w in words:\n",
    "                if w in drug_set:\n",
    "                    keep_sentences.append(s)\n",
    "                    break\n",
    "                    \n",
    "        if len(keep_sentences) == num_kept:\n",
    "            # discard this sentence\n",
    "            discard_sentences.append(s)\n",
    "    \n",
    "    # write the keep/discard sentences to separate files\n",
    "    data = [\n",
    "        (SENTENCE_FILE, keep_sentences),\n",
    "        (DISCARD_FILE, discard_sentences)\n",
    "    ]\n",
    "    \n",
    "    for data_file, data_list in data:\n",
    "        with open(data_file, 'wt') as outfile:\n",
    "            for sentence in data_list:\n",
    "                outfile.write('{0}\\n'.format(sentence))\n",
    "            print('\\nWrote {0} sentences to \"{1}\".'.format(len(data_list), data_file))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ngram Language Model and K-Fold Cross-Validation Code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have the code to generate the cross validation data, we turn our attention to the language model.\n",
    "\n",
    "An ngram language model is essentially a set of ngrams and their scores. The score represents the probability of occurrence for the ngram in the training text. Ngram frequency counts are used as a starting point for the probability estimate, but an additional \"smoothing\" procedure must be applied to make the model useful. You can find more information about ngram models in chapter three of the latest draft of [Jurafsky and Martin](https://web.stanford.edu/~jurafsky/slp3/). A comprehensive overview of smoothing techniques can be found in a [technical report by Chen and Goodman](https://dash.harvard.edu/handle/1/25104739).\n",
    "\n",
    "To generate the language model, we will use the state-of-the art in language model generation software, [KenLM](https://kheafield.com/code/kenlm/). This code is lightning fast and can handle language data at web scale. It is written in C++ for performance, so a binary will need to be built on your system if you want to run it.\n",
    "\n",
    "Download and build the code for your system with the following commands. You will need to install ``boost``, ``Eigen``, ``cmake``, and ``wget`` if you do not already have them installed on your system (or you can just download the tarball directly without using wget):\n",
    "\n",
    "<pre>\n",
    "wget -O - https://kheafield.com/code/kenlm.tar.gz |tar xz\n",
    "mkdir kenlm/build\n",
    "cd kenlm/build\n",
    "cmake ..\n",
    "make -j2\n",
    "</pre>\n",
    "\n",
    "The binary that we need is ``kenlm/build/lmplz``. A binary built for a MacOS High Serria system (MacOS 10.13.6) can be found in ``notebooks/cooking/session_10/mac_lmplz``.\n",
    "\n",
    "There is a python interface for kenlm, but the language model generation features do not seem to be callable from python."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our plan of attack will be as follows. First, we will randomly scramble our regimen-containing sentence dataset. Then we will divide it into K subsets, using one of the subsets as the test data and the remainder as the training data. We will use kenlm to generate a language model from the training data, then evaluate it on the test data. The evaluation process will report a score when it concludes. We will repeat the evaluation process K times and conclude by generating a final model trained on all of the data. The results of the evaluation will be the average and standard deviation of all scores."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The kenlm code wants to read the training data from a file on disk, so we need a function to write a list of sentences to a file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "###############################################################################\n",
    "def write_training_file(k, training_sentences):\n",
    "    \"\"\"\n",
    "    Write a list of training sentences to disk, one per line.\n",
    "    Returns the name of the file.\n",
    "    \"\"\"\n",
    "    \n",
    "    filename = 'session_10/train_data_{0}.txt'.format(k)\n",
    "    with open(filename, 'wt') as outfile:\n",
    "        for s in training_sentences:\n",
    "            outfile.write('{0}\\n'.format(s))\n",
    "\n",
    "    return filename\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will also need a function to load our training and test sentences from a file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "###############################################################################\n",
    "def load_sentences(filepath):\n",
    "    \"\"\"\n",
    "    Load sentences from a file, assumed to be one per line. Returns the data\n",
    "    as a list of strings.\n",
    "    \"\"\"\n",
    "\n",
    "    sentences = set()\n",
    "    with open(filepath, 'rt') as infile:\n",
    "        for line in infile:\n",
    "            sentence = line.strip()\n",
    "            assert REGIMEN_TOKEN in sentence\n",
    "            sentences.add(sentence)\n",
    "            \n",
    "    return list(sentences)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also need code to generate ngrams confined to a window near the REGIMEN_TOKEN. The start and end of sentence will be denoted with the START_TOKEN and END_TOKEN strings defined in the initial code cell above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "###############################################################################\n",
    "def gen_ngrams(n, sentence):\n",
    "    \"\"\"\n",
    "    Generate all ngrams of length n from the sentence that include\n",
    "    the REGIMEN_TOKEN. Returns a list of these ngrams. Denote the start and\n",
    "    end of the sentence with START_TOKEN and END_TOKEN.\n",
    "    \"\"\"\n",
    "    \n",
    "    words = sentence.split()\n",
    "    m = len(words)\n",
    "    assert REGIMEN_TOKEN in words\n",
    "\n",
    "    ngram_list = []\n",
    "    i = words.index(REGIMEN_TOKEN)\n",
    "    for start in range(i-n+1, i+1):\n",
    "        if start < -1:\n",
    "            # no ngram, too far left\n",
    "            continue\n",
    "        end = start + n\n",
    "        if end > m:\n",
    "            # no ngram, too far right\n",
    "            continue\n",
    "        ngram = []\n",
    "        for j in range(start, end):\n",
    "            if j < 0:\n",
    "                ngram.append(START_TOKEN)\n",
    "            elif j >= m:\n",
    "                ngram.append(END_TOKEN)\n",
    "            else:\n",
    "                ngram.append(words[j])\n",
    "\n",
    "        assert n == len(ngram)\n",
    "        ngram_list.append(ngram)\n",
    "\n",
    "    return ngram_list\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The kenlm code generates a language model file in .arpa format. This is a simple format [documented here](http://www.speech.sri.com/projects/srilm/manpages/ngram-format.5.html). Here is a function to read an .arpa file and extract the ngrams and their probabilities:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "###############################################################################\n",
    "def load_arpa_file(filepath):\n",
    "    \"\"\"\n",
    "    Load a language model file in .arpa format. Build a dict that maps the\n",
    "    ngram length n to a list of (probability, ngram) tuples. Sort the list\n",
    "    for each n in decreasing order of probability and return the dict.\n",
    "    \"\"\"\n",
    "\n",
    "    section = 0\n",
    "    counts = {}\n",
    "    ngram_dict = {}\n",
    "    with open(filepath, 'rt') as infile:\n",
    "        for line in infile:\n",
    "            if line.isspace() or 0 == len(line):\n",
    "                continue\n",
    "            if line.startswith('\\end'):\n",
    "                section = -1\n",
    "                continue\n",
    "\n",
    "            # match \\1-grams, \\2-grams, etc.\n",
    "            match = re.match(r'\\\\(?P<num>\\d+)\\-grams', line)\n",
    "            if match:\n",
    "                section = int(match.group('num'))\n",
    "                assert section >= 1\n",
    "                ngram_dict[section] = []\n",
    "                continue\n",
    "\n",
    "            if section == 0:\n",
    "                # get ngram counts\n",
    "                match = re.match(r'\\Angram\\s(?P<n>\\d+)=(?P<count>\\d+)', line)\n",
    "                if match:\n",
    "                    n = int(match.group('n'))\n",
    "                    count = int(match.group('count'))\n",
    "                    counts[n] = count\n",
    "            else:\n",
    "                components = line.split()\n",
    "                prob = float(components[0])\n",
    "                ngram = components[1:1+section]\n",
    "                ngram_dict[section].append( (prob, ngram) )\n",
    "    \n",
    "    # sort the ngrams for each n value in order of decreasing score\n",
    "    for n, tup_list in ngram_dict.items():\n",
    "        tup_list = sorted(tup_list, key=lambda x: x[0], reverse=True)\n",
    "        ngram_dict[n] = tup_list\n",
    "\n",
    "    return ngram_dict\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The kenlm language models include all ngrams generated from the training text. We only want those ngrams that contain the REGIMEN_TOKEN, so this function filters out the extraneous ngrams:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "###############################################################################\n",
    "def filter_ngrams(ngram_dict):\n",
    "    \"\"\"\n",
    "    The ngram_dict is a dict that maps the ngram length n to a list of \n",
    "    (probability, ngram) tuples. Remove all ngrams from the dict that do not\n",
    "    include the REGIMEN_TOKEN.\n",
    "    \"\"\"\n",
    "    \n",
    "    for n, tup_list in ngram_dict.items():\n",
    "        keep_list = []\n",
    "        for prob, ng in tup_list:\n",
    "            if REGIMEN_TOKEN in ng:\n",
    "                keep_list.append( (prob, ng) )\n",
    "        ngram_dict[n] = keep_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the kenlm language model must be generated by running the binary file that we built above, here is a function that launches a shell to run the binary and generate the model file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "###############################################################################\n",
    "def gen_language_model(train_file, model_file, log_file):\n",
    "    \"\"\"\n",
    "    Spawn a shell and run the kenlm binary. If the run fails (probably because\n",
    "    of insufficient data), write the error message to the log file. Returns a\n",
    "    Boolean indicating success or failure.\n",
    "    \"\"\"\n",
    "    \n",
    "    # shell command to run kenlm\n",
    "    command = 'session_10/mac_lmplz -o 5 --discount_fallback < {0} > {1}'.format(train_file, \n",
    "                                                                                 model_file)\n",
    "\n",
    "    # run the shell command\n",
    "    result = subprocess.run(command, shell=True, stderr=subprocess.PIPE)\n",
    "    if 0 != result.returncode:\n",
    "        log_file.write('\\nkenlm run failure: ')\n",
    "        log_file.write(result.stderr)\n",
    "        log_file.write('\\n')\n",
    "        return False\n",
    "    \n",
    "    return True\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is our model evaluation code. It works by finding all ngrams in the test and training data that include the ``REGIMEN_TOKEN``. For each such ngram in the test data, it tries to find a matching ngram in the training data. It checks all values of n from 3 to MAX_N (currently 5). It does not check bigrams since they tend to not be very informative. If multiple ngram matches are found, the winner is the ngram with the highest probability. If no matches are found it declares failure for that test ngram. The success percentage is returned as the score for the evaluation process.\n",
    "\n",
    "The idea here is that if the training data adequately represents the forms of expression used for chemotherapy regimens in clinical trial data, then the scores should be closer to 1 than to 0. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "###############################################################################\n",
    "def best_candidate(ngram_dict, sentence):\n",
    "    \"\"\"\n",
    "    Generate all REGIMEN_TOKEN-containing ngrams of lengths 3 .. MAX_N from\n",
    "    the sentence. Start with the sentence ngrams of length n = MAX_N. \n",
    "    For each ngram of this length, look for a match in the training ngrams. \n",
    "    If a match is found, save it if its probability is higher than that found\n",
    "    thus far. Then repeat with n = MAX_N - 1.\n",
    "    \n",
    "    Returns a tuple of the highest-probability matching ngram, if any, along\n",
    "    with the value of n and the probability. If no ngram is found, the first\n",
    "    component of the returned tuple has the value None.\n",
    "    \"\"\"\n",
    "    \n",
    "    best_n = None\n",
    "    best_prob = -9999999.0\n",
    "    best_ngram = None\n",
    "    \n",
    "    # skip bigrams in the ngram search, not necessarily informative\n",
    "    for n in reversed(range(3, MAX_N+1)):\n",
    "\n",
    "        # get the training ngrams that include \"<regimen>\"\n",
    "        #     this is a list of (prob, ngram) tuples\n",
    "        #     the ngram itself is a list of strings\n",
    "        train_ngrams = ngram_dict[n]\n",
    "\n",
    "        # list of ngrams from test sentence s that include \"<regimen>\"\n",
    "        test_ngrams = gen_ngrams(n, sentence)\n",
    "\n",
    "        for ng_test in test_ngrams:\n",
    "            # search for it in the training ngrams\n",
    "            for prob, ng_train in train_ngrams:\n",
    "                if ng_test == ng_train:\n",
    "                    # test ngram matches the training ngram\n",
    "                    # keep it if has higher probability\n",
    "                    if prob > best_prob:\n",
    "                        best_n = n\n",
    "                        best_prob = prob\n",
    "                        best_ngram = ng_test\n",
    "                        \n",
    "    return (best_n, best_prob, best_ngram)\n",
    "\n",
    "###############################################################################\n",
    "def evaluate(ngram_dict, test_data):\n",
    "    \"\"\"\n",
    "    Run the model on all test sentences. Accumulate a count of all matches\n",
    "    found and return a score equal to the percentage of the test set for which\n",
    "    a match was found.\n",
    "    \"\"\"\n",
    "\n",
    "    success_count = 0\n",
    "    failure_count = 0\n",
    "    sentence_count = len(test_data)\n",
    "    \n",
    "    for s in test_data:\n",
    "        \n",
    "        best_n, best_prob, best_ngram = best_candidate(ngram_dict, s)\n",
    "                            \n",
    "        if best_n is not None:\n",
    "            success_count += 1\n",
    "        else:\n",
    "            failure_count += 1\n",
    "\n",
    "    assert success_count + failure_count == sentence_count\n",
    "\n",
    "    score = success_count / sentence_count\n",
    "    return score\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, here is the K-fold cross-validation code. The default value of K is 10."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "###############################################################################\n",
    "def cross_validate(filepath, K=10):\n",
    "    \"\"\"\n",
    "    Perform K-fold cross validation of our model on the given data file.\n",
    "    The data file contains preprocessed and cleaned sentences, one per line.\n",
    "    Each sentence contains one or more instances of REGIMEN_TOKEN.\n",
    "    \"\"\"\n",
    "\n",
    "    orig_sentence_list = load_sentences(filepath)\n",
    "    m = len(orig_sentence_list)\n",
    "    print('Loaded {0} unique sentences.'.format(m))\n",
    "\n",
    "    # shuffle the sentences\n",
    "    indices = [i for i in range(m)]\n",
    "    random.shuffle(indices)\n",
    "    sentence_list = [orig_sentence_list[indices[i]] for i in indices]\n",
    "\n",
    "    # split data into K chunks\n",
    "    chunksize = m // K\n",
    "    print('K: {0}, chunksize: {1}.'.format(K, chunksize))\n",
    "\n",
    "    # open the error log\n",
    "    log_file = open(LOG_FILE, 'wt')\n",
    "    \n",
    "    scores = []\n",
    "    \n",
    "    # do the K-fold cross validation\n",
    "    for k in range(K):\n",
    "\n",
    "        print('*** Iteration {0}/{1} ***'.format(k+1, K))\n",
    "        \n",
    "        # index range for the kth chunk\n",
    "        lo = int((k+0) * chunksize)\n",
    "        hi = int((k+1) * chunksize)\n",
    "\n",
    "        # the test data is the kth chunk\n",
    "        test_data = sentence_list[lo:hi]\n",
    "\n",
    "        # the training data is everything else\n",
    "        train_data = sentence_list[:lo]\n",
    "        train_data.extend(sentence_list[hi:])\n",
    "        \n",
    "        # consistency checks\n",
    "        assert len(train_data) + len(test_data) == m\n",
    "        for s in test_data:\n",
    "            assert s not in train_data\n",
    "\n",
    "        # write training sentences to a file, to be loaded by kenlm\n",
    "        train_file = write_training_file(k, train_data)\n",
    "        \n",
    "        # name of kenlm .arpa model file\n",
    "        model_file = 'session_10/model_{0}.arpa'.format(k)\n",
    "\n",
    "        # run kenlm and create the .arpa file\n",
    "        if not gen_language_model(train_file, model_file, log_file):\n",
    "            print('kenlm run failed...check error log')\n",
    "            print('skipping this iteration...')\n",
    "            continue\n",
    "        \n",
    "        # load the language model file, get the ngrams with probabilities\n",
    "        # map of n -> [ (prob_0, [words_0]), (prob_1, [words_1]), ...] \n",
    "        ngram_dict = load_arpa_file(model_file)\n",
    "\n",
    "        # keep only those ngrams that contain the <regimen> token\n",
    "        filter_ngrams(ngram_dict)\n",
    "\n",
    "        # print stats on ngrams containing the <regimen> token\n",
    "        print('\\tNgrams containing REGIMEN_TOKEN: ')\n",
    "        for n, tup_list in ngram_dict.items():\n",
    "            if n > 1:\n",
    "                print('\\tn = {0}, count: {1}'.format(n, len(tup_list)))\n",
    "        \n",
    "        # evaluate and record score for this round\n",
    "        score = evaluate(ngram_dict, test_data)\n",
    "        print('\\tscore: {0:0.4f}'.format(score))\n",
    "        \n",
    "        scores.append(score)\n",
    "\n",
    "    print('\\nMean score: {0:.4f}'.format(statistics.mean(scores)))\n",
    "    print('  Std. dev: {0:.4f}'.format(statistics.stdev(scores)))\n",
    "\n",
    "    # generate final model using all data\n",
    "    gen_language_model(filepath, FINAL_MODEL_FILE, log_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the next cell to generate the language model data (``session_10/sentences.txt``):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cleaning terms...\n",
      "cleaning sentences...\n",
      "\tprocessed 1000/6891\n",
      "\tprocessed 2000/6891\n",
      "\tprocessed 3000/6891\n",
      "\tprocessed 4000/6891\n",
      "\tprocessed 5000/6891\n",
      "\tprocessed 6000/6891\n",
      "\n",
      "Wrote 1266 sentences to \"session_10/sentences.txt\".\n",
      "\n",
      "Wrote 212 sentences to \"session_10/discard_sentences.txt\".\n"
     ]
    }
   ],
   "source": [
    "phenotype_file = 'session_10/phenotype_regimens.csv'\n",
    "termset_file   = 'session_10/regimen_termset.txt'\n",
    "drug_file      = 'session_10/drug_names.txt'\n",
    "\n",
    "# generate the data\n",
    "gen_language_model_data(phenotype_file, termset_file, drug_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the code in the next cell to generate, train, and test language models on the test data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 1266 unique sentences.\n",
      "K: 10, chunksize: 126.\n",
      "*** Iteration 1/10 ***\n",
      "\tNgrams containing REGIMEN_TOKEN: \n",
      "\tn = 2, count: 490\n",
      "\tn = 3, count: 2193\n",
      "\tn = 4, count: 3906\n",
      "\tn = 5, count: 5084\n",
      "\tscore: 0.8016\n",
      "*** Iteration 2/10 ***\n",
      "\tNgrams containing REGIMEN_TOKEN: \n",
      "\tn = 2, count: 484\n",
      "\tn = 3, count: 2185\n",
      "\tn = 4, count: 3880\n",
      "\tn = 5, count: 5064\n",
      "\tscore: 0.7937\n",
      "*** Iteration 3/10 ***\n",
      "\tNgrams containing REGIMEN_TOKEN: \n",
      "\tn = 2, count: 482\n",
      "\tn = 3, count: 2182\n",
      "\tn = 4, count: 3881\n",
      "\tn = 5, count: 5062\n",
      "\tscore: 0.7778\n",
      "*** Iteration 4/10 ***\n",
      "\tNgrams containing REGIMEN_TOKEN: \n",
      "\tn = 2, count: 480\n",
      "\tn = 3, count: 2157\n",
      "\tn = 4, count: 3866\n",
      "\tn = 5, count: 5043\n",
      "\tscore: 0.7698\n",
      "*** Iteration 5/10 ***\n",
      "\tNgrams containing REGIMEN_TOKEN: \n",
      "\tn = 2, count: 492\n",
      "\tn = 3, count: 2195\n",
      "\tn = 4, count: 3887\n",
      "\tn = 5, count: 5070\n",
      "\tscore: 0.8175\n",
      "*** Iteration 6/10 ***\n",
      "\tNgrams containing REGIMEN_TOKEN: \n",
      "\tn = 2, count: 488\n",
      "\tn = 3, count: 2203\n",
      "\tn = 4, count: 3894\n",
      "\tn = 5, count: 5053\n",
      "\tscore: 0.8889\n",
      "*** Iteration 7/10 ***\n",
      "\tNgrams containing REGIMEN_TOKEN: \n",
      "\tn = 2, count: 482\n",
      "\tn = 3, count: 2195\n",
      "\tn = 4, count: 3918\n",
      "\tn = 5, count: 5079\n",
      "\tscore: 0.8254\n",
      "*** Iteration 8/10 ***\n",
      "\tNgrams containing REGIMEN_TOKEN: \n",
      "\tn = 2, count: 482\n",
      "\tn = 3, count: 2184\n",
      "\tn = 4, count: 3866\n",
      "\tn = 5, count: 5050\n",
      "\tscore: 0.8254\n",
      "*** Iteration 9/10 ***\n",
      "\tNgrams containing REGIMEN_TOKEN: \n",
      "\tn = 2, count: 487\n",
      "\tn = 3, count: 2198\n",
      "\tn = 4, count: 3921\n",
      "\tn = 5, count: 5108\n",
      "\tscore: 0.8333\n",
      "*** Iteration 10/10 ***\n",
      "\tNgrams containing REGIMEN_TOKEN: \n",
      "\tn = 2, count: 491\n",
      "\tn = 3, count: 2176\n",
      "\tn = 4, count: 3863\n",
      "\tn = 5, count: 5025\n",
      "\tscore: 0.8254\n",
      "\n",
      "Mean score: 0.8159\n",
      "  Std. dev: 0.0336\n"
     ]
    }
   ],
   "source": [
    "# perform the cross-validation\n",
    "K = 10\n",
    "cross_validate(SENTENCE_FILE, K)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We observe that our model can correctly find mentions of chemotherapy regimens slightly more than 80% of the time.\n",
    "\n",
    "**We contend that we can improve the accuracy of our model and move the score towards 1.0 by accumulating more training data.** With more data, the ability of the model to correctly recognize proper usage of chemotherapy regimens should increase."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can assess the model by running it on the sentences that were discarded. We would expect the score for these sentences to be substantially less the 80% score we obtained on the training sentences. The score will not be zero for the following reasons:\n",
    "\n",
    "* Our pruning heuristics above are not 100% reliable, and we incorrectly discard some valid data\n",
    "* Our pruning heuristics may generate a data set subtly dependent on our keywords\n",
    "* Sometimes an ngram from the training data occurs in the discarded sentences\n",
    "\n",
    "The code in the next cell performs this evaluation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating the final model on discarded sentences...\n",
      "\n",
      "Score on discarded sentences: 0.4764\n"
     ]
    }
   ],
   "source": [
    "# load the model trained on all the data\n",
    "ngram_dict = load_arpa_file('session_10/final_model.arpa')\n",
    "\n",
    "sentences = []\n",
    "with open(DISCARD_FILE, 'rt') as infile:\n",
    "    for line in infile:\n",
    "        sentences.append(line.strip())\n",
    "        \n",
    "print('Evaluating the final model on discarded sentences...')\n",
    "score = evaluate(ngram_dict, sentences)\n",
    "print('\\nScore on discarded sentences: {0:.4f}'.format(score))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This score on the discarded data is substantially less than the ~80% score that we obtained on our training data, confirming that the model does indeed exhibit worse performance on the discarded sentences. This is what we had hoped for. Hence our smoothed ngram language model does have some predictive ability, notwithstanding the fact that it was trained on a small corpus of only 1266 sentences.\n",
    "\n",
    "The score on the discarded data probably has a lower bound that we will never be able to overcome, since the intersection of the following sets is likely to be nonempty:\n",
    "\n",
    "* the set of ngrams used to state chemotherapy regimens in clinical trial eligibility criteria\n",
    "* the set of ngrams used for common words and abbreviations that also happen to be chemotherapy regimen names\n",
    "\n",
    "Thus this ngram language model can only be one component of a still-to-be-determined algorithm for **reliably** identifying chemotherapy regimen names in clinical trial data. We plan to improve this model by incorporating machine learning techniques, perhaps involving word embeddings or other context-aware methods.\n",
    "\n",
    "We may revisit this topic in future cooking sessions. Stay tuned!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
